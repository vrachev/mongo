[resmoke] 2019-04-12T10:29:38.464-0400 verbatim resmoke.py invocation: buildscripts/resmoke.py initSyncDBHashMismatch/shouldworkRepro.js
[resmoke] 2019-04-12T10:29:38.469-0400 YAML configuration of suite with_server
test_kind: js_test

selector:
  roots:
  - initSyncDBHashMismatch/shouldworkRepro.js

executor:
  config:
    shell_options:
      readMode: commands
  fixture:
    class: MongoDFixture
    mongod_options:
      set_parameters:
        enableTestCommands: 1

logging:
  executor:
    format: '[%(name)s] %(asctime)s %(message)s'
    handlers:
    - class: logging.StreamHandler
  fixture:
    format: '[%(name)s] %(message)s'
    handlers:
    - class: logging.StreamHandler
  tests:
    format: '[%(name)s] %(asctime)s %(message)s'
    handlers:
    - class: logging.StreamHandler
[executor] 2019-04-12T10:29:38.469-0400 Starting execution of js_tests...
[executor:js_test:job0] 2019-04-12T10:29:38.470-0400 Running job0_fixture_setup...
[js_test:job0_fixture_setup] 2019-04-12T10:29:38.470-0400 Starting the setup of MongoDFixture (Job #0).
[MongoDFixture:job0] Starting mongod on port 20000...
./mongod --setParameter enableTestCommands=1 --setParameter logComponentVerbosity={'replication': {'rollback': 2}, 'transaction': 4} --setParameter disableLogicalSessionCacheRefresh=true --setParameter transactionLifetimeLimitSeconds=86400 --dbpath=/data/db/job0/resmoke --port=20000 --enableMajorityReadConcern=True
[MongoDFixture:job0] mongod started on port 20000 with pid 61774.
[js_test:job0_fixture_setup] 2019-04-12T10:29:38.478-0400 Waiting for MongoDFixture (Job #0) to be ready.
[MongoDFixture:job0] 2019-04-12T10:29:38.503-0400 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
[MongoDFixture:job0] 2019-04-12T10:29:38.514-0400 I  CONTROL  [initandlisten] MongoDB starting : pid=61774 port=20000 dbpath=/data/db/job0/resmoke 64-bit host=vladmac
[MongoDFixture:job0] 2019-04-12T10:29:38.514-0400 I  CONTROL  [initandlisten] DEBUG build (which is slower)
[MongoDFixture:job0] 2019-04-12T10:29:38.514-0400 I  CONTROL  [initandlisten] db version v4.1.9-115-gb09f71e81a
[MongoDFixture:job0] 2019-04-12T10:29:38.514-0400 I  CONTROL  [initandlisten] git version: b09f71e81a5921e3a63edd396e6b1b7a3deece60
[MongoDFixture:job0] 2019-04-12T10:29:38.514-0400 I  CONTROL  [initandlisten] allocator: system
[MongoDFixture:job0] 2019-04-12T10:29:38.514-0400 I  CONTROL  [initandlisten] modules: none
[MongoDFixture:job0] 2019-04-12T10:29:38.514-0400 I  CONTROL  [initandlisten] build environment:
[MongoDFixture:job0] 2019-04-12T10:29:38.514-0400 I  CONTROL  [initandlisten]     distarch: x86_64
[MongoDFixture:job0] 2019-04-12T10:29:38.514-0400 I  CONTROL  [initandlisten]     target_arch: x86_64
[MongoDFixture:job0] 2019-04-12T10:29:38.514-0400 I  CONTROL  [initandlisten] options: { net: { port: 20000 }, replication: { enableMajorityReadConcern: true }, setParameter: { disableLogicalSessionCacheRefresh: "true", enableTestCommands: "1", logComponentVerbosity: "{'replication': {'rollback': 2}, 'transaction': 4}", transactionLifetimeLimitSeconds: "86400" }, storage: { dbPath: "/data/db/job0/resmoke" } }
[MongoDFixture:job0] 2019-04-12T10:29:38.515-0400 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=7680M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=(recovery_progress),
[MongoDFixture:job0] Waiting to connect to mongod on port 20000.
[MongoDFixture:job0] 2019-04-12T10:29:39.000-0400 I  STORAGE  [initandlisten] WiredTiger message [1555079379:586][61774:0x1134655c0], txn-recover: Set global recovery timestamp: (0,0)
[MongoDFixture:job0] 2019-04-12T10:29:39.045-0400 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
[MongoDFixture:job0] 2019-04-12T10:29:39.074-0400 I  STORAGE  [initandlisten] Timestamp monitor starting
[MongoDFixture:job0] 2019-04-12T10:29:39.075-0400 I  CONTROL  [initandlisten]
[MongoDFixture:job0] 2019-04-12T10:29:39.075-0400 I  CONTROL  [initandlisten] ** NOTE: This is a development version (4.1.9-115-gb09f71e81a) of MongoDB.
[MongoDFixture:job0] 2019-04-12T10:29:39.075-0400 I  CONTROL  [initandlisten] **       Not recommended for production.
[MongoDFixture:job0] 2019-04-12T10:29:39.075-0400 I  CONTROL  [initandlisten]
[MongoDFixture:job0] 2019-04-12T10:29:39.075-0400 I  CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
[MongoDFixture:job0] 2019-04-12T10:29:39.075-0400 I  CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
[MongoDFixture:job0] 2019-04-12T10:29:39.075-0400 I  CONTROL  [initandlisten]
[MongoDFixture:job0] 2019-04-12T10:29:39.075-0400 I  CONTROL  [initandlisten] ** WARNING: This server is bound to localhost.
[MongoDFixture:job0] 2019-04-12T10:29:39.075-0400 I  CONTROL  [initandlisten] **          Remote systems will be unable to connect to this server.
[MongoDFixture:job0] 2019-04-12T10:29:39.075-0400 I  CONTROL  [initandlisten] **          Start the server with --bind_ip <address> to specify which IP
[MongoDFixture:job0] 2019-04-12T10:29:39.075-0400 I  CONTROL  [initandlisten] **          addresses it should serve responses from, or with --bind_ip_all to
[MongoDFixture:job0] 2019-04-12T10:29:39.075-0400 I  CONTROL  [initandlisten] **          bind to all interfaces. If this behavior is desired, start the
[MongoDFixture:job0] 2019-04-12T10:29:39.075-0400 I  CONTROL  [initandlisten] **          server with --bind_ip 127.0.0.1 to disable this warning.
[MongoDFixture:job0] 2019-04-12T10:29:39.075-0400 I  CONTROL  [initandlisten]
[MongoDFixture:job0] 2019-04-12T10:29:39.077-0400 I  STORAGE  [initandlisten] createCollection: admin.system.version with provided UUID: 2f89bb2e-7e56-49d9-aee6-41b254ffeef0
[MongoDFixture:job0] 2019-04-12T10:29:39.105-0400 I  INDEX    [initandlisten] index build: done building index _id_ on ns admin.system.version
[MongoDFixture:job0] 2019-04-12T10:29:39.106-0400 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
[MongoDFixture:job0] 2019-04-12T10:29:39.106-0400 I  COMMAND  [initandlisten] setting featureCompatibilityVersion to 4.2
[MongoDFixture:job0] 2019-04-12T10:29:39.109-0400 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
[MongoDFixture:job0] 2019-04-12T10:29:39.109-0400 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
[MongoDFixture:job0] 2019-04-12T10:29:39.109-0400 I  STORAGE  [initandlisten] createCollection: local.startup_log with generated UUID: 992681e1-d9f3-4168-a6ae-f90161d1b0d1
[MongoDFixture:job0] 2019-04-12T10:29:39.135-0400 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.startup_log
[MongoDFixture:job0] 2019-04-12T10:29:39.135-0400 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
[MongoDFixture:job0] 2019-04-12T10:29:39.136-0400 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/data/db/job0/resmoke/diagnostic.data'
[MongoDFixture:job0] 2019-04-12T10:29:39.138-0400 I  NETWORK  [initandlisten] Listening on /tmp/mongodb-20000.sock
[MongoDFixture:job0] 2019-04-12T10:29:39.138-0400 I  NETWORK  [initandlisten] Listening on 127.0.0.1
[MongoDFixture:job0] 2019-04-12T10:29:39.138-0400 I  NETWORK  [initandlisten] waiting for connections on port 20000
[MongoDFixture:job0] 2019-04-12T10:29:39.189-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61832 #1 (1 connection now open)
[MongoDFixture:job0] 2019-04-12T10:29:39.190-0400 I  NETWORK  [conn1] end connection 127.0.0.1:61832 (0 connections now open)
[MongoDFixture:job0] 2019-04-12T10:29:39.196-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61833 #2 (1 connection now open)
[MongoDFixture:job0] 2019-04-12T10:29:39.196-0400 I  NETWORK  [conn2] received client metadata from 127.0.0.1:61833 conn2: { driver: { name: "PyMongo", version: "3.7.2" }, os: { type: "Darwin", name: "Darwin", architecture: "x86_64", version: "10.14" }, platform: "CPython 3.7.3.final.0" }
[MongoDFixture:job0] 2019-04-12T10:29:39.198-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61834 #3 (2 connections now open)
[MongoDFixture:job0] 2019-04-12T10:29:39.198-0400 I  NETWORK  [conn3] received client metadata from 127.0.0.1:61834 conn3: { driver: { name: "PyMongo", version: "3.7.2" }, os: { type: "Darwin", name: "Darwin", architecture: "x86_64", version: "10.14" }, platform: "CPython 3.7.3.final.0" }
[MongoDFixture:job0] Successfully contacted the mongod on port 20000.
[js_test:job0_fixture_setup] 2019-04-12T10:29:39.200-0400 Finished the setup of MongoDFixture (Job #0).
[executor:js_test:job0] 2019-04-12T10:29:39.200-0400 job0_fixture_setup ran in 0.73 seconds: no failures detected.
[MongoDFixture:job0] 2019-04-12T10:29:39.200-0400 I  NETWORK  [conn3] end connection 127.0.0.1:61834 (1 connection now open)
[MongoDFixture:job0] 2019-04-12T10:29:39.200-0400 I  NETWORK  [conn2] end connection 127.0.0.1:61833 (0 connections now open)
[executor:js_test:job0] 2019-04-12T10:29:39.202-0400 Running shouldworkRepro.js...
./mongo --eval MongoRunner.dataDir = "/data/db/job0/mongorunner"; MongoRunner.dataPath = "/data/db/job0/mongorunner/"; MongoRunner.mongoShellPath = "/Users/vrachev/mongoDevelop/mongo/mongo"; TestData = new Object(); TestData.minPort = 20020; TestData.maxPort = 20249; TestData.failIfUnterminatedProcesses = true; TestData.enableMajorityReadConcern = true; TestData.noJournal = false; TestData.serviceExecutor = ""; TestData.storageEngine = ""; TestData.storageEngineCacheSizeGB = ""; TestData.testName = "shouldworkRepro"; TestData.transportLayer = ""; TestData.wiredTigerCollectionConfigString = ""; TestData.wiredTigerEngineConfigString = ""; TestData.wiredTigerIndexConfigString = ""; TestData.setParameters = new Object(); TestData.setParameters.logComponentVerbosity = new Object(); TestData.setParameters.logComponentVerbosity.replication = new Object(); TestData.setParameters.logComponentVerbosity.replication.rollback = 2; TestData.setParameters.logComponentVerbosity.transaction = 4; TestData.setParametersMongos = new Object(); TestData.setParametersMongos.logComponentVerbosity = new Object(); TestData.setParametersMongos.logComponentVerbosity.transaction = 3; TestData.transactionLifetimeLimitSeconds = 86400; load('jstests/libs/override_methods/validate_collections_on_shutdown.js');; load('jstests/libs/override_methods/check_uuids_consistent_across_cluster.js');; load('jstests/libs/override_methods/implicitly_retry_on_background_op_in_progress.js'); --readMode=commands mongodb://localhost:20000 initSyncDBHashMismatch/shouldworkRepro.js
[js_test:shouldworkRepro] 2019-04-12T10:29:39.203-0400 Starting JSTest initSyncDBHashMismatch/shouldworkRepro.js...
./mongo --eval MongoRunner.dataDir = "/data/db/job0/mongorunner"; MongoRunner.dataPath = "/data/db/job0/mongorunner/"; MongoRunner.mongoShellPath = "/Users/vrachev/mongoDevelop/mongo/mongo"; TestData = new Object(); TestData.minPort = 20020; TestData.maxPort = 20249; TestData.failIfUnterminatedProcesses = true; TestData.isMainTest = true; TestData.numTestClients = 1; TestData.enableMajorityReadConcern = true; TestData.noJournal = false; TestData.serviceExecutor = ""; TestData.storageEngine = ""; TestData.storageEngineCacheSizeGB = ""; TestData.testName = "shouldworkRepro"; TestData.transportLayer = ""; TestData.wiredTigerCollectionConfigString = ""; TestData.wiredTigerEngineConfigString = ""; TestData.wiredTigerIndexConfigString = ""; TestData.setParameters = new Object(); TestData.setParameters.logComponentVerbosity = new Object(); TestData.setParameters.logComponentVerbosity.replication = new Object(); TestData.setParameters.logComponentVerbosity.replication.rollback = 2; TestData.setParameters.logComponentVerbosity.transaction = 4; TestData.setParametersMongos = new Object(); TestData.setParametersMongos.logComponentVerbosity = new Object(); TestData.setParametersMongos.logComponentVerbosity.transaction = 3; TestData.transactionLifetimeLimitSeconds = 86400; load('jstests/libs/override_methods/validate_collections_on_shutdown.js');; load('jstests/libs/override_methods/check_uuids_consistent_across_cluster.js');; load('jstests/libs/override_methods/implicitly_retry_on_background_op_in_progress.js'); --readMode=commands mongodb://localhost:20000 initSyncDBHashMismatch/shouldworkRepro.js
[js_test:shouldworkRepro] 2019-04-12T10:29:39.208-0400 JSTest initSyncDBHashMismatch/shouldworkRepro.js started with pid 61775.
[js_test:shouldworkRepro] 2019-04-12T10:29:39.383-0400 MongoDB shell version v4.1.9-115-gb09f71e81a
[js_test:shouldworkRepro] 2019-04-12T10:29:39.418-0400 connecting to: mongodb://localhost:20000/?compressors=disabled&gssapiServiceName=mongodb
[MongoDFixture:job0] 2019-04-12T10:29:39.419-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61835 #4 (1 connection now open)
[MongoDFixture:job0] 2019-04-12T10:29:39.420-0400 I  NETWORK  [conn4] received client metadata from 127.0.0.1:61835 conn4: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:39.422-0400 Implicit session: session { "id" : UUID("20cf9133-ef50-4789-bcca-acf90a9fde33") }
[js_test:shouldworkRepro] 2019-04-12T10:29:39.423-0400 MongoDB server version: 4.1.9-115-gb09f71e81a
[js_test:shouldworkRepro] 2019-04-12T10:29:39.425-0400 true
[js_test:shouldworkRepro] 2019-04-12T10:29:39.427-0400 Starting new replica set InitialSyncTest
[js_test:shouldworkRepro] 2019-04-12T10:29:39.428-0400 ReplSetTest starting set
[js_test:shouldworkRepro] 2019-04-12T10:29:39.428-0400 ReplSetTest n is : 0
[js_test:shouldworkRepro] 2019-04-12T10:29:39.428-0400 {
[js_test:shouldworkRepro] 2019-04-12T10:29:39.428-0400 	"useHostName" : true,
[js_test:shouldworkRepro] 2019-04-12T10:29:39.429-0400 	"oplogSize" : 40,
[js_test:shouldworkRepro] 2019-04-12T10:29:39.429-0400 	"keyFile" : undefined,
[js_test:shouldworkRepro] 2019-04-12T10:29:39.429-0400 	"port" : 20020,
[js_test:shouldworkRepro] 2019-04-12T10:29:39.429-0400 	"replSet" : "InitialSyncTest",
[js_test:shouldworkRepro] 2019-04-12T10:29:39.429-0400 	"dbpath" : "$set-$node",
[js_test:shouldworkRepro] 2019-04-12T10:29:39.429-0400 	"restart" : undefined,
[js_test:shouldworkRepro] 2019-04-12T10:29:39.429-0400 	"pathOpts" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:39.429-0400 		"node" : 0,
[js_test:shouldworkRepro] 2019-04-12T10:29:39.429-0400 		"set" : "InitialSyncTest"
[js_test:shouldworkRepro] 2019-04-12T10:29:39.429-0400 	},
[js_test:shouldworkRepro] 2019-04-12T10:29:39.429-0400 	"setParameter" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:39.429-0400 		"writePeriodicNoops" : false,
[js_test:shouldworkRepro] 2019-04-12T10:29:39.429-0400 		"numInitialSyncConnectAttempts" : 60
[js_test:shouldworkRepro] 2019-04-12T10:29:39.429-0400 	}
[js_test:shouldworkRepro] 2019-04-12T10:29:39.429-0400 }
[js_test:shouldworkRepro] 2019-04-12T10:29:39.430-0400 ReplSetTest Starting....
[js_test:shouldworkRepro] 2019-04-12T10:29:39.430-0400 Resetting db path '/data/db/job0/mongorunner/InitialSyncTest-0'
[js_test:shouldworkRepro] 2019-04-12T10:29:39.433-0400 2019-04-12T10:29:39.433-0400 I  -        [js] shell: started program (sh61776):  /Users/vrachev/mongoDevelop/mongo/mongod --oplogSize 40 --port 20020 --replSet InitialSyncTest --dbpath /data/db/job0/mongorunner/InitialSyncTest-0 --setParameter writePeriodicNoops=false --setParameter numInitialSyncConnectAttempts=60 --bind_ip 0.0.0.0 --setParameter enableTestCommands=1 --setParameter disableLogicalSessionCacheRefresh=true --setParameter transactionLifetimeLimitSeconds=86400 --setParameter orphanCleanupDelaySecs=1 --enableMajorityReadConcern true --setParameter logComponentVerbosity={"replication":{"rollback":2},"transaction":4}
[js_test:shouldworkRepro] 2019-04-12T10:29:39.469-0400 d20020| 2019-04-12T10:29:39.468-0400 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
[js_test:shouldworkRepro] 2019-04-12T10:29:39.473-0400 d20020| 2019-04-12T10:29:39.473-0400 I  CONTROL  [initandlisten] MongoDB starting : pid=61776 port=20020 dbpath=/data/db/job0/mongorunner/InitialSyncTest-0 64-bit host=vladmac
[js_test:shouldworkRepro] 2019-04-12T10:29:39.473-0400 d20020| 2019-04-12T10:29:39.473-0400 I  CONTROL  [initandlisten] DEBUG build (which is slower)
[js_test:shouldworkRepro] 2019-04-12T10:29:39.473-0400 d20020| 2019-04-12T10:29:39.473-0400 I  CONTROL  [initandlisten] db version v4.1.9-115-gb09f71e81a
[js_test:shouldworkRepro] 2019-04-12T10:29:39.473-0400 d20020| 2019-04-12T10:29:39.473-0400 I  CONTROL  [initandlisten] git version: b09f71e81a5921e3a63edd396e6b1b7a3deece60
[js_test:shouldworkRepro] 2019-04-12T10:29:39.473-0400 d20020| 2019-04-12T10:29:39.473-0400 I  CONTROL  [initandlisten] allocator: system
[js_test:shouldworkRepro] 2019-04-12T10:29:39.473-0400 d20020| 2019-04-12T10:29:39.473-0400 I  CONTROL  [initandlisten] modules: none
[js_test:shouldworkRepro] 2019-04-12T10:29:39.473-0400 d20020| 2019-04-12T10:29:39.473-0400 I  CONTROL  [initandlisten] build environment:
[js_test:shouldworkRepro] 2019-04-12T10:29:39.473-0400 d20020| 2019-04-12T10:29:39.473-0400 I  CONTROL  [initandlisten]     distarch: x86_64
[js_test:shouldworkRepro] 2019-04-12T10:29:39.473-0400 d20020| 2019-04-12T10:29:39.473-0400 I  CONTROL  [initandlisten]     target_arch: x86_64
[js_test:shouldworkRepro] 2019-04-12T10:29:39.474-0400 d20020| 2019-04-12T10:29:39.473-0400 I  CONTROL  [initandlisten] options: { net: { bindIp: "0.0.0.0", port: 20020 }, replication: { enableMajorityReadConcern: true, oplogSizeMB: 40, replSet: "InitialSyncTest" }, setParameter: { disableLogicalSessionCacheRefresh: "true", enableTestCommands: "1", logComponentVerbosity: "{"replication":{"rollback":2},"transaction":4}", numInitialSyncConnectAttempts: "60", orphanCleanupDelaySecs: "1", transactionLifetimeLimitSeconds: "86400", writePeriodicNoops: "false" }, storage: { dbPath: "/data/db/job0/mongorunner/InitialSyncTest-0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:39.474-0400 d20020| 2019-04-12T10:29:39.474-0400 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=7680M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=(recovery_progress),
[js_test:shouldworkRepro] 2019-04-12T10:29:39.954-0400 d20020| 2019-04-12T10:29:39.954-0400 I  STORAGE  [initandlisten] WiredTiger message [1555079379:954468][61776:0x119b9f5c0], txn-recover: Set global recovery timestamp: (0,0)
[js_test:shouldworkRepro] 2019-04-12T10:29:39.997-0400 d20020| 2019-04-12T10:29:39.997-0400 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
[js_test:shouldworkRepro] 2019-04-12T10:29:40.022-0400 d20020| 2019-04-12T10:29:40.022-0400 I  STORAGE  [initandlisten] Timestamp monitor starting
[js_test:shouldworkRepro] 2019-04-12T10:29:40.023-0400 d20020| 2019-04-12T10:29:40.023-0400 I  CONTROL  [initandlisten]
[js_test:shouldworkRepro] 2019-04-12T10:29:40.023-0400 d20020| 2019-04-12T10:29:40.023-0400 I  CONTROL  [initandlisten] ** NOTE: This is a development version (4.1.9-115-gb09f71e81a) of MongoDB.
[js_test:shouldworkRepro] 2019-04-12T10:29:40.023-0400 d20020| 2019-04-12T10:29:40.023-0400 I  CONTROL  [initandlisten] **       Not recommended for production.
[js_test:shouldworkRepro] 2019-04-12T10:29:40.023-0400 d20020| 2019-04-12T10:29:40.023-0400 I  CONTROL  [initandlisten]
[js_test:shouldworkRepro] 2019-04-12T10:29:40.023-0400 d20020| 2019-04-12T10:29:40.023-0400 I  CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
[js_test:shouldworkRepro] 2019-04-12T10:29:40.023-0400 d20020| 2019-04-12T10:29:40.023-0400 I  CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
[js_test:shouldworkRepro] 2019-04-12T10:29:40.023-0400 d20020| 2019-04-12T10:29:40.023-0400 I  CONTROL  [initandlisten]
[js_test:shouldworkRepro] 2019-04-12T10:29:40.025-0400 d20020| 2019-04-12T10:29:40.025-0400 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:40.025-0400 d20020| 2019-04-12T10:29:40.025-0400 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:40.026-0400 d20020| 2019-04-12T10:29:40.026-0400 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:40.027-0400 d20020| 2019-04-12T10:29:40.027-0400 I  STORAGE  [initandlisten] createCollection: local.startup_log with generated UUID: 36a8a47e-33f4-4c85-9009-1ded81383630
[js_test:shouldworkRepro] 2019-04-12T10:29:40.054-0400 d20020| 2019-04-12T10:29:40.054-0400 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.startup_log
[js_test:shouldworkRepro] 2019-04-12T10:29:40.055-0400 d20020| 2019-04-12T10:29:40.054-0400 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:40.055-0400 d20020| 2019-04-12T10:29:40.055-0400 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/data/db/job0/mongorunner/InitialSyncTest-0/diagnostic.data'
[js_test:shouldworkRepro] 2019-04-12T10:29:40.057-0400 d20020| 2019-04-12T10:29:40.057-0400 I  STORAGE  [initandlisten] createCollection: local.replset.oplogTruncateAfterPoint with generated UUID: c21a565e-6449-4e31-96f3-cde8961d155d
[js_test:shouldworkRepro] 2019-04-12T10:29:40.057-0400 d20020| 2019-04-12T10:29:40.057-0400 I  SHARDING [monitoring-keys-for-HMAC] Marking collection admin.system.keys as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:40.086-0400 d20020| 2019-04-12T10:29:40.086-0400 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.oplogTruncateAfterPoint
[js_test:shouldworkRepro] 2019-04-12T10:29:40.086-0400 d20020| 2019-04-12T10:29:40.086-0400 I  STORAGE  [initandlisten] createCollection: local.replset.minvalid with generated UUID: fb182528-7039-41c8-ba91-e1ce8b426c4d
[js_test:shouldworkRepro] 2019-04-12T10:29:40.114-0400 d20020| 2019-04-12T10:29:40.114-0400 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.minvalid
[js_test:shouldworkRepro] 2019-04-12T10:29:40.114-0400 d20020| 2019-04-12T10:29:40.114-0400 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:40.114-0400 d20020| 2019-04-12T10:29:40.114-0400 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:40.115-0400 d20020| 2019-04-12T10:29:40.114-0400 I  REPL     [initandlisten] Did not find local voted for document at startup.
[js_test:shouldworkRepro] 2019-04-12T10:29:40.115-0400 d20020| 2019-04-12T10:29:40.114-0400 I  REPL     [initandlisten] Did not find local Rollback ID document at startup. Creating one.
[js_test:shouldworkRepro] 2019-04-12T10:29:40.115-0400 d20020| 2019-04-12T10:29:40.114-0400 I  STORAGE  [initandlisten] createCollection: local.system.rollback.id with generated UUID: bad7ac5c-8865-4ae4-be02-4d2754fc9a6e
[js_test:shouldworkRepro] 2019-04-12T10:29:40.140-0400 d20020| 2019-04-12T10:29:40.140-0400 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.system.rollback.id
[js_test:shouldworkRepro] 2019-04-12T10:29:40.140-0400 d20020| 2019-04-12T10:29:40.140-0400 I  SHARDING [initandlisten] Marking collection local.system.rollback.id as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:40.141-0400 d20020| 2019-04-12T10:29:40.140-0400 I  REPL     [initandlisten] Initialized the rollback ID to 1
[js_test:shouldworkRepro] 2019-04-12T10:29:40.141-0400 d20020| 2019-04-12T10:29:40.140-0400 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
[js_test:shouldworkRepro] 2019-04-12T10:29:40.141-0400 d20020| 2019-04-12T10:29:40.141-0400 I  NETWORK  [initandlisten] Listening on /tmp/mongodb-20020.sock
[js_test:shouldworkRepro] 2019-04-12T10:29:40.141-0400 d20020| 2019-04-12T10:29:40.141-0400 I  NETWORK  [initandlisten] Listening on 0.0.0.0
[js_test:shouldworkRepro] 2019-04-12T10:29:40.141-0400 d20020| 2019-04-12T10:29:40.141-0400 I  NETWORK  [initandlisten] waiting for connections on port 20020
[js_test:shouldworkRepro] 2019-04-12T10:29:40.150-0400 d20020| 2019-04-12T10:29:40.150-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61837 #1 (1 connection now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:40.150-0400 d20020| 2019-04-12T10:29:40.150-0400 I  NETWORK  [conn1] received client metadata from 127.0.0.1:61837 conn1: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:40.152-0400 [ connection to vladmac:20020 ]
[js_test:shouldworkRepro] 2019-04-12T10:29:40.152-0400 ReplSetTest n is : 1
[js_test:shouldworkRepro] 2019-04-12T10:29:40.152-0400 {
[js_test:shouldworkRepro] 2019-04-12T10:29:40.152-0400 	"useHostName" : true,
[js_test:shouldworkRepro] 2019-04-12T10:29:40.152-0400 	"oplogSize" : 40,
[js_test:shouldworkRepro] 2019-04-12T10:29:40.152-0400 	"keyFile" : undefined,
[js_test:shouldworkRepro] 2019-04-12T10:29:40.152-0400 	"port" : 20021,
[js_test:shouldworkRepro] 2019-04-12T10:29:40.152-0400 	"replSet" : "InitialSyncTest",
[js_test:shouldworkRepro] 2019-04-12T10:29:40.152-0400 	"dbpath" : "$set-$node",
[js_test:shouldworkRepro] 2019-04-12T10:29:40.153-0400 	"restart" : undefined,
[js_test:shouldworkRepro] 2019-04-12T10:29:40.153-0400 	"pathOpts" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:40.153-0400 		"node" : 1,
[js_test:shouldworkRepro] 2019-04-12T10:29:40.153-0400 		"set" : "InitialSyncTest"
[js_test:shouldworkRepro] 2019-04-12T10:29:40.153-0400 	},
[js_test:shouldworkRepro] 2019-04-12T10:29:40.153-0400 	"setParameter" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:40.153-0400 		"writePeriodicNoops" : false,
[js_test:shouldworkRepro] 2019-04-12T10:29:40.153-0400 		"numInitialSyncConnectAttempts" : 60
[js_test:shouldworkRepro] 2019-04-12T10:29:40.153-0400 	}
[js_test:shouldworkRepro] 2019-04-12T10:29:40.153-0400 }
[js_test:shouldworkRepro] 2019-04-12T10:29:40.153-0400 ReplSetTest Starting....
[js_test:shouldworkRepro] 2019-04-12T10:29:40.153-0400 Resetting db path '/data/db/job0/mongorunner/InitialSyncTest-1'
[js_test:shouldworkRepro] 2019-04-12T10:29:40.155-0400 2019-04-12T10:29:40.155-0400 I  -        [js] shell: started program (sh61777):  /Users/vrachev/mongoDevelop/mongo/mongod --oplogSize 40 --port 20021 --replSet InitialSyncTest --dbpath /data/db/job0/mongorunner/InitialSyncTest-1 --setParameter writePeriodicNoops=false --setParameter numInitialSyncConnectAttempts=60 --bind_ip 0.0.0.0 --setParameter enableTestCommands=1 --setParameter disableLogicalSessionCacheRefresh=true --setParameter transactionLifetimeLimitSeconds=86400 --setParameter orphanCleanupDelaySecs=1 --enableMajorityReadConcern true --setParameter logComponentVerbosity={"replication":{"rollback":2},"transaction":4}
[js_test:shouldworkRepro] 2019-04-12T10:29:40.184-0400 d20021| 2019-04-12T10:29:40.183-0400 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
[js_test:shouldworkRepro] 2019-04-12T10:29:40.195-0400 d20021| 2019-04-12T10:29:40.195-0400 I  CONTROL  [initandlisten] MongoDB starting : pid=61777 port=20021 dbpath=/data/db/job0/mongorunner/InitialSyncTest-1 64-bit host=vladmac
[js_test:shouldworkRepro] 2019-04-12T10:29:40.195-0400 d20021| 2019-04-12T10:29:40.195-0400 I  CONTROL  [initandlisten] DEBUG build (which is slower)
[js_test:shouldworkRepro] 2019-04-12T10:29:40.195-0400 d20021| 2019-04-12T10:29:40.195-0400 I  CONTROL  [initandlisten] db version v4.1.9-115-gb09f71e81a
[js_test:shouldworkRepro] 2019-04-12T10:29:40.195-0400 d20021| 2019-04-12T10:29:40.195-0400 I  CONTROL  [initandlisten] git version: b09f71e81a5921e3a63edd396e6b1b7a3deece60
[js_test:shouldworkRepro] 2019-04-12T10:29:40.195-0400 d20021| 2019-04-12T10:29:40.195-0400 I  CONTROL  [initandlisten] allocator: system
[js_test:shouldworkRepro] 2019-04-12T10:29:40.195-0400 d20021| 2019-04-12T10:29:40.195-0400 I  CONTROL  [initandlisten] modules: none
[js_test:shouldworkRepro] 2019-04-12T10:29:40.195-0400 d20021| 2019-04-12T10:29:40.195-0400 I  CONTROL  [initandlisten] build environment:
[js_test:shouldworkRepro] 2019-04-12T10:29:40.196-0400 d20021| 2019-04-12T10:29:40.195-0400 I  CONTROL  [initandlisten]     distarch: x86_64
[js_test:shouldworkRepro] 2019-04-12T10:29:40.196-0400 d20021| 2019-04-12T10:29:40.195-0400 I  CONTROL  [initandlisten]     target_arch: x86_64
[js_test:shouldworkRepro] 2019-04-12T10:29:40.196-0400 d20021| 2019-04-12T10:29:40.195-0400 I  CONTROL  [initandlisten] options: { net: { bindIp: "0.0.0.0", port: 20021 }, replication: { enableMajorityReadConcern: true, oplogSizeMB: 40, replSet: "InitialSyncTest" }, setParameter: { disableLogicalSessionCacheRefresh: "true", enableTestCommands: "1", logComponentVerbosity: "{"replication":{"rollback":2},"transaction":4}", numInitialSyncConnectAttempts: "60", orphanCleanupDelaySecs: "1", transactionLifetimeLimitSeconds: "86400", writePeriodicNoops: "false" }, storage: { dbPath: "/data/db/job0/mongorunner/InitialSyncTest-1" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:40.196-0400 d20021| 2019-04-12T10:29:40.196-0400 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=7680M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=(recovery_progress),
[js_test:shouldworkRepro] 2019-04-12T10:29:40.673-0400 d20021| 2019-04-12T10:29:40.673-0400 I  STORAGE  [initandlisten] WiredTiger message [1555079380:673289][61777:0x10f71c5c0], txn-recover: Set global recovery timestamp: (0,0)
[js_test:shouldworkRepro] 2019-04-12T10:29:40.719-0400 d20021| 2019-04-12T10:29:40.719-0400 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
[js_test:shouldworkRepro] 2019-04-12T10:29:40.744-0400 d20021| 2019-04-12T10:29:40.743-0400 I  STORAGE  [initandlisten] Timestamp monitor starting
[js_test:shouldworkRepro] 2019-04-12T10:29:40.744-0400 d20021| 2019-04-12T10:29:40.744-0400 I  CONTROL  [initandlisten]
[js_test:shouldworkRepro] 2019-04-12T10:29:40.744-0400 d20021| 2019-04-12T10:29:40.744-0400 I  CONTROL  [initandlisten] ** NOTE: This is a development version (4.1.9-115-gb09f71e81a) of MongoDB.
[js_test:shouldworkRepro] 2019-04-12T10:29:40.745-0400 d20021| 2019-04-12T10:29:40.744-0400 I  CONTROL  [initandlisten] **       Not recommended for production.
[js_test:shouldworkRepro] 2019-04-12T10:29:40.745-0400 d20021| 2019-04-12T10:29:40.744-0400 I  CONTROL  [initandlisten]
[js_test:shouldworkRepro] 2019-04-12T10:29:40.745-0400 d20021| 2019-04-12T10:29:40.744-0400 I  CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
[js_test:shouldworkRepro] 2019-04-12T10:29:40.745-0400 d20021| 2019-04-12T10:29:40.744-0400 I  CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
[js_test:shouldworkRepro] 2019-04-12T10:29:40.745-0400 d20021| 2019-04-12T10:29:40.744-0400 I  CONTROL  [initandlisten]
[js_test:shouldworkRepro] 2019-04-12T10:29:40.746-0400 d20021| 2019-04-12T10:29:40.746-0400 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:40.746-0400 d20021| 2019-04-12T10:29:40.746-0400 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:40.747-0400 d20021| 2019-04-12T10:29:40.747-0400 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:40.748-0400 d20021| 2019-04-12T10:29:40.748-0400 I  STORAGE  [initandlisten] createCollection: local.startup_log with generated UUID: 17033994-3479-4c9d-8d63-27104a483ae7
[js_test:shouldworkRepro] 2019-04-12T10:29:40.772-0400 d20021| 2019-04-12T10:29:40.772-0400 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.startup_log
[js_test:shouldworkRepro] 2019-04-12T10:29:40.773-0400 d20021| 2019-04-12T10:29:40.772-0400 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:40.773-0400 d20021| 2019-04-12T10:29:40.773-0400 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/data/db/job0/mongorunner/InitialSyncTest-1/diagnostic.data'
[js_test:shouldworkRepro] 2019-04-12T10:29:40.776-0400 d20021| 2019-04-12T10:29:40.776-0400 I  STORAGE  [initandlisten] createCollection: local.replset.oplogTruncateAfterPoint with generated UUID: 711227e7-d55c-4cc6-884e-c71bee25b402
[js_test:shouldworkRepro] 2019-04-12T10:29:40.776-0400 d20021| 2019-04-12T10:29:40.776-0400 I  SHARDING [monitoring-keys-for-HMAC] Marking collection admin.system.keys as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:40.803-0400 d20021| 2019-04-12T10:29:40.803-0400 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.oplogTruncateAfterPoint
[js_test:shouldworkRepro] 2019-04-12T10:29:40.803-0400 d20021| 2019-04-12T10:29:40.803-0400 I  STORAGE  [initandlisten] createCollection: local.replset.minvalid with generated UUID: 2568aeb5-fcd4-4084-b6aa-f5a12afbf43a
[js_test:shouldworkRepro] 2019-04-12T10:29:40.835-0400 d20021| 2019-04-12T10:29:40.835-0400 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.minvalid
[js_test:shouldworkRepro] 2019-04-12T10:29:40.835-0400 d20021| 2019-04-12T10:29:40.835-0400 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:40.835-0400 d20021| 2019-04-12T10:29:40.835-0400 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:40.836-0400 d20021| 2019-04-12T10:29:40.835-0400 I  REPL     [initandlisten] Did not find local voted for document at startup.
[js_test:shouldworkRepro] 2019-04-12T10:29:40.836-0400 d20021| 2019-04-12T10:29:40.835-0400 I  REPL     [initandlisten] Did not find local Rollback ID document at startup. Creating one.
[js_test:shouldworkRepro] 2019-04-12T10:29:40.836-0400 d20021| 2019-04-12T10:29:40.835-0400 I  STORAGE  [initandlisten] createCollection: local.system.rollback.id with generated UUID: 63b1cd92-0cee-48f7-aa9c-672d637ce700
[js_test:shouldworkRepro] 2019-04-12T10:29:40.860-0400 d20021| 2019-04-12T10:29:40.860-0400 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.system.rollback.id
[js_test:shouldworkRepro] 2019-04-12T10:29:40.860-0400 d20021| 2019-04-12T10:29:40.860-0400 I  SHARDING [initandlisten] Marking collection local.system.rollback.id as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:40.860-0400 d20021| 2019-04-12T10:29:40.860-0400 I  REPL     [initandlisten] Initialized the rollback ID to 1
[js_test:shouldworkRepro] 2019-04-12T10:29:40.860-0400 d20021| 2019-04-12T10:29:40.860-0400 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
[js_test:shouldworkRepro] 2019-04-12T10:29:40.861-0400 d20021| 2019-04-12T10:29:40.861-0400 I  NETWORK  [initandlisten] Listening on /tmp/mongodb-20021.sock
[js_test:shouldworkRepro] 2019-04-12T10:29:40.861-0400 d20021| 2019-04-12T10:29:40.861-0400 I  NETWORK  [initandlisten] Listening on 0.0.0.0
[js_test:shouldworkRepro] 2019-04-12T10:29:40.861-0400 d20021| 2019-04-12T10:29:40.861-0400 I  NETWORK  [initandlisten] waiting for connections on port 20021
[js_test:shouldworkRepro] 2019-04-12T10:29:40.862-0400 d20021| 2019-04-12T10:29:40.862-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61841 #1 (1 connection now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:40.863-0400 d20021| 2019-04-12T10:29:40.863-0400 I  NETWORK  [conn1] received client metadata from 127.0.0.1:61841 conn1: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:40.863-0400 [ connection to vladmac:20020, connection to vladmac:20021 ]
[js_test:shouldworkRepro] 2019-04-12T10:29:40.865-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:40.865-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:40.865-0400 [jsTest] ----
[js_test:shouldworkRepro] 2019-04-12T10:29:40.865-0400 [jsTest] New session started with sessionID: { "id" : UUID("939ef704-1682-44c8-9bdb-67cba9e46184") }
[js_test:shouldworkRepro] 2019-04-12T10:29:40.865-0400 [jsTest] ----
[js_test:shouldworkRepro] 2019-04-12T10:29:40.865-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:40.865-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:40.868-0400 {
[js_test:shouldworkRepro] 2019-04-12T10:29:40.868-0400 	"replSetInitiate" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:40.868-0400 		"_id" : "InitialSyncTest",
[js_test:shouldworkRepro] 2019-04-12T10:29:40.868-0400 		"protocolVersion" : 1,
[js_test:shouldworkRepro] 2019-04-12T10:29:40.868-0400 		"members" : [
[js_test:shouldworkRepro] 2019-04-12T10:29:40.868-0400 			{
[js_test:shouldworkRepro] 2019-04-12T10:29:40.868-0400 				"_id" : 0,
[js_test:shouldworkRepro] 2019-04-12T10:29:40.868-0400 				"host" : "vladmac:20020"
[js_test:shouldworkRepro] 2019-04-12T10:29:40.868-0400 			}
[js_test:shouldworkRepro] 2019-04-12T10:29:40.868-0400 		]
[js_test:shouldworkRepro] 2019-04-12T10:29:40.868-0400 	}
[js_test:shouldworkRepro] 2019-04-12T10:29:40.868-0400 }
[js_test:shouldworkRepro] 2019-04-12T10:29:40.868-0400 d20020| 2019-04-12T10:29:40.868-0400 I  REPL     [conn1] replSetInitiate admin command received from client
[js_test:shouldworkRepro] 2019-04-12T10:29:40.869-0400 d20020| 2019-04-12T10:29:40.869-0400 I  REPL     [conn1] replSetInitiate config object with 1 members parses ok
[js_test:shouldworkRepro] 2019-04-12T10:29:40.869-0400 d20020| 2019-04-12T10:29:40.869-0400 I  SHARDING [conn1] Marking collection local.oplog.rs as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:40.869-0400 d20020| 2019-04-12T10:29:40.869-0400 I  REPL     [conn1] ******
[js_test:shouldworkRepro] 2019-04-12T10:29:40.869-0400 d20020| 2019-04-12T10:29:40.869-0400 I  REPL     [conn1] creating replication oplog of size: 40MB...
[js_test:shouldworkRepro] 2019-04-12T10:29:40.870-0400 d20020| 2019-04-12T10:29:40.869-0400 I  STORAGE  [conn1] createCollection: local.oplog.rs with generated UUID: afad9498-39cf-4f83-bbe7-349638c18dd7
[js_test:shouldworkRepro] 2019-04-12T10:29:40.881-0400 d20020| 2019-04-12T10:29:40.881-0400 I  STORAGE  [conn1] Starting OplogTruncaterThread local.oplog.rs
[js_test:shouldworkRepro] 2019-04-12T10:29:40.881-0400 d20020| 2019-04-12T10:29:40.881-0400 I  STORAGE  [conn1] The size storer reports that the oplog contains 0 records totaling to 0 bytes
[js_test:shouldworkRepro] 2019-04-12T10:29:40.881-0400 d20020| 2019-04-12T10:29:40.881-0400 I  STORAGE  [conn1] Scanning the oplog to determine where to place markers for truncation
[js_test:shouldworkRepro] 2019-04-12T10:29:40.968-0400 d20020| 2019-04-12T10:29:40.968-0400 I  REPL     [conn1] ******
[js_test:shouldworkRepro] 2019-04-12T10:29:40.969-0400 d20020| 2019-04-12T10:29:40.968-0400 I  STORAGE  [conn1] createCollection: local.system.replset with generated UUID: 16b3ac43-be77-4341-b45a-ff43144f3242
[js_test:shouldworkRepro] 2019-04-12T10:29:40.996-0400 d20020| 2019-04-12T10:29:40.996-0400 I  INDEX    [conn1] index build: done building index _id_ on ns local.system.replset
[js_test:shouldworkRepro] 2019-04-12T10:29:41.002-0400 d20020| 2019-04-12T10:29:41.002-0400 I  SHARDING [conn1] Marking collection local.replset.oplogTruncateAfterPoint as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:41.002-0400 d20020| 2019-04-12T10:29:41.002-0400 I  STORAGE  [conn1] createCollection: admin.system.version with provided UUID: 79fe740f-34c2-436a-b791-66b5f56f6718
[js_test:shouldworkRepro] 2019-04-12T10:29:41.004-0400 d20021| 2019-04-12T10:29:41.004-0400 I  SHARDING [ftdc] Marking collection local.oplog.rs as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:41.028-0400 d20020| 2019-04-12T10:29:41.028-0400 I  INDEX    [conn1] index build: done building index _id_ on ns admin.system.version
[js_test:shouldworkRepro] 2019-04-12T10:29:41.028-0400 d20020| 2019-04-12T10:29:41.028-0400 I  COMMAND  [conn1] setting featureCompatibilityVersion to 4.2
[js_test:shouldworkRepro] 2019-04-12T10:29:41.028-0400 d20020| 2019-04-12T10:29:41.028-0400 I  NETWORK  [conn1] Skip closing connection for connection # 1
[js_test:shouldworkRepro] 2019-04-12T10:29:41.028-0400 d20020| 2019-04-12T10:29:41.028-0400 I  REPL     [conn1] New replica set config in use: { _id: "InitialSyncTest", version: 1, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "vladmac:20020", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5cb0a0d43e462239f4270e2b') } }
[js_test:shouldworkRepro] 2019-04-12T10:29:41.029-0400 d20020| 2019-04-12T10:29:41.028-0400 I  REPL     [conn1] This node is vladmac:20020 in the config
[js_test:shouldworkRepro] 2019-04-12T10:29:41.029-0400 d20020| 2019-04-12T10:29:41.028-0400 I  REPL     [conn1] transition to STARTUP2 from STARTUP
[js_test:shouldworkRepro] 2019-04-12T10:29:41.029-0400 d20020| 2019-04-12T10:29:41.028-0400 I  REPL     [conn1] Starting replication storage threads
[js_test:shouldworkRepro] 2019-04-12T10:29:41.029-0400 d20020| 2019-04-12T10:29:41.029-0400 I  REPL     [conn1] transition to RECOVERING from STARTUP2
[js_test:shouldworkRepro] 2019-04-12T10:29:41.029-0400 d20020| 2019-04-12T10:29:41.029-0400 I  REPL     [conn1] Starting replication fetcher thread
[js_test:shouldworkRepro] 2019-04-12T10:29:41.029-0400 d20020| 2019-04-12T10:29:41.029-0400 I  REPL     [conn1] Starting replication applier thread
[js_test:shouldworkRepro] 2019-04-12T10:29:41.029-0400 d20020| 2019-04-12T10:29:41.029-0400 I  REPL     [conn1] Starting replication reporter thread
[js_test:shouldworkRepro] 2019-04-12T10:29:41.029-0400 d20020| 2019-04-12T10:29:41.029-0400 I  REPL     [rsSync-0] Starting oplog application
[js_test:shouldworkRepro] 2019-04-12T10:29:41.029-0400 d20020| 2019-04-12T10:29:41.029-0400 I  COMMAND  [conn1] command local.replset.minvalid appName: "MongoDB Shell" command: replSetInitiate { replSetInitiate: { _id: "InitialSyncTest", protocolVersion: 1.0, members: [ { _id: 0.0, host: "vladmac:20020" } ] }, lsid: { id: UUID("939ef704-1682-44c8-9bdb-67cba9e46184") }, $clusterTime: { clusterTime: Timestamp(0, 0), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $db: "admin" } numYields:0 reslen:163 locks:{ Global: { acquireCount: { r: 25, w: 33, W: 2 }, acquireWaitCount: { W: 1 }, timeAcquiringMicros: { W: 88 } }, Database: { acquireCount: { r: 2, w: 3, R: 1, W: 10 } }, Collection: { acquireCount: { r: 1, w: 2 } }, Mutex: { acquireCount: { r: 16 } }, oplog: { acquireCount: { r: 1, w: 2 } } } storage:{} protocol:op_msg 161ms
[js_test:shouldworkRepro] 2019-04-12T10:29:41.030-0400 d20020| 2019-04-12T10:29:41.029-0400 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
[js_test:shouldworkRepro] 2019-04-12T10:29:41.030-0400 d20020| 2019-04-12T10:29:41.030-0400 I  ELECTION [rsSync-0] conducting a dry run election to see if we could be elected. current term: 0
[js_test:shouldworkRepro] 2019-04-12T10:29:41.030-0400 d20020| 2019-04-12T10:29:41.030-0400 I  ELECTION [replexec-0] dry election run succeeded, running for election in term 1
[js_test:shouldworkRepro] 2019-04-12T10:29:41.030-0400 d20020| 2019-04-12T10:29:41.030-0400 I  STORAGE  [replexec-1] createCollection: local.replset.election with generated UUID: 262526c2-05ef-4187-9e13-4fa7027d2337
[js_test:shouldworkRepro] 2019-04-12T10:29:41.031-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:41.031-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:41.031-0400 [jsTest] ----
[js_test:shouldworkRepro] 2019-04-12T10:29:41.031-0400 [jsTest] New session started with sessionID: { "id" : UUID("0fcb8d46-a6ae-420b-bfe5-aceb6ad06451") }
[js_test:shouldworkRepro] 2019-04-12T10:29:41.031-0400 [jsTest] ----
[js_test:shouldworkRepro] 2019-04-12T10:29:41.031-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:41.031-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:41.056-0400 d20020| 2019-04-12T10:29:41.055-0400 I  INDEX    [replexec-1] index build: done building index _id_ on ns local.replset.election
[js_test:shouldworkRepro] 2019-04-12T10:29:41.062-0400 d20020| 2019-04-12T10:29:41.062-0400 I  ELECTION [replexec-1] election succeeded, assuming primary role in term 1
[js_test:shouldworkRepro] 2019-04-12T10:29:41.062-0400 d20020| 2019-04-12T10:29:41.062-0400 I  REPL     [replexec-1] transition to PRIMARY from SECONDARY
[js_test:shouldworkRepro] 2019-04-12T10:29:41.062-0400 d20020| 2019-04-12T10:29:41.062-0400 I  REPL     [replexec-1] Resetting sync source to empty, which was :27017
[js_test:shouldworkRepro] 2019-04-12T10:29:41.062-0400 d20020| 2019-04-12T10:29:41.062-0400 I  REPL     [replexec-1] Entering primary catch-up mode.
[js_test:shouldworkRepro] 2019-04-12T10:29:41.062-0400 d20020| 2019-04-12T10:29:41.062-0400 I  REPL     [replexec-1] Exited primary catch-up mode.
[js_test:shouldworkRepro] 2019-04-12T10:29:41.062-0400 d20020| 2019-04-12T10:29:41.062-0400 I  REPL     [replexec-1] Stopping replication producer
[js_test:shouldworkRepro] 2019-04-12T10:29:43.036-0400 d20020| 2019-04-12T10:29:43.036-0400 I  SHARDING [rsSync-0] Marking collection config.transactions as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:43.036-0400 d20020| 2019-04-12T10:29:43.036-0400 I  STORAGE  [rsSync-0] createCollection: config.transactions with generated UUID: a6348a5e-ec46-4ef4-9436-f5d70a411e91
[js_test:shouldworkRepro] 2019-04-12T10:29:43.072-0400 d20020| 2019-04-12T10:29:43.071-0400 I  INDEX    [rsSync-0] index build: done building index _id_ on ns config.transactions
[js_test:shouldworkRepro] 2019-04-12T10:29:43.072-0400 d20020| 2019-04-12T10:29:43.072-0400 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
[js_test:shouldworkRepro] 2019-04-12T10:29:43.075-0400 d20020| 2019-04-12T10:29:43.075-0400 I  STORAGE  [monitoring-keys-for-HMAC] createCollection: admin.system.keys with generated UUID: 344947af-588d-4a6c-95ed-fc25d1d38bfc
[js_test:shouldworkRepro] 2019-04-12T10:29:43.081-0400 Reconfiguring replica set to add in other nodes
[js_test:shouldworkRepro] 2019-04-12T10:29:43.081-0400 {
[js_test:shouldworkRepro] 2019-04-12T10:29:43.081-0400 	"replSetReconfig" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:43.081-0400 		"_id" : "InitialSyncTest",
[js_test:shouldworkRepro] 2019-04-12T10:29:43.081-0400 		"protocolVersion" : 1,
[js_test:shouldworkRepro] 2019-04-12T10:29:43.081-0400 		"members" : [
[js_test:shouldworkRepro] 2019-04-12T10:29:43.081-0400 			{
[js_test:shouldworkRepro] 2019-04-12T10:29:43.082-0400 				"_id" : 0,
[js_test:shouldworkRepro] 2019-04-12T10:29:43.082-0400 				"host" : "vladmac:20020"
[js_test:shouldworkRepro] 2019-04-12T10:29:43.082-0400 			},
[js_test:shouldworkRepro] 2019-04-12T10:29:43.082-0400 			{
[js_test:shouldworkRepro] 2019-04-12T10:29:43.082-0400 				"_id" : 1,
[js_test:shouldworkRepro] 2019-04-12T10:29:43.082-0400 				"host" : "vladmac:20021",
[js_test:shouldworkRepro] 2019-04-12T10:29:43.082-0400 				"priority" : 0,
[js_test:shouldworkRepro] 2019-04-12T10:29:43.082-0400 				"votes" : 0
[js_test:shouldworkRepro] 2019-04-12T10:29:43.082-0400 			}
[js_test:shouldworkRepro] 2019-04-12T10:29:43.082-0400 		],
[js_test:shouldworkRepro] 2019-04-12T10:29:43.082-0400 		"version" : 2
[js_test:shouldworkRepro] 2019-04-12T10:29:43.082-0400 	}
[js_test:shouldworkRepro] 2019-04-12T10:29:43.083-0400 }
[js_test:shouldworkRepro] 2019-04-12T10:29:43.083-0400 d20020| 2019-04-12T10:29:43.081-0400 I  REPL     [conn1] replSetReconfig admin command received from client; new config: { _id: "InitialSyncTest", protocolVersion: 1.0, members: [ { _id: 0.0, host: "vladmac:20020" }, { _id: 1.0, host: "vladmac:20021", priority: 0.0, votes: 0.0 } ], version: 2.0 }
[js_test:shouldworkRepro] 2019-04-12T10:29:43.084-0400 d20021| 2019-04-12T10:29:43.084-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61842 #2 (2 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:43.084-0400 d20020| 2019-04-12T10:29:43.084-0400 I  REPL     [conn1] replSetReconfig config object with 2 members parses ok
[js_test:shouldworkRepro] 2019-04-12T10:29:43.084-0400 d20021| 2019-04-12T10:29:43.084-0400 I  NETWORK  [conn2] end connection 127.0.0.1:61842 (1 connection now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:43.085-0400 d20020| 2019-04-12T10:29:43.085-0400 I  REPL     [replexec-0] New replica set config in use: { _id: "InitialSyncTest", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "vladmac:20020", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "vladmac:20021", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 0.0, tags: {}, slaveDelay: 0, votes: 0 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5cb0a0d43e462239f4270e2b') } }
[js_test:shouldworkRepro] 2019-04-12T10:29:43.085-0400 d20020| 2019-04-12T10:29:43.085-0400 I  REPL     [replexec-0] This node is vladmac:20020 in the config
[js_test:shouldworkRepro] 2019-04-12T10:29:43.085-0400 d20020| 2019-04-12T10:29:43.085-0400 I  CONNPOOL [Replication] Connecting to vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:43.086-0400 d20021| 2019-04-12T10:29:43.086-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61843 #3 (2 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:43.086-0400 d20021| 2019-04-12T10:29:43.086-0400 I  NETWORK  [conn3] received client metadata from 127.0.0.1:61843 conn3: { driver: { name: "NetworkInterfaceTL", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:43.087-0400 d20021| 2019-04-12T10:29:43.086-0400 I  CONNPOOL [Replication] Connecting to vladmac:20020
[js_test:shouldworkRepro] 2019-04-12T10:29:43.087-0400 d20020| 2019-04-12T10:29:43.087-0400 I  REPL     [replexec-1] Member vladmac:20021 is now in state STARTUP
[js_test:shouldworkRepro] 2019-04-12T10:29:43.088-0400 d20020| 2019-04-12T10:29:43.088-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61844 #4 (2 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:43.088-0400 d20020| 2019-04-12T10:29:43.088-0400 I  NETWORK  [conn4] received client metadata from 127.0.0.1:61844 conn4: { driver: { name: "NetworkInterfaceTL", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:43.090-0400 d20020| 2019-04-12T10:29:43.089-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61845 #5 (3 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:43.090-0400 d20020| 2019-04-12T10:29:43.090-0400 I  NETWORK  [conn5] end connection 127.0.0.1:61845 (2 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:43.091-0400 d20021| 2019-04-12T10:29:43.090-0400 I  STORAGE  [replexec-0] createCollection: local.system.replset with generated UUID: e4b6364a-7b23-4358-81f4-f46a326705e3
[js_test:shouldworkRepro] 2019-04-12T10:29:43.105-0400 d20020| 2019-04-12T10:29:43.105-0400 I  INDEX    [monitoring-keys-for-HMAC] index build: done building index _id_ on ns admin.system.keys
[js_test:shouldworkRepro] 2019-04-12T10:29:43.115-0400 d20020| 2019-04-12T10:29:43.115-0400 I  STORAGE  [monitoring-keys-for-HMAC] Triggering the first stable checkpoint. Initial Data: Timestamp(1555079380, 1) PrevStable: Timestamp(0, 0) CurrStable: Timestamp(1555079383, 5)
[js_test:shouldworkRepro] 2019-04-12T10:29:43.136-0400 d20021| 2019-04-12T10:29:43.136-0400 I  INDEX    [replexec-0] index build: done building index _id_ on ns local.system.replset
[js_test:shouldworkRepro] 2019-04-12T10:29:43.136-0400 d20021| 2019-04-12T10:29:43.136-0400 I  REPL     [replexec-0] New replica set config in use: { _id: "InitialSyncTest", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "vladmac:20020", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "vladmac:20021", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 0.0, tags: {}, slaveDelay: 0, votes: 0 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5cb0a0d43e462239f4270e2b') } }
[js_test:shouldworkRepro] 2019-04-12T10:29:43.136-0400 d20021| 2019-04-12T10:29:43.136-0400 I  REPL     [replexec-0] This node is vladmac:20021 in the config
[js_test:shouldworkRepro] 2019-04-12T10:29:43.136-0400 d20021| 2019-04-12T10:29:43.136-0400 I  REPL     [replexec-0] transition to STARTUP2 from STARTUP
[js_test:shouldworkRepro] 2019-04-12T10:29:43.136-0400 d20021| 2019-04-12T10:29:43.136-0400 I  REPL     [replexec-0] Starting replication storage threads
[js_test:shouldworkRepro] 2019-04-12T10:29:43.137-0400 d20021| 2019-04-12T10:29:43.136-0400 I  REPL     [replexec-2] Member vladmac:20020 is now in state PRIMARY
[js_test:shouldworkRepro] 2019-04-12T10:29:43.143-0400 d20021| 2019-04-12T10:29:43.143-0400 I  STORAGE  [replexec-0] createCollection: local.temp_oplog_buffer with generated UUID: 31723062-af89-4b08-8505-a48ec18fb12d
[js_test:shouldworkRepro] 2019-04-12T10:29:43.192-0400 d20021| 2019-04-12T10:29:43.192-0400 I  INDEX    [replexec-0] index build: done building index _id_ on ns local.temp_oplog_buffer
[js_test:shouldworkRepro] 2019-04-12T10:29:43.193-0400 d20021| 2019-04-12T10:29:43.192-0400 I  INITSYNC [replication-0] Starting initial sync (attempt 1 of 10)
[js_test:shouldworkRepro] 2019-04-12T10:29:43.193-0400 d20021| 2019-04-12T10:29:43.193-0400 I  STORAGE  [replication-0] Finishing collection drop for local.temp_oplog_buffer (31723062-af89-4b08-8505-a48ec18fb12d).
[js_test:shouldworkRepro] 2019-04-12T10:29:43.223-0400 d20021| 2019-04-12T10:29:43.222-0400 I  STORAGE  [replication-0] createCollection: local.temp_oplog_buffer with generated UUID: 3bd2df98-fd8f-45c5-bce7-9ec9dd7bdefe
[js_test:shouldworkRepro] 2019-04-12T10:29:43.286-0400 d20021| 2019-04-12T10:29:43.285-0400 I  INDEX    [replication-0] index build: done building index _id_ on ns local.temp_oplog_buffer
[js_test:shouldworkRepro] 2019-04-12T10:29:43.286-0400 d20021| 2019-04-12T10:29:43.286-0400 I  REPL     [replication-0] sync source candidate: vladmac:20020
[js_test:shouldworkRepro] 2019-04-12T10:29:43.286-0400 d20021| 2019-04-12T10:29:43.286-0400 I  INITSYNC [replication-0] Initial syncer oplog truncation finished in: 0ms
[js_test:shouldworkRepro] 2019-04-12T10:29:43.286-0400 d20021| 2019-04-12T10:29:43.286-0400 I  REPL     [replication-0] ******
[js_test:shouldworkRepro] 2019-04-12T10:29:43.286-0400 d20021| 2019-04-12T10:29:43.286-0400 I  REPL     [replication-0] creating replication oplog of size: 40MB...
[js_test:shouldworkRepro] 2019-04-12T10:29:43.286-0400 d20021| 2019-04-12T10:29:43.286-0400 I  STORAGE  [replication-0] createCollection: local.oplog.rs with generated UUID: ca7e20ea-6b89-468b-926b-a7c7bf503865
[js_test:shouldworkRepro] 2019-04-12T10:29:43.308-0400 d20021| 2019-04-12T10:29:43.308-0400 I  STORAGE  [replication-0] Starting OplogTruncaterThread local.oplog.rs
[js_test:shouldworkRepro] 2019-04-12T10:29:43.308-0400 d20021| 2019-04-12T10:29:43.308-0400 I  STORAGE  [replication-0] The size storer reports that the oplog contains 0 records totaling to 0 bytes
[js_test:shouldworkRepro] 2019-04-12T10:29:43.308-0400 d20021| 2019-04-12T10:29:43.308-0400 I  STORAGE  [replication-0] Scanning the oplog to determine where to place markers for truncation
[js_test:shouldworkRepro] 2019-04-12T10:29:43.414-0400 d20021| 2019-04-12T10:29:43.414-0400 I  REPL     [replication-0] ******
[js_test:shouldworkRepro] 2019-04-12T10:29:43.414-0400 d20021| 2019-04-12T10:29:43.414-0400 I  REPL     [replication-0] dropReplicatedDatabases - dropping 1 databases
[js_test:shouldworkRepro] 2019-04-12T10:29:43.415-0400 d20021| 2019-04-12T10:29:43.414-0400 I  REPL     [replication-0] dropReplicatedDatabases - dropped 1 databases
[js_test:shouldworkRepro] 2019-04-12T10:29:43.416-0400 d20020| 2019-04-12T10:29:43.416-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61846 #6 (3 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:43.417-0400 d20020| 2019-04-12T10:29:43.417-0400 I  NETWORK  [conn6] received client metadata from 127.0.0.1:61846 conn6: { driver: { name: "NetworkInterfaceTL", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:43.419-0400 d20020| 2019-04-12T10:29:43.419-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61847 #7 (4 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:43.419-0400 d20021| 2019-04-12T10:29:43.419-0400 I  INITSYNC [replication-1] CollectionCloner::start called, on ns:admin.system.keys
[js_test:shouldworkRepro] 2019-04-12T10:29:43.419-0400 d20020| 2019-04-12T10:29:43.419-0400 I  NETWORK  [conn7] received client metadata from 127.0.0.1:61847 conn7: { driver: { name: "NetworkInterfaceTL", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:43.420-0400 d20021| 2019-04-12T10:29:43.420-0400 I  STORAGE  [repl-writer-worker-0] createCollection: admin.system.keys with provided UUID: 344947af-588d-4a6c-95ed-fc25d1d38bfc
[js_test:shouldworkRepro] 2019-04-12T10:29:43.448-0400 d20021| 2019-04-12T10:29:43.448-0400 I  INDEX    [repl-writer-worker-0] index build: starting on admin.system.keys properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.keys" } using method: Foreground
[js_test:shouldworkRepro] 2019-04-12T10:29:43.448-0400 d20021| 2019-04-12T10:29:43.448-0400 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 500 megabytes of RAM
[js_test:shouldworkRepro] 2019-04-12T10:29:43.449-0400 d20020| 2019-04-12T10:29:43.449-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61848 #8 (5 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:43.449-0400 d20020| 2019-04-12T10:29:43.449-0400 I  NETWORK  [conn8] received client metadata from 127.0.0.1:61848 conn8: { driver: { name: "MongoDB Internal Client", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:43.450-0400 d20021| 2019-04-12T10:29:43.450-0400 I  INITSYNC [replication-0] CollectionCloner ns:admin.system.keys finished cloning with status: OK
[js_test:shouldworkRepro] 2019-04-12T10:29:43.450-0400 d20020| 2019-04-12T10:29:43.450-0400 I  NETWORK  [conn8] end connection 127.0.0.1:61848 (4 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:43.450-0400 d20021| 2019-04-12T10:29:43.450-0400 I  INDEX    [replication-0] index build: inserted 2 keys from external sorter into index in 0 seconds
[js_test:shouldworkRepro] 2019-04-12T10:29:43.456-0400 d20021| 2019-04-12T10:29:43.456-0400 I  INDEX    [replication-0] index build: done building index _id_ on ns admin.system.keys
[js_test:shouldworkRepro] 2019-04-12T10:29:43.456-0400 d20021| 2019-04-12T10:29:43.456-0400 I  INITSYNC [replication-0] CollectionCloner::start called, on ns:admin.system.version
[js_test:shouldworkRepro] 2019-04-12T10:29:43.457-0400 d20021| 2019-04-12T10:29:43.457-0400 I  STORAGE  [repl-writer-worker-2] createCollection: admin.system.version with provided UUID: 79fe740f-34c2-436a-b791-66b5f56f6718
[js_test:shouldworkRepro] 2019-04-12T10:29:43.488-0400 d20021| 2019-04-12T10:29:43.488-0400 I  INDEX    [repl-writer-worker-2] index build: starting on admin.system.version properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.version" } using method: Foreground
[js_test:shouldworkRepro] 2019-04-12T10:29:43.488-0400 d20021| 2019-04-12T10:29:43.488-0400 I  INDEX    [repl-writer-worker-2] build may temporarily use up to 500 megabytes of RAM
[js_test:shouldworkRepro] 2019-04-12T10:29:43.490-0400 d20020| 2019-04-12T10:29:43.490-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61849 #9 (5 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:43.490-0400 d20020| 2019-04-12T10:29:43.490-0400 I  NETWORK  [conn9] received client metadata from 127.0.0.1:61849 conn9: { driver: { name: "MongoDB Internal Client", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:43.491-0400 d20021| 2019-04-12T10:29:43.491-0400 I  COMMAND  [repl-writer-worker-3] setting featureCompatibilityVersion to 4.2
[js_test:shouldworkRepro] 2019-04-12T10:29:43.491-0400 d20021| 2019-04-12T10:29:43.491-0400 I  NETWORK  [repl-writer-worker-3] Skip closing connection for connection # 3
[js_test:shouldworkRepro] 2019-04-12T10:29:43.491-0400 d20021| 2019-04-12T10:29:43.491-0400 I  NETWORK  [repl-writer-worker-3] Skip closing connection for connection # 1
[js_test:shouldworkRepro] 2019-04-12T10:29:43.491-0400 d20021| 2019-04-12T10:29:43.491-0400 I  INITSYNC [replication-1] CollectionCloner ns:admin.system.version finished cloning with status: OK
[js_test:shouldworkRepro] 2019-04-12T10:29:43.492-0400 d20020| 2019-04-12T10:29:43.491-0400 I  NETWORK  [conn9] end connection 127.0.0.1:61849 (4 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:43.492-0400 d20021| 2019-04-12T10:29:43.492-0400 I  INDEX    [replication-1] index build: inserted 1 keys from external sorter into index in 0 seconds
[js_test:shouldworkRepro] 2019-04-12T10:29:43.499-0400 d20021| 2019-04-12T10:29:43.499-0400 I  INDEX    [replication-1] index build: done building index _id_ on ns admin.system.version
[js_test:shouldworkRepro] 2019-04-12T10:29:43.501-0400 d20021| 2019-04-12T10:29:43.501-0400 I  INITSYNC [replication-0] CollectionCloner::start called, on ns:config.transactions
[js_test:shouldworkRepro] 2019-04-12T10:29:43.502-0400 d20021| 2019-04-12T10:29:43.502-0400 I  STORAGE  [repl-writer-worker-4] createCollection: config.transactions with provided UUID: a6348a5e-ec46-4ef4-9436-f5d70a411e91
[js_test:shouldworkRepro] 2019-04-12T10:29:43.552-0400 d20021| 2019-04-12T10:29:43.552-0400 I  INDEX    [repl-writer-worker-4] index build: starting on config.transactions properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "config.transactions" } using method: Hybrid
[js_test:shouldworkRepro] 2019-04-12T10:29:43.552-0400 d20021| 2019-04-12T10:29:43.552-0400 I  INDEX    [repl-writer-worker-4] build may temporarily use up to 500 megabytes of RAM
[js_test:shouldworkRepro] 2019-04-12T10:29:43.554-0400 d20020| 2019-04-12T10:29:43.554-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61850 #10 (5 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:43.555-0400 d20020| 2019-04-12T10:29:43.554-0400 I  NETWORK  [conn10] received client metadata from 127.0.0.1:61850 conn10: { driver: { name: "MongoDB Internal Client", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:43.555-0400 d20021| 2019-04-12T10:29:43.555-0400 I  INITSYNC [replication-1] CollectionCloner ns:config.transactions finished cloning with status: OK
[js_test:shouldworkRepro] 2019-04-12T10:29:43.555-0400 d20020| 2019-04-12T10:29:43.555-0400 I  NETWORK  [conn10] end connection 127.0.0.1:61850 (4 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:43.556-0400 d20021| 2019-04-12T10:29:43.556-0400 I  INDEX    [replication-1] index build: inserted 0 keys from external sorter into index in 0 seconds
[js_test:shouldworkRepro] 2019-04-12T10:29:43.561-0400 d20021| 2019-04-12T10:29:43.561-0400 I  INDEX    [replication-1] index build: done building index _id_ on ns config.transactions
[js_test:shouldworkRepro] 2019-04-12T10:29:43.576-0400 d20021| 2019-04-12T10:29:43.576-0400 I  INITSYNC [replication-1] Finished cloning data: OK. Beginning oplog replay.
[js_test:shouldworkRepro] 2019-04-12T10:29:43.576-0400 d20021| 2019-04-12T10:29:43.576-0400 I  INITSYNC [replication-0] No need to apply operations. (currently at { : Timestamp(1555079383, 6) })
[js_test:shouldworkRepro] 2019-04-12T10:29:43.577-0400 d20021| 2019-04-12T10:29:43.577-0400 I  SHARDING [replication-1] Marking collection local.temp_oplog_buffer as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:43.577-0400 d20021| 2019-04-12T10:29:43.577-0400 I  SHARDING [replication-1] Marking collection local.replset.oplogTruncateAfterPoint as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:43.577-0400 d20021| 2019-04-12T10:29:43.577-0400 I  INITSYNC [replication-0] Finished fetching oplog during initial sync: CallbackCanceled: error in fetcher batch callback: oplog fetcher is shutting down. Last fetched optime: { ts: Timestamp(0, 0), t: -1 }
[js_test:shouldworkRepro] 2019-04-12T10:29:43.577-0400 d20021| 2019-04-12T10:29:43.577-0400 I  INITSYNC [replication-0] Initial sync attempt finishing up.
[js_test:shouldworkRepro] 2019-04-12T10:29:43.577-0400 d20021| 2019-04-12T10:29:43.577-0400 I  INITSYNC [replication-0] Initial Sync Attempt Statistics: { failedInitialSyncAttempts: 0, maxFailedInitialSyncAttempts: 10, initialSyncStart: new Date(1555079383192), initialSyncAttempts: [], fetchedMissingDocs: 0, appliedOps: 0, initialSyncOplogStart: Timestamp(1555079383, 6), initialSyncOplogEnd: Timestamp(1555079383, 6), databases: { databasesCloned: 2, admin: { collections: 2, clonedCollections: 2, start: new Date(1555079383419), end: new Date(1555079383500), elapsedMillis: 81, admin.system.keys: { documentsToCopy: 2, documentsCopied: 2, indexes: 1, fetchedBatches: 1, start: new Date(1555079383419), end: new Date(1555079383456), elapsedMillis: 37, receivedBatches: 1 }, admin.system.version: { documentsToCopy: 1, documentsCopied: 1, indexes: 1, fetchedBatches: 1, start: new Date(1555079383456), end: new Date(1555079383500), elapsedMillis: 44, receivedBatches: 1 } }, config: { collections: 1, clonedCollections: 1, start: new Date(1555079383500), end: new Date(1555079383576), elapsedMillis: 76, config.transactions: { documentsToCopy: 0, documentsCopied: 0, indexes: 1, fetchedBatches: 0, start: new Date(1555079383501), end: new Date(1555079383576), elapsedMillis: 75, receivedBatches: 0 } } } }
[js_test:shouldworkRepro] 2019-04-12T10:29:43.578-0400 d20021| 2019-04-12T10:29:43.577-0400 I  STORAGE  [replication-0] Finishing collection drop for local.temp_oplog_buffer (3bd2df98-fd8f-45c5-bce7-9ec9dd7bdefe).
[js_test:shouldworkRepro] 2019-04-12T10:29:43.597-0400 d20021| 2019-04-12T10:29:43.597-0400 I  INITSYNC [replication-0] initial sync done; took 0s.
[js_test:shouldworkRepro] 2019-04-12T10:29:43.597-0400 d20021| 2019-04-12T10:29:43.597-0400 I  REPL     [replication-0] transition to RECOVERING from STARTUP2
[js_test:shouldworkRepro] 2019-04-12T10:29:43.597-0400 d20021| 2019-04-12T10:29:43.597-0400 I  REPL     [replication-0] Starting replication fetcher thread
[js_test:shouldworkRepro] 2019-04-12T10:29:43.597-0400 d20021| 2019-04-12T10:29:43.597-0400 I  REPL     [replication-0] Starting replication applier thread
[js_test:shouldworkRepro] 2019-04-12T10:29:43.597-0400 d20021| 2019-04-12T10:29:43.597-0400 I  REPL     [replication-0] Starting replication reporter thread
[js_test:shouldworkRepro] 2019-04-12T10:29:43.597-0400 d20021| 2019-04-12T10:29:43.597-0400 I  REPL     [rsSync-0] Starting oplog application
[js_test:shouldworkRepro] 2019-04-12T10:29:43.597-0400 d20021| 2019-04-12T10:29:43.597-0400 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
[js_test:shouldworkRepro] 2019-04-12T10:29:43.597-0400 d20021| 2019-04-12T10:29:43.597-0400 I  REPL     [rsSync-0] Resetting sync source to empty, which was :27017
[js_test:shouldworkRepro] 2019-04-12T10:29:43.598-0400 d20021| 2019-04-12T10:29:43.597-0400 I  REPL     [rsBackgroundSync] could not find member to sync from
[js_test:shouldworkRepro] 2019-04-12T10:29:43.598-0400 d20021| 2019-04-12T10:29:43.598-0400 I  STORAGE  [replexec-0] Triggering the first stable checkpoint. Initial Data: Timestamp(1555079383, 6) PrevStable: Timestamp(0, 0) CurrStable: Timestamp(1555079383, 6)
[js_test:shouldworkRepro] 2019-04-12T10:29:43.718-0400 AwaitNodesAgreeOnPrimary: Waiting for nodes to agree on any primary.
[js_test:shouldworkRepro] 2019-04-12T10:29:43.719-0400 AwaitNodesAgreeOnPrimary: Nodes agreed on primary vladmac:20020
[js_test:shouldworkRepro] 2019-04-12T10:29:43.720-0400 Waiting for keys to sign $clusterTime to be generated
[js_test:shouldworkRepro] 2019-04-12T10:29:43.724-0400 AwaitLastStableRecoveryTimestamp: Beginning for [ "vladmac:20020", "vladmac:20021" ]
[js_test:shouldworkRepro] 2019-04-12T10:29:43.724-0400 AwaitNodesAgreeOnPrimary: Waiting for nodes to agree on any primary.
[js_test:shouldworkRepro] 2019-04-12T10:29:43.725-0400 AwaitNodesAgreeOnPrimary: Nodes agreed on primary vladmac:20020
[js_test:shouldworkRepro] 2019-04-12T10:29:43.726-0400 AwaitLastStableRecoveryTimestamp: ensuring the commit point advances for [ "vladmac:20020", "vladmac:20021" ]
[js_test:shouldworkRepro] 2019-04-12T10:29:43.732-0400 AwaitLastStableRecoveryTimestamp: Waiting for stable recovery timestamps for [ "vladmac:20020", "vladmac:20021" ]
[js_test:shouldworkRepro] 2019-04-12T10:29:43.734-0400 AwaitLastStableRecoveryTimestamp: A stable recovery timestamp has successfully established on [ "vladmac:20020", "vladmac:20021" ]
[js_test:shouldworkRepro] 2019-04-12T10:29:43.739-0400 ReplSetTest awaitReplication: starting: optime for primary, vladmac:20020, is { "ts" : Timestamp(1555079383, 8), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:43.739-0400 ReplSetTest awaitReplication: checking secondaries against latest primary optime { "ts" : Timestamp(1555079383, 8), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:43.740-0400 ReplSetTest awaitReplication: checking secondary #1: vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:43.741-0400 ReplSetTest awaitReplication: optime for secondary #1, vladmac:20021, is { "ts" : Timestamp(1555079383, 6), "t" : NumberLong(1) } but latest is { "ts" : Timestamp(1555079383, 8), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:43.741-0400 ReplSetTest awaitReplication: secondary #1, vladmac:20021, is NOT synced
[js_test:shouldworkRepro] 2019-04-12T10:29:43.945-0400 ReplSetTest awaitReplication: checking secondaries against latest primary optime { "ts" : Timestamp(1555079383, 8), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:43.948-0400 ReplSetTest awaitReplication: checking secondary #1: vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:43.950-0400 ReplSetTest awaitReplication: optime for secondary #1, vladmac:20021, is { "ts" : Timestamp(1555079383, 6), "t" : NumberLong(1) } but latest is { "ts" : Timestamp(1555079383, 8), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:43.950-0400 ReplSetTest awaitReplication: secondary #1, vladmac:20021, is NOT synced
[js_test:shouldworkRepro] 2019-04-12T10:29:44.152-0400 ReplSetTest awaitReplication: checking secondaries against latest primary optime { "ts" : Timestamp(1555079383, 8), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:44.156-0400 ReplSetTest awaitReplication: checking secondary #1: vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:44.158-0400 ReplSetTest awaitReplication: optime for secondary #1, vladmac:20021, is { "ts" : Timestamp(1555079383, 6), "t" : NumberLong(1) } but latest is { "ts" : Timestamp(1555079383, 8), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:44.158-0400 ReplSetTest awaitReplication: secondary #1, vladmac:20021, is NOT synced
[js_test:shouldworkRepro] 2019-04-12T10:29:44.361-0400 ReplSetTest awaitReplication: checking secondaries against latest primary optime { "ts" : Timestamp(1555079383, 8), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:44.365-0400 ReplSetTest awaitReplication: checking secondary #1: vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:44.367-0400 ReplSetTest awaitReplication: optime for secondary #1, vladmac:20021, is { "ts" : Timestamp(1555079383, 6), "t" : NumberLong(1) } but latest is { "ts" : Timestamp(1555079383, 8), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:44.367-0400 ReplSetTest awaitReplication: secondary #1, vladmac:20021, is NOT synced
[js_test:shouldworkRepro] 2019-04-12T10:29:44.569-0400 ReplSetTest awaitReplication: checking secondaries against latest primary optime { "ts" : Timestamp(1555079383, 8), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:44.572-0400 ReplSetTest awaitReplication: checking secondary #1: vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:44.574-0400 ReplSetTest awaitReplication: optime for secondary #1, vladmac:20021, is { "ts" : Timestamp(1555079383, 6), "t" : NumberLong(1) } but latest is { "ts" : Timestamp(1555079383, 8), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:44.574-0400 ReplSetTest awaitReplication: secondary #1, vladmac:20021, is NOT synced
[js_test:shouldworkRepro] 2019-04-12T10:29:44.601-0400 d20021| 2019-04-12T10:29:44.601-0400 I  REPL     [rsBackgroundSync] sync source candidate: vladmac:20020
[js_test:shouldworkRepro] 2019-04-12T10:29:44.602-0400 d20021| 2019-04-12T10:29:44.602-0400 I  REPL     [rsBackgroundSync] Changed sync source from empty to vladmac:20020
[js_test:shouldworkRepro] 2019-04-12T10:29:44.603-0400 d20021| 2019-04-12T10:29:44.603-0400 I  REPL     [rsBackgroundSync] scheduling fetcher to read remote oplog on vladmac:20020 starting at filter: { ts: { $gte: Timestamp(1555079383, 6) } }
[js_test:shouldworkRepro] 2019-04-12T10:29:44.603-0400 d20021| 2019-04-12T10:29:44.603-0400 I  CONNPOOL [RS] Connecting to vladmac:20020
[js_test:shouldworkRepro] 2019-04-12T10:29:44.609-0400 d20020| 2019-04-12T10:29:44.609-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61851 #11 (5 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:44.610-0400 d20020| 2019-04-12T10:29:44.609-0400 I  NETWORK  [conn11] received client metadata from 127.0.0.1:61851 conn11: { driver: { name: "NetworkInterfaceTL", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:44.775-0400 ReplSetTest awaitReplication: checking secondaries against latest primary optime { "ts" : Timestamp(1555079383, 8), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:44.776-0400 ReplSetTest awaitReplication: checking secondary #1: vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:44.777-0400 ReplSetTest awaitReplication: secondary #1, vladmac:20021, is synced
[js_test:shouldworkRepro] 2019-04-12T10:29:44.777-0400 ReplSetTest awaitReplication: finished: all 1 secondaries synced at optime { "ts" : Timestamp(1555079383, 8), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:44.779-0400 AwaitNodesAgreeOnAppliedOpTime: Retrying because node vladmac:20020 sees optime { "ts" : Timestamp(0, 0), "t" : NumberLong(-1) } on node vladmac:20021 but expects to see optime { "ts" : Timestamp(1555079383, 8), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:44.987-0400 AwaitNodesAgreeOnAppliedOpTime: Retrying because node vladmac:20020 sees optime { "ts" : Timestamp(0, 0), "t" : NumberLong(-1) } on node vladmac:20021 but expects to see optime { "ts" : Timestamp(1555079383, 8), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:45.090-0400 d20020| 2019-04-12T10:29:45.089-0400 I  REPL     [replexec-2] Member vladmac:20021 is now in state SECONDARY
[js_test:shouldworkRepro] 2019-04-12T10:29:45.195-0400 AwaitNodesAgreeOnAppliedOpTime: All nodes agree that all ops are applied up to { "ts" : Timestamp(1555079383, 8), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:45.195-0400 AwaitNodesAgreeOnPrimary: Waiting for nodes to agree on any primary.
[js_test:shouldworkRepro] 2019-04-12T10:29:45.196-0400 AwaitNodesAgreeOnPrimary: Nodes agreed on primary vladmac:20020
[js_test:shouldworkRepro] 2019-04-12T10:29:45.201-0400 ReplSetTest waitForIndicator state on connection to vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:45.201-0400 [ 2 ]
[js_test:shouldworkRepro] 2019-04-12T10:29:45.201-0400 ReplSetTest waitForIndicator from node connection to vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:45.203-0400 ReplSetTest waitForIndicator Initial status (timeout : 600000) :
[js_test:shouldworkRepro] 2019-04-12T10:29:45.204-0400 {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.204-0400 	"set" : "InitialSyncTest",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.204-0400 	"date" : ISODate("2019-04-12T14:29:45.203Z"),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.205-0400 	"myState" : 1,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.205-0400 	"term" : NumberLong(1),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.205-0400 	"syncingTo" : "",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.205-0400 	"syncSourceHost" : "",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.205-0400 	"syncSourceId" : -1,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.205-0400 	"heartbeatIntervalMillis" : NumberLong(2000),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.205-0400 	"optimes" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.205-0400 		"lastCommittedOpTime" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.205-0400 			"ts" : Timestamp(1555079383, 8),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.205-0400 			"t" : NumberLong(1)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.205-0400 		},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.205-0400 		"readConcernMajorityOpTime" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.205-0400 			"ts" : Timestamp(1555079383, 8),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.205-0400 			"t" : NumberLong(1)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.205-0400 		},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.205-0400 		"appliedOpTime" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.206-0400 			"ts" : Timestamp(1555079383, 8),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.206-0400 			"t" : NumberLong(1)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.206-0400 		},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.206-0400 		"durableOpTime" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.206-0400 			"ts" : Timestamp(1555079383, 8),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.206-0400 			"t" : NumberLong(1)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.206-0400 		},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.206-0400 		"lastAppliedWallTime" : ISODate("2019-04-12T14:29:43.732Z"),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.206-0400 		"lastDurableWallTime" : ISODate("2019-04-12T14:29:43.732Z")
[js_test:shouldworkRepro] 2019-04-12T10:29:45.206-0400 	},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.206-0400 	"lastStableRecoveryTimestamp" : Timestamp(1555079383, 5),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.206-0400 	"lastStableCheckpointTimestamp" : Timestamp(1555079383, 5),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.206-0400 	"members" : [
[js_test:shouldworkRepro] 2019-04-12T10:29:45.206-0400 		{
[js_test:shouldworkRepro] 2019-04-12T10:29:45.206-0400 			"_id" : 0,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.206-0400 			"name" : "vladmac:20020",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.206-0400 			"health" : 1,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.207-0400 			"state" : 1,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.207-0400 			"stateStr" : "PRIMARY",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.207-0400 			"uptime" : 6,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.207-0400 			"optime" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.207-0400 				"ts" : Timestamp(1555079383, 8),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.207-0400 				"t" : NumberLong(1)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.207-0400 			},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.207-0400 			"optimeDate" : ISODate("2019-04-12T14:29:43Z"),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.207-0400 			"syncingTo" : "",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.207-0400 			"syncSourceHost" : "",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.207-0400 			"syncSourceId" : -1,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.207-0400 			"infoMessage" : "could not find member to sync from",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.207-0400 			"electionTime" : Timestamp(1555079381, 1),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.207-0400 			"electionDate" : ISODate("2019-04-12T14:29:41Z"),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.207-0400 			"configVersion" : 2,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.207-0400 			"self" : true,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.207-0400 			"lastHeartbeatMessage" : ""
[js_test:shouldworkRepro] 2019-04-12T10:29:45.207-0400 		},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.208-0400 		{
[js_test:shouldworkRepro] 2019-04-12T10:29:45.208-0400 			"_id" : 1,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.208-0400 			"name" : "vladmac:20021",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.208-0400 			"health" : 1,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.208-0400 			"state" : 2,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.208-0400 			"stateStr" : "SECONDARY",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.208-0400 			"uptime" : 2,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.208-0400 			"optime" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.208-0400 				"ts" : Timestamp(1555079383, 8),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.208-0400 				"t" : NumberLong(1)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.208-0400 			},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.208-0400 			"optimeDurable" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.208-0400 				"ts" : Timestamp(1555079383, 8),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.208-0400 				"t" : NumberLong(1)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.208-0400 			},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.208-0400 			"optimeDate" : ISODate("2019-04-12T14:29:43Z"),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.208-0400 			"optimeDurableDate" : ISODate("2019-04-12T14:29:43Z"),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.208-0400 			"lastHeartbeat" : ISODate("2019-04-12T14:29:45.089Z"),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.209-0400 			"lastHeartbeatRecv" : ISODate("2019-04-12T14:29:44.601Z"),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.209-0400 			"pingMs" : NumberLong(0),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.209-0400 			"lastHeartbeatMessage" : "",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.209-0400 			"syncingTo" : "vladmac:20020",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.209-0400 			"syncSourceHost" : "vladmac:20020",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.209-0400 			"syncSourceId" : 0,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.209-0400 			"infoMessage" : "",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.209-0400 			"configVersion" : 2
[js_test:shouldworkRepro] 2019-04-12T10:29:45.209-0400 		}
[js_test:shouldworkRepro] 2019-04-12T10:29:45.209-0400 	],
[js_test:shouldworkRepro] 2019-04-12T10:29:45.209-0400 	"ok" : 1,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.209-0400 	"$clusterTime" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.209-0400 		"clusterTime" : Timestamp(1555079383, 8),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.209-0400 		"signature" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.209-0400 			"hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.209-0400 			"keyId" : NumberLong(0)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.209-0400 		}
[js_test:shouldworkRepro] 2019-04-12T10:29:45.210-0400 	},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.210-0400 	"operationTime" : Timestamp(1555079383, 8)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.210-0400 }
[js_test:shouldworkRepro] 2019-04-12T10:29:45.210-0400 Status for : vladmac:20020, checking vladmac:20021/vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:45.210-0400 Status for : vladmac:20021, checking vladmac:20021/vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:45.210-0400 Status --  current state: 2,  target state : 2
[js_test:shouldworkRepro] 2019-04-12T10:29:45.210-0400 d20021| 2019-04-12T10:29:45.205-0400 I  SHARDING [conn1] Marking collection foo.bar as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:45.210-0400 ReplSetTest waitForIndicator final status:
[js_test:shouldworkRepro] 2019-04-12T10:29:45.210-0400 {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.210-0400 	"set" : "InitialSyncTest",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.210-0400 	"date" : ISODate("2019-04-12T14:29:45.203Z"),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.210-0400 	"myState" : 1,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.210-0400 	"term" : NumberLong(1),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.210-0400 	"syncingTo" : "",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.210-0400 	"syncSourceHost" : "",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.210-0400 	"syncSourceId" : -1,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.210-0400 	"heartbeatIntervalMillis" : NumberLong(2000),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.210-0400 	"optimes" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.210-0400 		"lastCommittedOpTime" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.210-0400 			"ts" : Timestamp(1555079383, 8),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.211-0400 			"t" : NumberLong(1)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.211-0400 		},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.211-0400 		"readConcernMajorityOpTime" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.211-0400 			"ts" : Timestamp(1555079383, 8),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.211-0400 			"t" : NumberLong(1)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.211-0400 		},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.211-0400 		"appliedOpTime" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.211-0400 			"ts" : Timestamp(1555079383, 8),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.211-0400 			"t" : NumberLong(1)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.211-0400 		},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.211-0400 		"durableOpTime" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.211-0400 			"ts" : Timestamp(1555079383, 8),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.211-0400 			"t" : NumberLong(1)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.211-0400 		},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.211-0400 		"lastAppliedWallTime" : ISODate("2019-04-12T14:29:43.732Z"),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.211-0400 		"lastDurableWallTime" : ISODate("2019-04-12T14:29:43.732Z")
[js_test:shouldworkRepro] 2019-04-12T10:29:45.211-0400 	},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.211-0400 	"lastStableRecoveryTimestamp" : Timestamp(1555079383, 5),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.211-0400 	"lastStableCheckpointTimestamp" : Timestamp(1555079383, 5),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.212-0400 	"members" : [
[js_test:shouldworkRepro] 2019-04-12T10:29:45.212-0400 		{
[js_test:shouldworkRepro] 2019-04-12T10:29:45.212-0400 			"_id" : 0,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.212-0400 			"name" : "vladmac:20020",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.212-0400 			"health" : 1,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.212-0400 			"state" : 1,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.212-0400 			"stateStr" : "PRIMARY",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.212-0400 			"uptime" : 6,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.212-0400 			"optime" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.212-0400 				"ts" : Timestamp(1555079383, 8),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.212-0400 				"t" : NumberLong(1)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.212-0400 			},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.212-0400 			"optimeDate" : ISODate("2019-04-12T14:29:43Z"),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.212-0400 			"syncingTo" : "",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.212-0400 			"syncSourceHost" : "",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.212-0400 			"syncSourceId" : -1,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.212-0400 			"infoMessage" : "could not find member to sync from",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.212-0400 			"electionTime" : Timestamp(1555079381, 1),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.212-0400 			"electionDate" : ISODate("2019-04-12T14:29:41Z"),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.212-0400 			"configVersion" : 2,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.213-0400 			"self" : true,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.213-0400 			"lastHeartbeatMessage" : ""
[js_test:shouldworkRepro] 2019-04-12T10:29:45.213-0400 		},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.213-0400 		{
[js_test:shouldworkRepro] 2019-04-12T10:29:45.213-0400 			"_id" : 1,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.213-0400 			"name" : "vladmac:20021",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.213-0400 			"health" : 1,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.213-0400 			"state" : 2,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.213-0400 			"stateStr" : "SECONDARY",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.213-0400 			"uptime" : 2,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.213-0400 			"optime" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.213-0400 				"ts" : Timestamp(1555079383, 8),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.213-0400 				"t" : NumberLong(1)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.213-0400 			},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.213-0400 			"optimeDurable" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.213-0400 				"ts" : Timestamp(1555079383, 8),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.213-0400 				"t" : NumberLong(1)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.213-0400 			},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.213-0400 			"optimeDate" : ISODate("2019-04-12T14:29:43Z"),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.213-0400 			"optimeDurableDate" : ISODate("2019-04-12T14:29:43Z"),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 			"lastHeartbeat" : ISODate("2019-04-12T14:29:45.089Z"),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 			"lastHeartbeatRecv" : ISODate("2019-04-12T14:29:44.601Z"),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 			"pingMs" : NumberLong(0),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 			"lastHeartbeatMessage" : "",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 			"syncingTo" : "vladmac:20020",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 			"syncSourceHost" : "vladmac:20020",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 			"syncSourceId" : 0,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 			"infoMessage" : "",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 			"configVersion" : 2
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 		}
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 	],
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 	"ok" : 1,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 	"$clusterTime" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 		"clusterTime" : Timestamp(1555079383, 8),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 		"signature" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 			"hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 			"keyId" : NumberLong(0)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 		}
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 	},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 	"operationTime" : Timestamp(1555079383, 8)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 }
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:45.214-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:45.215-0400 [jsTest] ----
[js_test:shouldworkRepro] 2019-04-12T10:29:45.215-0400 [jsTest] New session started with sessionID: { "id" : UUID("a820805f-d464-4be6-980c-7bff7087ff92") }
[js_test:shouldworkRepro] 2019-04-12T10:29:45.215-0400 [jsTest] ----
[js_test:shouldworkRepro] 2019-04-12T10:29:45.215-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:45.215-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:45.215-0400 d20020| 2019-04-12T10:29:45.207-0400 I  SHARDING [conn1] Marking collection db1.coll2 as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:45.215-0400 d20020| 2019-04-12T10:29:45.207-0400 I  STORAGE  [conn1] createCollection: db1.coll2 with generated UUID: ec5df8aa-a125-4399-90c2-c3f29e49bd99
[js_test:shouldworkRepro] 2019-04-12T10:29:45.232-0400 d20020| 2019-04-12T10:29:45.232-0400 I  INDEX    [conn1] index build: done building index _id_ on ns db1.coll2
[js_test:shouldworkRepro] 2019-04-12T10:29:45.233-0400 d20020| 2019-04-12T10:29:45.233-0400 I  SHARDING [conn1] Marking collection db2.coll1 as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:45.233-0400 d20020| 2019-04-12T10:29:45.233-0400 I  STORAGE  [conn1] createCollection: db2.coll1 with generated UUID: fd81e91b-6cae-471a-84ac-12035c9f2c3a
[js_test:shouldworkRepro] 2019-04-12T10:29:45.239-0400 d20021| 2019-04-12T10:29:45.239-0400 I  SHARDING [repl-writer-worker-7] Marking collection db1.coll2 as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:45.239-0400 d20021| 2019-04-12T10:29:45.239-0400 I  STORAGE  [repl-writer-worker-7] createCollection: db1.coll2 with provided UUID: ec5df8aa-a125-4399-90c2-c3f29e49bd99
[js_test:shouldworkRepro] 2019-04-12T10:29:45.283-0400 d20020| 2019-04-12T10:29:45.282-0400 I  INDEX    [conn1] index build: done building index _id_ on ns db2.coll1
[js_test:shouldworkRepro] 2019-04-12T10:29:45.284-0400 d20020| 2019-04-12T10:29:45.283-0400 I  SHARDING [conn1] Marking collection db2.coll2 as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:45.284-0400 d20020| 2019-04-12T10:29:45.283-0400 I  STORAGE  [conn1] createCollection: db2.coll2 with generated UUID: ba7ae8b6-5c7d-467a-a1e1-15c6bd96e087
[js_test:shouldworkRepro] 2019-04-12T10:29:45.288-0400 d20021| 2019-04-12T10:29:45.288-0400 I  INDEX    [repl-writer-worker-7] index build: done building index _id_ on ns db1.coll2
[js_test:shouldworkRepro] 2019-04-12T10:29:45.294-0400 d20021| 2019-04-12T10:29:45.294-0400 I  SHARDING [repl-writer-worker-9] Marking collection db2.coll1 as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:45.294-0400 d20021| 2019-04-12T10:29:45.294-0400 I  STORAGE  [repl-writer-worker-9] createCollection: db2.coll1 with provided UUID: fd81e91b-6cae-471a-84ac-12035c9f2c3a
[js_test:shouldworkRepro] 2019-04-12T10:29:45.335-0400 d20020| 2019-04-12T10:29:45.335-0400 I  INDEX    [conn1] index build: done building index _id_ on ns db2.coll2
[js_test:shouldworkRepro] 2019-04-12T10:29:45.336-0400 d20020| 2019-04-12T10:29:45.336-0400 I  SHARDING [conn1] Marking collection db1.tmpLL17D.convertToCapped.coll2 as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:45.336-0400 d20020| 2019-04-12T10:29:45.336-0400 I  STORAGE  [conn1] createCollection: db1.tmpLL17D.convertToCapped.coll2 with generated UUID: d81f3f9f-f597-47ca-9b9c-f9caf88e5544
[js_test:shouldworkRepro] 2019-04-12T10:29:45.341-0400 d20021| 2019-04-12T10:29:45.341-0400 I  INDEX    [repl-writer-worker-9] index build: done building index _id_ on ns db2.coll1
[js_test:shouldworkRepro] 2019-04-12T10:29:45.348-0400 d20021| 2019-04-12T10:29:45.348-0400 I  SHARDING [repl-writer-worker-11] Marking collection db2.coll2 as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:45.348-0400 d20021| 2019-04-12T10:29:45.348-0400 I  STORAGE  [repl-writer-worker-11] createCollection: db2.coll2 with provided UUID: ba7ae8b6-5c7d-467a-a1e1-15c6bd96e087
[js_test:shouldworkRepro] 2019-04-12T10:29:45.386-0400 d20020| 2019-04-12T10:29:45.386-0400 I  INDEX    [conn1] index build: done building index _id_ on ns db1.tmpLL17D.convertToCapped.coll2
[js_test:shouldworkRepro] 2019-04-12T10:29:45.386-0400 d20020| 2019-04-12T10:29:45.386-0400 I  COMMAND  [conn1] renameCollectionForCommand: rename db1.tmpLL17D.convertToCapped.coll2 to db1.coll2 and drop db1.coll2.
[js_test:shouldworkRepro] 2019-04-12T10:29:45.386-0400 d20020| 2019-04-12T10:29:45.386-0400 I  STORAGE  [conn1] dropCollection: db1.coll2 (ec5df8aa-a125-4399-90c2-c3f29e49bd99) - storage engine will take ownership of drop-pending collection with optime { ts: Timestamp(1555079385, 5), t: 1 } and commit timestamp Timestamp(0, 0)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.386-0400 d20020| 2019-04-12T10:29:45.386-0400 I  STORAGE  [conn1] Finishing collection drop for db1.coll2 (ec5df8aa-a125-4399-90c2-c3f29e49bd99).
[js_test:shouldworkRepro] 2019-04-12T10:29:45.387-0400 d20020| 2019-04-12T10:29:45.386-0400 I  STORAGE  [conn1] renameCollection: renaming collection d81f3f9f-f597-47ca-9b9c-f9caf88e5544 from db1.tmpLL17D.convertToCapped.coll2 to db1.coll2
[js_test:shouldworkRepro] 2019-04-12T10:29:45.387-0400 d20020| 2019-04-12T10:29:45.387-0400 I  STORAGE  [conn1] Deferring table drop for index '_id_' on collection 'db1.coll2.$_id_ (ec5df8aa-a125-4399-90c2-c3f29e49bd99)'. Ident: 'index-20--3534291432697553977', commit timestamp: 'Timestamp(1555079385, 5)'
[js_test:shouldworkRepro] 2019-04-12T10:29:45.387-0400 d20020| 2019-04-12T10:29:45.387-0400 I  STORAGE  [conn1] Deferring table drop for collection 'db1.coll2' (ec5df8aa-a125-4399-90c2-c3f29e49bd99). Ident: collection-19--3534291432697553977, commit timestamp: Timestamp(1555079385, 5)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.387-0400 ReplSetTest stop *** Shutting down mongod in port 20021 ***
[js_test:shouldworkRepro] 2019-04-12T10:29:45.388-0400 d20021| 2019-04-12T10:29:45.388-0400 I  CONTROL  [signalProcessingThread] got signal 15 (Terminated: 15), will terminate after current cmd ends
[js_test:shouldworkRepro] 2019-04-12T10:29:45.388-0400 d20021| 2019-04-12T10:29:45.388-0400 I  NETWORK  [signalProcessingThread] shutdown: going to close listening sockets...
[js_test:shouldworkRepro] 2019-04-12T10:29:45.388-0400 d20021| 2019-04-12T10:29:45.388-0400 I  NETWORK  [signalProcessingThread] removing socket file: /tmp/mongodb-20021.sock
[js_test:shouldworkRepro] 2019-04-12T10:29:45.391-0400 d20021| 2019-04-12T10:29:45.391-0400 I  REPL     [signalProcessingThread] shutting down replication subsystems
[js_test:shouldworkRepro] 2019-04-12T10:29:45.391-0400 d20021| 2019-04-12T10:29:45.391-0400 I  REPL     [signalProcessingThread] Stopping replication reporter thread
[js_test:shouldworkRepro] 2019-04-12T10:29:45.392-0400 d20021| 2019-04-12T10:29:45.391-0400 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to vladmac:20020: CallbackCanceled: Reporter no longer valid
[js_test:shouldworkRepro] 2019-04-12T10:29:45.392-0400 d20021| 2019-04-12T10:29:45.391-0400 I  REPL     [signalProcessingThread] Stopping replication fetcher thread
[js_test:shouldworkRepro] 2019-04-12T10:29:45.392-0400 d20021| 2019-04-12T10:29:45.392-0400 I  REPL     [signalProcessingThread] Stopping replication applier thread
[js_test:shouldworkRepro] 2019-04-12T10:29:45.392-0400 d20021| 2019-04-12T10:29:45.392-0400 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
[js_test:shouldworkRepro] 2019-04-12T10:29:45.392-0400 d20021| 2019-04-12T10:29:45.392-0400 I  REPL     [rsBackgroundSync] Stopping replication producer
[js_test:shouldworkRepro] 2019-04-12T10:29:45.392-0400 d20021| 2019-04-12T10:29:45.392-0400 I  INDEX    [repl-writer-worker-11] index build: done building index _id_ on ns db2.coll2
[js_test:shouldworkRepro] 2019-04-12T10:29:45.393-0400 d20021| 2019-04-12T10:29:45.393-0400 I  REPL     [rsSync-0] Finished oplog application
[js_test:shouldworkRepro] 2019-04-12T10:29:45.393-0400 d20021| 2019-04-12T10:29:45.393-0400 I  REPL     [signalProcessingThread] Stopping replication storage threads
[js_test:shouldworkRepro] 2019-04-12T10:29:45.393-0400 d20021| 2019-04-12T10:29:45.393-0400 I  ASIO     [RS] Killing all outstanding egress activity.
[js_test:shouldworkRepro] 2019-04-12T10:29:45.394-0400 d20021| 2019-04-12T10:29:45.393-0400 I  ASIO     [RS] Killing all outstanding egress activity.
[js_test:shouldworkRepro] 2019-04-12T10:29:45.394-0400 d20021| 2019-04-12T10:29:45.393-0400 I  CONNPOOL [RS] Dropping all pooled connections to vladmac:20020 due to ShutdownInProgress: Shutting down the connection pool
[js_test:shouldworkRepro] 2019-04-12T10:29:45.394-0400 d20020| 2019-04-12T10:29:45.394-0400 I  NETWORK  [conn11] end connection 127.0.0.1:61851 (4 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.395-0400 d20021| 2019-04-12T10:29:45.394-0400 I  ASIO     [Replication] Killing all outstanding egress activity.
[js_test:shouldworkRepro] 2019-04-12T10:29:45.395-0400 d20020| 2019-04-12T10:29:45.395-0400 I  NETWORK  [conn4] end connection 127.0.0.1:61844 (3 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.395-0400 d20021| 2019-04-12T10:29:45.395-0400 I  CONTROL  [signalProcessingThread] Shutting down free monitoring
[js_test:shouldworkRepro] 2019-04-12T10:29:45.395-0400 d20021| 2019-04-12T10:29:45.395-0400 I  FTDC     [signalProcessingThread] Shutting down full-time diagnostic data capture
[js_test:shouldworkRepro] 2019-04-12T10:29:45.403-0400 d20021| 2019-04-12T10:29:45.403-0400 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
[js_test:shouldworkRepro] 2019-04-12T10:29:45.403-0400 d20021| 2019-04-12T10:29:45.403-0400 I  STORAGE  [signalProcessingThread] Timestamp monitor shutting down
[js_test:shouldworkRepro] 2019-04-12T10:29:45.403-0400 d20021| 2019-04-12T10:29:45.403-0400 I  STORAGE  [signalProcessingThread] WiredTigerKVEngine shutting down
[js_test:shouldworkRepro] 2019-04-12T10:29:45.409-0400 d20021| 2019-04-12T10:29:45.409-0400 I  STORAGE  [signalProcessingThread] Shutting down session sweeper thread
[js_test:shouldworkRepro] 2019-04-12T10:29:45.409-0400 d20021| 2019-04-12T10:29:45.409-0400 I  STORAGE  [signalProcessingThread] Finished shutting down session sweeper thread
[js_test:shouldworkRepro] 2019-04-12T10:29:45.409-0400 d20021| 2019-04-12T10:29:45.409-0400 I  STORAGE  [signalProcessingThread] Shutting down journal flusher thread
[js_test:shouldworkRepro] 2019-04-12T10:29:45.473-0400 d20021| 2019-04-12T10:29:45.472-0400 I  STORAGE  [signalProcessingThread] Finished shutting down journal flusher thread
[js_test:shouldworkRepro] 2019-04-12T10:29:45.473-0400 d20021| 2019-04-12T10:29:45.472-0400 I  STORAGE  [signalProcessingThread] Shutting down checkpoint thread
[js_test:shouldworkRepro] 2019-04-12T10:29:45.473-0400 d20021| 2019-04-12T10:29:45.473-0400 I  STORAGE  [signalProcessingThread] Finished shutting down checkpoint thread
[js_test:shouldworkRepro] 2019-04-12T10:29:45.557-0400 d20021| 2019-04-12T10:29:45.557-0400 I  STORAGE  [signalProcessingThread] shutdown: removing fs lock...
[js_test:shouldworkRepro] 2019-04-12T10:29:45.558-0400 d20021| 2019-04-12T10:29:45.558-0400 I  CONTROL  [signalProcessingThread] now exiting
[js_test:shouldworkRepro] 2019-04-12T10:29:45.558-0400 d20021| 2019-04-12T10:29:45.558-0400 I  CONTROL  [signalProcessingThread] shutting down with code:0
[js_test:shouldworkRepro] 2019-04-12T10:29:45.562-0400 d20020| 2019-04-12T10:29:45.562-0400 I  NETWORK  [conn6] end connection 127.0.0.1:61846 (1 connection now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.562-0400 d20020| 2019-04-12T10:29:45.562-0400 I  NETWORK  [conn7] end connection 127.0.0.1:61847 (2 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.563-0400 2019-04-12T10:29:45.563-0400 I  -        [js] shell: stopped mongo program on port 20021
[js_test:shouldworkRepro] 2019-04-12T10:29:45.563-0400 ReplSetTest stop *** Mongod in port 20021 shutdown with code (0) ***
[js_test:shouldworkRepro] 2019-04-12T10:29:45.563-0400 ReplSetTest n is : 1
[js_test:shouldworkRepro] 2019-04-12T10:29:45.564-0400 {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.564-0400 	"useHostName" : true,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.564-0400 	"oplogSize" : 40,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.564-0400 	"keyFile" : undefined,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.564-0400 	"port" : 20021,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.564-0400 	"replSet" : "InitialSyncTest",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.564-0400 	"dbpath" : "/data/db/job0/mongorunner/InitialSyncTest-1",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.564-0400 	"restart" : true,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.564-0400 	"pathOpts" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.564-0400 		"node" : 1,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.564-0400 		"set" : "InitialSyncTest",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.564-0400 		"port" : "20021",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.564-0400 		"runId" : "5cb0a0d4faa4595868dc38b4",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.565-0400 		"dbpath" : "/data/db/job0/mongorunner/InitialSyncTest-1"
[js_test:shouldworkRepro] 2019-04-12T10:29:45.565-0400 	},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.565-0400 	"setParameter" : {
[js_test:shouldworkRepro] 2019-04-12T10:29:45.565-0400 		"failpoint.initialSyncFuzzerSynchronizationPoint1" : "{ \"mode\" : \"alwaysOn\" }",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.565-0400 		"writePeriodicNoops" : false,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.565-0400 		"numInitialSyncConnectAttempts" : 60
[js_test:shouldworkRepro] 2019-04-12T10:29:45.565-0400 	},
[js_test:shouldworkRepro] 2019-04-12T10:29:45.565-0400 	"remember" : false,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.565-0400 	"runId" : ObjectId("5cb0a0d4faa4595868dc38b4"),
[js_test:shouldworkRepro] 2019-04-12T10:29:45.565-0400 	"waitForConnect" : true,
[js_test:shouldworkRepro] 2019-04-12T10:29:45.565-0400 	"bind_ip" : "0.0.0.0",
[js_test:shouldworkRepro] 2019-04-12T10:29:45.565-0400 	"startClean" : true
[js_test:shouldworkRepro] 2019-04-12T10:29:45.565-0400 }
[js_test:shouldworkRepro] 2019-04-12T10:29:45.565-0400 ReplSetTest (Re)Starting....
[js_test:shouldworkRepro] 2019-04-12T10:29:45.565-0400 Resetting db path '/data/db/job0/mongorunner/InitialSyncTest-1'
[js_test:shouldworkRepro] 2019-04-12T10:29:45.571-0400 2019-04-12T10:29:45.571-0400 I  -        [js] shell: started program (sh61778):  /Users/vrachev/mongoDevelop/mongo/mongod --oplogSize 40 --port 20021 --replSet InitialSyncTest --dbpath /data/db/job0/mongorunner/InitialSyncTest-1 --setParameter failpoint.initialSyncFuzzerSynchronizationPoint1={ "mode" : "alwaysOn" } --setParameter writePeriodicNoops=false --setParameter numInitialSyncConnectAttempts=60 --bind_ip 0.0.0.0 --setParameter enableTestCommands=1 --setParameter disableLogicalSessionCacheRefresh=true --setParameter transactionLifetimeLimitSeconds=86400 --setParameter orphanCleanupDelaySecs=1 --enableMajorityReadConcern true --setParameter logComponentVerbosity={"replication":{"rollback":2},"transaction":4}
[js_test:shouldworkRepro] 2019-04-12T10:29:45.601-0400 d20021| 2019-04-12T10:29:45.600-0400 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
[js_test:shouldworkRepro] 2019-04-12T10:29:45.611-0400 d20021| 2019-04-12T10:29:45.611-0400 I  CONTROL  [initandlisten] MongoDB starting : pid=61778 port=20021 dbpath=/data/db/job0/mongorunner/InitialSyncTest-1 64-bit host=vladmac
[js_test:shouldworkRepro] 2019-04-12T10:29:45.611-0400 d20021| 2019-04-12T10:29:45.611-0400 I  CONTROL  [initandlisten] DEBUG build (which is slower)
[js_test:shouldworkRepro] 2019-04-12T10:29:45.611-0400 d20021| 2019-04-12T10:29:45.611-0400 I  CONTROL  [initandlisten] db version v4.1.9-115-gb09f71e81a
[js_test:shouldworkRepro] 2019-04-12T10:29:45.611-0400 d20021| 2019-04-12T10:29:45.611-0400 I  CONTROL  [initandlisten] git version: b09f71e81a5921e3a63edd396e6b1b7a3deece60
[js_test:shouldworkRepro] 2019-04-12T10:29:45.611-0400 d20021| 2019-04-12T10:29:45.611-0400 I  CONTROL  [initandlisten] allocator: system
[js_test:shouldworkRepro] 2019-04-12T10:29:45.612-0400 d20021| 2019-04-12T10:29:45.611-0400 I  CONTROL  [initandlisten] modules: none
[js_test:shouldworkRepro] 2019-04-12T10:29:45.612-0400 d20021| 2019-04-12T10:29:45.611-0400 I  CONTROL  [initandlisten] build environment:
[js_test:shouldworkRepro] 2019-04-12T10:29:45.612-0400 d20021| 2019-04-12T10:29:45.611-0400 I  CONTROL  [initandlisten]     distarch: x86_64
[js_test:shouldworkRepro] 2019-04-12T10:29:45.612-0400 d20021| 2019-04-12T10:29:45.611-0400 I  CONTROL  [initandlisten]     target_arch: x86_64
[js_test:shouldworkRepro] 2019-04-12T10:29:45.612-0400 d20021| 2019-04-12T10:29:45.611-0400 I  CONTROL  [initandlisten] options: { net: { bindIp: "0.0.0.0", port: 20021 }, replication: { enableMajorityReadConcern: true, oplogSizeMB: 40, replSet: "InitialSyncTest" }, setParameter: { disableLogicalSessionCacheRefresh: "true", enableTestCommands: "1", failpoint.initialSyncFuzzerSynchronizationPoint1: "{ "mode" : "alwaysOn" }", logComponentVerbosity: "{"replication":{"rollback":2},"transaction":4}", numInitialSyncConnectAttempts: "60", orphanCleanupDelaySecs: "1", transactionLifetimeLimitSeconds: "86400", writePeriodicNoops: "false" }, storage: { dbPath: "/data/db/job0/mongorunner/InitialSyncTest-1" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:45.612-0400 d20021| 2019-04-12T10:29:45.612-0400 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=7680M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=(recovery_progress),
[js_test:shouldworkRepro] 2019-04-12T10:29:46.110-0400 d20021| 2019-04-12T10:29:46.110-0400 I  STORAGE  [initandlisten] WiredTiger message [1555079386:110439][61778:0x10fa9c5c0], txn-recover: Set global recovery timestamp: (0,0)
[js_test:shouldworkRepro] 2019-04-12T10:29:46.141-0400 d20021| 2019-04-12T10:29:46.141-0400 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
[js_test:shouldworkRepro] 2019-04-12T10:29:46.174-0400 d20021| 2019-04-12T10:29:46.174-0400 I  STORAGE  [initandlisten] Timestamp monitor starting
[js_test:shouldworkRepro] 2019-04-12T10:29:46.175-0400 d20021| 2019-04-12T10:29:46.175-0400 I  CONTROL  [initandlisten]
[js_test:shouldworkRepro] 2019-04-12T10:29:46.175-0400 d20021| 2019-04-12T10:29:46.175-0400 I  CONTROL  [initandlisten] ** NOTE: This is a development version (4.1.9-115-gb09f71e81a) of MongoDB.
[js_test:shouldworkRepro] 2019-04-12T10:29:46.175-0400 d20021| 2019-04-12T10:29:46.175-0400 I  CONTROL  [initandlisten] **       Not recommended for production.
[js_test:shouldworkRepro] 2019-04-12T10:29:46.175-0400 d20021| 2019-04-12T10:29:46.175-0400 I  CONTROL  [initandlisten]
[js_test:shouldworkRepro] 2019-04-12T10:29:46.175-0400 d20021| 2019-04-12T10:29:46.175-0400 I  CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
[js_test:shouldworkRepro] 2019-04-12T10:29:46.176-0400 d20021| 2019-04-12T10:29:46.175-0400 I  CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
[js_test:shouldworkRepro] 2019-04-12T10:29:46.176-0400 d20021| 2019-04-12T10:29:46.175-0400 I  CONTROL  [initandlisten]
[js_test:shouldworkRepro] 2019-04-12T10:29:46.177-0400 d20021| 2019-04-12T10:29:46.177-0400 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:46.177-0400 d20021| 2019-04-12T10:29:46.177-0400 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:46.178-0400 d20021| 2019-04-12T10:29:46.178-0400 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:46.179-0400 d20021| 2019-04-12T10:29:46.179-0400 I  STORAGE  [initandlisten] createCollection: local.startup_log with generated UUID: 53539d5c-5b1a-4df8-bb9f-5251f5653274
[js_test:shouldworkRepro] 2019-04-12T10:29:46.203-0400 d20021| 2019-04-12T10:29:46.203-0400 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.startup_log
[js_test:shouldworkRepro] 2019-04-12T10:29:46.203-0400 d20021| 2019-04-12T10:29:46.203-0400 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:46.203-0400 d20021| 2019-04-12T10:29:46.203-0400 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/data/db/job0/mongorunner/InitialSyncTest-1/diagnostic.data'
[js_test:shouldworkRepro] 2019-04-12T10:29:46.205-0400 d20021| 2019-04-12T10:29:46.205-0400 I  STORAGE  [initandlisten] createCollection: local.replset.oplogTruncateAfterPoint with generated UUID: 4e8f2907-3b9f-4e9c-9431-4214b017890d
[js_test:shouldworkRepro] 2019-04-12T10:29:46.205-0400 d20021| 2019-04-12T10:29:46.205-0400 I  SHARDING [monitoring-keys-for-HMAC] Marking collection admin.system.keys as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:46.230-0400 d20021| 2019-04-12T10:29:46.230-0400 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.oplogTruncateAfterPoint
[js_test:shouldworkRepro] 2019-04-12T10:29:46.230-0400 d20021| 2019-04-12T10:29:46.230-0400 I  STORAGE  [initandlisten] createCollection: local.replset.minvalid with generated UUID: 4c232498-8574-4602-afbb-94d0091e7faa
[js_test:shouldworkRepro] 2019-04-12T10:29:46.257-0400 d20021| 2019-04-12T10:29:46.257-0400 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.minvalid
[js_test:shouldworkRepro] 2019-04-12T10:29:46.257-0400 d20021| 2019-04-12T10:29:46.257-0400 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:46.257-0400 d20021| 2019-04-12T10:29:46.257-0400 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:46.258-0400 d20021| 2019-04-12T10:29:46.257-0400 I  REPL     [initandlisten] Did not find local voted for document at startup.
[js_test:shouldworkRepro] 2019-04-12T10:29:46.258-0400 d20021| 2019-04-12T10:29:46.257-0400 I  REPL     [initandlisten] Did not find local Rollback ID document at startup. Creating one.
[js_test:shouldworkRepro] 2019-04-12T10:29:46.258-0400 d20021| 2019-04-12T10:29:46.258-0400 I  STORAGE  [initandlisten] createCollection: local.system.rollback.id with generated UUID: 1da3dfd8-7cfd-48a0-9d21-a5436718d2d7
[js_test:shouldworkRepro] 2019-04-12T10:29:46.282-0400 d20021| 2019-04-12T10:29:46.282-0400 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.system.rollback.id
[js_test:shouldworkRepro] 2019-04-12T10:29:46.283-0400 d20021| 2019-04-12T10:29:46.283-0400 I  SHARDING [initandlisten] Marking collection local.system.rollback.id as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:46.283-0400 d20021| 2019-04-12T10:29:46.283-0400 I  REPL     [initandlisten] Initialized the rollback ID to 1
[js_test:shouldworkRepro] 2019-04-12T10:29:46.283-0400 d20021| 2019-04-12T10:29:46.283-0400 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
[js_test:shouldworkRepro] 2019-04-12T10:29:46.283-0400 d20021| 2019-04-12T10:29:46.283-0400 I  NETWORK  [initandlisten] Listening on /tmp/mongodb-20021.sock
[js_test:shouldworkRepro] 2019-04-12T10:29:46.283-0400 d20021| 2019-04-12T10:29:46.283-0400 I  NETWORK  [initandlisten] Listening on 0.0.0.0
[js_test:shouldworkRepro] 2019-04-12T10:29:46.283-0400 d20021| 2019-04-12T10:29:46.283-0400 I  NETWORK  [initandlisten] waiting for connections on port 20021
[js_test:shouldworkRepro] 2019-04-12T10:29:46.484-0400 d20021| 2019-04-12T10:29:46.484-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61853 #1 (1 connection now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:46.485-0400 d20021| 2019-04-12T10:29:46.485-0400 I  NETWORK  [conn1] received client metadata from 127.0.0.1:61853 conn1: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:46.487-0400 [ connection to vladmac:20020, connection to vladmac:20021 ]
[js_test:shouldworkRepro] 2019-04-12T10:29:46.488-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:46.488-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:46.488-0400 [jsTest] ----
[js_test:shouldworkRepro] 2019-04-12T10:29:46.488-0400 [jsTest] New session started with sessionID: { "id" : UUID("c61334b7-92d3-48b1-9828-84921e44a789") }
[js_test:shouldworkRepro] 2019-04-12T10:29:46.489-0400 [jsTest] ----
[js_test:shouldworkRepro] 2019-04-12T10:29:46.489-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:46.489-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:47.007-0400 d20021| 2019-04-12T10:29:47.007-0400 I  SHARDING [ftdc] Marking collection local.oplog.rs as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:47.089-0400 d20020| 2019-04-12T10:29:47.089-0400 I  CONNPOOL [replexec-0] dropping unhealthy pooled connection to vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:47.089-0400 d20020| 2019-04-12T10:29:47.089-0400 I  CONNPOOL [Replication] Connecting to vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:47.091-0400 d20021| 2019-04-12T10:29:47.091-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61854 #2 (2 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:47.091-0400 d20021| 2019-04-12T10:29:47.091-0400 I  NETWORK  [conn2] received client metadata from 127.0.0.1:61854 conn2: { driver: { name: "NetworkInterfaceTL", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:47.092-0400 d20020| 2019-04-12T10:29:47.092-0400 I  REPL     [replexec-2] Member vladmac:20021 is now in state STARTUP
[js_test:shouldworkRepro] 2019-04-12T10:29:47.092-0400 d20021| 2019-04-12T10:29:47.092-0400 I  CONNPOOL [Replication] Connecting to vladmac:20020
[js_test:shouldworkRepro] 2019-04-12T10:29:47.093-0400 d20020| 2019-04-12T10:29:47.093-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61855 #13 (2 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:47.093-0400 d20020| 2019-04-12T10:29:47.093-0400 I  NETWORK  [conn13] received client metadata from 127.0.0.1:61855 conn13: { driver: { name: "NetworkInterfaceTL", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:47.094-0400 d20020| 2019-04-12T10:29:47.094-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61856 #14 (3 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:47.095-0400 d20020| 2019-04-12T10:29:47.094-0400 I  NETWORK  [conn14] end connection 127.0.0.1:61856 (2 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:47.095-0400 d20021| 2019-04-12T10:29:47.095-0400 I  STORAGE  [replexec-0] createCollection: local.system.replset with generated UUID: ee3bdbf1-d6ee-48d3-a9ac-ab1343c0d453
[js_test:shouldworkRepro] 2019-04-12T10:29:47.123-0400 d20021| 2019-04-12T10:29:47.123-0400 I  INDEX    [replexec-0] index build: done building index _id_ on ns local.system.replset
[js_test:shouldworkRepro] 2019-04-12T10:29:47.123-0400 d20021| 2019-04-12T10:29:47.123-0400 I  REPL     [replexec-0] New replica set config in use: { _id: "InitialSyncTest", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "vladmac:20020", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "vladmac:20021", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 0.0, tags: {}, slaveDelay: 0, votes: 0 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5cb0a0d43e462239f4270e2b') } }
[js_test:shouldworkRepro] 2019-04-12T10:29:47.124-0400 d20021| 2019-04-12T10:29:47.123-0400 I  REPL     [replexec-0] This node is vladmac:20021 in the config
[js_test:shouldworkRepro] 2019-04-12T10:29:47.124-0400 d20021| 2019-04-12T10:29:47.123-0400 I  REPL     [replexec-0] transition to STARTUP2 from STARTUP
[js_test:shouldworkRepro] 2019-04-12T10:29:47.124-0400 d20021| 2019-04-12T10:29:47.124-0400 I  REPL     [replexec-0] Starting replication storage threads
[js_test:shouldworkRepro] 2019-04-12T10:29:47.124-0400 d20021| 2019-04-12T10:29:47.124-0400 I  REPL     [replexec-1] Member vladmac:20020 is now in state PRIMARY
[js_test:shouldworkRepro] 2019-04-12T10:29:47.131-0400 d20021| 2019-04-12T10:29:47.131-0400 I  STORAGE  [replexec-0] createCollection: local.temp_oplog_buffer with generated UUID: c1dfdaae-c33a-4e89-8655-acb97c2aaf0e
[js_test:shouldworkRepro] 2019-04-12T10:29:47.163-0400 d20021| 2019-04-12T10:29:47.163-0400 I  INDEX    [replexec-0] index build: done building index _id_ on ns local.temp_oplog_buffer
[js_test:shouldworkRepro] 2019-04-12T10:29:47.164-0400 d20021| 2019-04-12T10:29:47.164-0400 I  INITSYNC [replication-0] Starting initial sync (attempt 1 of 10)
[js_test:shouldworkRepro] 2019-04-12T10:29:47.164-0400 d20021| 2019-04-12T10:29:47.164-0400 I  STORAGE  [replication-0] Finishing collection drop for local.temp_oplog_buffer (c1dfdaae-c33a-4e89-8655-acb97c2aaf0e).
[js_test:shouldworkRepro] 2019-04-12T10:29:47.183-0400 d20021| 2019-04-12T10:29:47.183-0400 I  STORAGE  [replication-0] createCollection: local.temp_oplog_buffer with generated UUID: 14a74fad-be16-4c0a-aa3b-cbb7300342b9
[js_test:shouldworkRepro] 2019-04-12T10:29:47.210-0400 d20021| 2019-04-12T10:29:47.210-0400 I  INDEX    [replication-0] index build: done building index _id_ on ns local.temp_oplog_buffer
[js_test:shouldworkRepro] 2019-04-12T10:29:47.210-0400 d20021| 2019-04-12T10:29:47.210-0400 I  REPL     [replication-0] sync source candidate: vladmac:20020
[js_test:shouldworkRepro] 2019-04-12T10:29:47.210-0400 d20021| 2019-04-12T10:29:47.210-0400 I  INITSYNC [replication-0] Initial syncer oplog truncation finished in: 0ms
[js_test:shouldworkRepro] 2019-04-12T10:29:47.210-0400 d20021| 2019-04-12T10:29:47.210-0400 I  REPL     [replication-0] ******
[js_test:shouldworkRepro] 2019-04-12T10:29:47.211-0400 d20021| 2019-04-12T10:29:47.210-0400 I  REPL     [replication-0] creating replication oplog of size: 40MB...
[js_test:shouldworkRepro] 2019-04-12T10:29:47.211-0400 d20021| 2019-04-12T10:29:47.210-0400 I  STORAGE  [replication-0] createCollection: local.oplog.rs with generated UUID: b7ba6591-1aec-4686-b3a3-54696c8418de
[js_test:shouldworkRepro] 2019-04-12T10:29:47.224-0400 d20021| 2019-04-12T10:29:47.224-0400 I  STORAGE  [replication-0] Starting OplogTruncaterThread local.oplog.rs
[js_test:shouldworkRepro] 2019-04-12T10:29:47.224-0400 d20021| 2019-04-12T10:29:47.224-0400 I  STORAGE  [replication-0] The size storer reports that the oplog contains 0 records totaling to 0 bytes
[js_test:shouldworkRepro] 2019-04-12T10:29:47.224-0400 d20021| 2019-04-12T10:29:47.224-0400 I  STORAGE  [replication-0] Scanning the oplog to determine where to place markers for truncation
[js_test:shouldworkRepro] 2019-04-12T10:29:47.299-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:47.300-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:47.300-0400 [jsTest] ----
[js_test:shouldworkRepro] 2019-04-12T10:29:47.300-0400 [jsTest] Transitioning to: "kDuringInitialSync"
[js_test:shouldworkRepro] 2019-04-12T10:29:47.300-0400 [jsTest] ----
[js_test:shouldworkRepro] 2019-04-12T10:29:47.300-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:47.300-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:47.328-0400 d20021| 2019-04-12T10:29:47.328-0400 I  REPL     [replication-0] ******
[js_test:shouldworkRepro] 2019-04-12T10:29:47.328-0400 d20021| 2019-04-12T10:29:47.328-0400 I  REPL     [replication-0] dropReplicatedDatabases - dropping 1 databases
[js_test:shouldworkRepro] 2019-04-12T10:29:47.328-0400 d20021| 2019-04-12T10:29:47.328-0400 I  REPL     [replication-0] dropReplicatedDatabases - dropped 1 databases
[js_test:shouldworkRepro] 2019-04-12T10:29:47.328-0400 d20021| 2019-04-12T10:29:47.328-0400 I  COMMAND  [monitoring-keys-for-HMAC] command admin.system.keys command: find { find: "system.keys", filter: { purpose: "HMAC", expiresAt: { $gt: Timestamp(0, 0) } }, sort: { expiresAt: 1 }, $readPreference: { mode: "nearest", tags: [] }, $db: "admin" } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:231 locks:{ Global: { acquireCount: { r: 2, w: 2 }, acquireWaitCount: { r: 1 }, timeAcquiringMicros: { r: 111938 } }, Database: { acquireCount: { r: 2 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_msg 112ms
[js_test:shouldworkRepro] 2019-04-12T10:29:47.330-0400 d20020| 2019-04-12T10:29:47.330-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61857 #15 (3 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:47.330-0400 d20020| 2019-04-12T10:29:47.330-0400 I  NETWORK  [conn15] received client metadata from 127.0.0.1:61857 conn15: { driver: { name: "NetworkInterfaceTL", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:47.332-0400 d20021| 2019-04-12T10:29:47.332-0400 I  EXECUTOR [replication-0] Collection Cloner scheduled a remote command on the admin db: { listDatabases: true, nameOnly: true }
[js_test:shouldworkRepro] 2019-04-12T10:29:47.332-0400 d20021| 2019-04-12T10:29:47.332-0400 I  EXECUTOR [replication-0] initialSyncFuzzerSynchronizationPoint1 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:47.608-0400 d20020| 2019-04-12T10:29:47.608-0400 I  COMMAND  [conn1] renameCollectionForCommand: rename db1.coll2 to db2.coll1 and drop db2.coll1.
[js_test:shouldworkRepro] 2019-04-12T10:29:47.608-0400 d20020| 2019-04-12T10:29:47.608-0400 I  STORAGE  [conn1] createCollection: db2.tmpKpwDl.renameCollection with provided UUID: 2072af0a-fd07-46e6-8685-8a32259a9c95
[js_test:shouldworkRepro] 2019-04-12T10:29:47.645-0400 d20020| 2019-04-12T10:29:47.645-0400 I  INDEX    [conn1] index build: done building index _id_ on ns db2.tmpKpwDl.renameCollection
[js_test:shouldworkRepro] 2019-04-12T10:29:47.646-0400 d20020| 2019-04-12T10:29:47.646-0400 I  SHARDING [conn1] Marking collection db2.tmpKpwDl.renameCollection as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:47.646-0400 d20020| 2019-04-12T10:29:47.646-0400 I  STORAGE  [conn1] dropCollection: db2.coll1 (fd81e91b-6cae-471a-84ac-12035c9f2c3a) - storage engine will take ownership of drop-pending collection with optime { ts: Timestamp(1555079387, 2), t: 1 } and commit timestamp Timestamp(0, 0)
[js_test:shouldworkRepro] 2019-04-12T10:29:47.647-0400 d20020| 2019-04-12T10:29:47.646-0400 I  STORAGE  [conn1] Finishing collection drop for db2.coll1 (fd81e91b-6cae-471a-84ac-12035c9f2c3a).
[js_test:shouldworkRepro] 2019-04-12T10:29:47.647-0400 d20020| 2019-04-12T10:29:47.647-0400 I  STORAGE  [conn1] renameCollection: renaming collection 2072af0a-fd07-46e6-8685-8a32259a9c95 from db2.tmpKpwDl.renameCollection to db2.coll1
[js_test:shouldworkRepro] 2019-04-12T10:29:47.647-0400 d20020| 2019-04-12T10:29:47.647-0400 I  STORAGE  [conn1] Deferring table drop for index '_id_' on collection 'db2.coll1.$_id_ (fd81e91b-6cae-471a-84ac-12035c9f2c3a)'. Ident: 'index-22--3534291432697553977', commit timestamp: 'Timestamp(1555079387, 2)'
[js_test:shouldworkRepro] 2019-04-12T10:29:47.647-0400 d20020| 2019-04-12T10:29:47.647-0400 I  STORAGE  [conn1] Deferring table drop for collection 'db2.coll1' (fd81e91b-6cae-471a-84ac-12035c9f2c3a). Ident: collection-21--3534291432697553977, commit timestamp: Timestamp(1555079387, 2)
[js_test:shouldworkRepro] 2019-04-12T10:29:47.647-0400 d20020| 2019-04-12T10:29:47.647-0400 I  COMMAND  [conn1] CMD: drop db1.coll2
[js_test:shouldworkRepro] 2019-04-12T10:29:47.648-0400 d20020| 2019-04-12T10:29:47.647-0400 I  STORAGE  [conn1] dropCollection: db1.coll2 (d81f3f9f-f597-47ca-9b9c-f9caf88e5544) - storage engine will take ownership of drop-pending collection with optime { ts: Timestamp(0, 0), t: -1 } and commit timestamp Timestamp(0, 0)
[js_test:shouldworkRepro] 2019-04-12T10:29:47.648-0400 d20020| 2019-04-12T10:29:47.648-0400 I  STORAGE  [conn1] Finishing collection drop for db1.coll2 (d81f3f9f-f597-47ca-9b9c-f9caf88e5544).
[js_test:shouldworkRepro] 2019-04-12T10:29:47.648-0400 d20020| 2019-04-12T10:29:47.648-0400 I  STORAGE  [conn1] Deferring table drop for index '_id_' on collection 'db1.coll2.$_id_ (d81f3f9f-f597-47ca-9b9c-f9caf88e5544)'. Ident: 'index-26--3534291432697553977', commit timestamp: 'Timestamp(1555079387, 3)'
[js_test:shouldworkRepro] 2019-04-12T10:29:47.648-0400 d20020| 2019-04-12T10:29:47.648-0400 I  STORAGE  [conn1] Deferring table drop for collection 'db1.coll2' (d81f3f9f-f597-47ca-9b9c-f9caf88e5544). Ident: collection-25--3534291432697553977, commit timestamp: Timestamp(1555079387, 3)
[js_test:shouldworkRepro] 2019-04-12T10:29:47.649-0400 d20020| 2019-04-12T10:29:47.649-0400 I  COMMAND  [conn1] CMD: drop db2.coll2
[js_test:shouldworkRepro] 2019-04-12T10:29:47.649-0400 d20020| 2019-04-12T10:29:47.649-0400 I  STORAGE  [conn1] dropCollection: db2.coll2 (ba7ae8b6-5c7d-467a-a1e1-15c6bd96e087) - storage engine will take ownership of drop-pending collection with optime { ts: Timestamp(0, 0), t: -1 } and commit timestamp Timestamp(0, 0)
[js_test:shouldworkRepro] 2019-04-12T10:29:47.650-0400 d20020| 2019-04-12T10:29:47.649-0400 I  STORAGE  [conn1] Finishing collection drop for db2.coll2 (ba7ae8b6-5c7d-467a-a1e1-15c6bd96e087).
[js_test:shouldworkRepro] 2019-04-12T10:29:47.650-0400 d20020| 2019-04-12T10:29:47.650-0400 I  STORAGE  [conn1] Deferring table drop for index '_id_' on collection 'db2.coll2.$_id_ (ba7ae8b6-5c7d-467a-a1e1-15c6bd96e087)'. Ident: 'index-24--3534291432697553977', commit timestamp: 'Timestamp(1555079387, 4)'
[js_test:shouldworkRepro] 2019-04-12T10:29:47.650-0400 d20020| 2019-04-12T10:29:47.650-0400 I  STORAGE  [conn1] Deferring table drop for collection 'db2.coll2' (ba7ae8b6-5c7d-467a-a1e1-15c6bd96e087). Ident: collection-23--3534291432697553977, commit timestamp: Timestamp(1555079387, 4)
[js_test:shouldworkRepro] 2019-04-12T10:29:47.651-0400 d20021| 2019-04-12T10:29:47.651-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:47.651-0400 d20021| 2019-04-12T10:29:47.651-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:47.739-0400 d20021| 2019-04-12T10:29:47.739-0400 I  EXECUTOR [replication-0] initialSyncFuzzerSynchronizationPoint2 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:47.958-0400 d20021| 2019-04-12T10:29:47.958-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:47.959-0400 d20021| 2019-04-12T10:29:47.959-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:48.048-0400 d20021| 2019-04-12T10:29:48.048-0400 I  SHARDING [replication-1] Marking collection local.temp_oplog_buffer as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:48.049-0400 d20021| 2019-04-12T10:29:48.048-0400 I  EXECUTOR [replication-0] Collection Cloner scheduled a remote command on the admin db: { listCollections: 1, filter: { $or: [ { type: "collection" }, { type: { $exists: false } } ] } }
[js_test:shouldworkRepro] 2019-04-12T10:29:48.049-0400 d20021| 2019-04-12T10:29:48.048-0400 I  EXECUTOR [replication-0] initialSyncFuzzerSynchronizationPoint1 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:48.165-0400 d20020| 2019-04-12T10:29:48.165-0400 I  COMMAND  [conn1] renameCollectionForCommand: rename db2.coll1 to db2.coll2.
[js_test:shouldworkRepro] 2019-04-12T10:29:48.165-0400 d20020| 2019-04-12T10:29:48.165-0400 I  STORAGE  [conn1] renameCollection: renaming collection 2072af0a-fd07-46e6-8685-8a32259a9c95 from db2.coll1 to db2.coll2
[js_test:shouldworkRepro] 2019-04-12T10:29:48.168-0400 d20021| 2019-04-12T10:29:48.167-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:48.169-0400 d20021| 2019-04-12T10:29:48.169-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:48.254-0400 d20021| 2019-04-12T10:29:48.253-0400 I  EXECUTOR [replication-0] initialSyncFuzzerSynchronizationPoint2 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:48.477-0400 d20021| 2019-04-12T10:29:48.477-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:48.478-0400 d20021| 2019-04-12T10:29:48.478-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:48.561-0400 d20021| 2019-04-12T10:29:48.561-0400 I  CONNPOOL [RS] Connecting to vladmac:20020
[js_test:shouldworkRepro] 2019-04-12T10:29:48.565-0400 d20020| 2019-04-12T10:29:48.565-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61858 #16 (4 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:48.565-0400 d20020| 2019-04-12T10:29:48.565-0400 I  NETWORK  [conn16] received client metadata from 127.0.0.1:61858 conn16: { driver: { name: "NetworkInterfaceTL", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:48.566-0400 d20021| 2019-04-12T10:29:48.566-0400 I  INITSYNC [replication-1] CollectionCloner::start called, on ns:admin.system.keys
[js_test:shouldworkRepro] 2019-04-12T10:29:48.567-0400 d20021| 2019-04-12T10:29:48.566-0400 I  EXECUTOR [replication-0] Collection Cloner scheduled a remote command on the admin db: { listIndexes: UUID("344947af-588d-4a6c-95ed-fc25d1d38bfc") }
[js_test:shouldworkRepro] 2019-04-12T10:29:48.567-0400 d20021| 2019-04-12T10:29:48.566-0400 I  EXECUTOR [replication-0] initialSyncFuzzerSynchronizationPoint1 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:48.686-0400 d20020| 2019-04-12T10:29:48.686-0400 I  SHARDING [conn1] Marking collection db1.coll1 as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:48.686-0400 d20020| 2019-04-12T10:29:48.686-0400 I  STORAGE  [conn1] createCollection: db1.coll1 with generated UUID: 53e9cfbc-0461-4cb7-9fe4-c404b6839cd6
[js_test:shouldworkRepro] 2019-04-12T10:29:48.711-0400 d20020| 2019-04-12T10:29:48.711-0400 I  INDEX    [conn1] index build: done building index _id_ on ns db1.coll1
[js_test:shouldworkRepro] 2019-04-12T10:29:48.712-0400 d20020| 2019-04-12T10:29:48.712-0400 I  SHARDING [conn1] Marking collection db1.tmpRxZmH.convertToCapped.coll1 as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:48.712-0400 d20020| 2019-04-12T10:29:48.712-0400 I  STORAGE  [conn1] createCollection: db1.tmpRxZmH.convertToCapped.coll1 with generated UUID: ec4b65f5-3ddb-4f12-8b09-0cf72100d944
[js_test:shouldworkRepro] 2019-04-12T10:29:48.740-0400 d20020| 2019-04-12T10:29:48.740-0400 I  INDEX    [conn1] index build: done building index _id_ on ns db1.tmpRxZmH.convertToCapped.coll1
[js_test:shouldworkRepro] 2019-04-12T10:29:48.740-0400 d20020| 2019-04-12T10:29:48.740-0400 I  COMMAND  [conn1] renameCollectionForCommand: rename db1.tmpRxZmH.convertToCapped.coll1 to db1.coll1 and drop db1.coll1.
[js_test:shouldworkRepro] 2019-04-12T10:29:48.741-0400 d20020| 2019-04-12T10:29:48.741-0400 I  STORAGE  [conn1] dropCollection: db1.coll1 (53e9cfbc-0461-4cb7-9fe4-c404b6839cd6) - storage engine will take ownership of drop-pending collection with optime { ts: Timestamp(1555079388, 4), t: 1 } and commit timestamp Timestamp(0, 0)
[js_test:shouldworkRepro] 2019-04-12T10:29:48.741-0400 d20020| 2019-04-12T10:29:48.741-0400 I  STORAGE  [conn1] Finishing collection drop for db1.coll1 (53e9cfbc-0461-4cb7-9fe4-c404b6839cd6).
[js_test:shouldworkRepro] 2019-04-12T10:29:48.741-0400 d20020| 2019-04-12T10:29:48.741-0400 I  STORAGE  [conn1] renameCollection: renaming collection ec4b65f5-3ddb-4f12-8b09-0cf72100d944 from db1.tmpRxZmH.convertToCapped.coll1 to db1.coll1
[js_test:shouldworkRepro] 2019-04-12T10:29:48.741-0400 d20020| 2019-04-12T10:29:48.741-0400 I  STORAGE  [conn1] Deferring table drop for index '_id_' on collection 'db1.coll1.$_id_ (53e9cfbc-0461-4cb7-9fe4-c404b6839cd6)'. Ident: 'index-30--3534291432697553977', commit timestamp: 'Timestamp(1555079388, 4)'
[js_test:shouldworkRepro] 2019-04-12T10:29:48.741-0400 d20020| 2019-04-12T10:29:48.741-0400 I  STORAGE  [conn1] Deferring table drop for collection 'db1.coll1' (53e9cfbc-0461-4cb7-9fe4-c404b6839cd6). Ident: collection-29--3534291432697553977, commit timestamp: Timestamp(1555079388, 4)
[js_test:shouldworkRepro] 2019-04-12T10:29:48.742-0400 d20021| 2019-04-12T10:29:48.742-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:48.742-0400 d20021| 2019-04-12T10:29:48.742-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:48.772-0400 d20021| 2019-04-12T10:29:48.772-0400 I  EXECUTOR [replication-0] initialSyncFuzzerSynchronizationPoint2 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:49.048-0400 d20021| 2019-04-12T10:29:49.048-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:49.049-0400 d20021| 2019-04-12T10:29:49.049-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:49.080-0400 d20021| 2019-04-12T10:29:49.080-0400 I  STORAGE  [repl-writer-worker-0] createCollection: admin.system.keys with provided UUID: 344947af-588d-4a6c-95ed-fc25d1d38bfc
[js_test:shouldworkRepro] 2019-04-12T10:29:49.095-0400 d20020| 2019-04-12T10:29:49.095-0400 I  REPL     [replexec-0] Member vladmac:20021 is now in state STARTUP2
[js_test:shouldworkRepro] 2019-04-12T10:29:49.109-0400 d20021| 2019-04-12T10:29:49.109-0400 I  INDEX    [repl-writer-worker-0] index build: starting on admin.system.keys properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.keys" } using method: Foreground
[js_test:shouldworkRepro] 2019-04-12T10:29:49.109-0400 d20021| 2019-04-12T10:29:49.109-0400 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 500 megabytes of RAM
[js_test:shouldworkRepro] 2019-04-12T10:29:49.110-0400 d20020| 2019-04-12T10:29:49.110-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61859 #17 (5 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:49.110-0400 d20020| 2019-04-12T10:29:49.110-0400 I  NETWORK  [conn17] received client metadata from 127.0.0.1:61859 conn17: { driver: { name: "MongoDB Internal Client", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:49.111-0400 d20021| 2019-04-12T10:29:49.111-0400 I  INITSYNC [replication-0] CollectionCloner ns:admin.system.keys finished cloning with status: OK
[js_test:shouldworkRepro] 2019-04-12T10:29:49.111-0400 d20020| 2019-04-12T10:29:49.111-0400 I  NETWORK  [conn17] end connection 127.0.0.1:61859 (4 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:49.111-0400 d20021| 2019-04-12T10:29:49.111-0400 I  INDEX    [replication-0] index build: inserted 2 keys from external sorter into index in 0 seconds
[js_test:shouldworkRepro] 2019-04-12T10:29:49.117-0400 d20021| 2019-04-12T10:29:49.117-0400 I  INDEX    [replication-0] index build: done building index _id_ on ns admin.system.keys
[js_test:shouldworkRepro] 2019-04-12T10:29:49.117-0400 d20021| 2019-04-12T10:29:49.117-0400 I  INITSYNC [replication-0] CollectionCloner::start called, on ns:admin.system.version
[js_test:shouldworkRepro] 2019-04-12T10:29:49.118-0400 d20021| 2019-04-12T10:29:49.117-0400 I  EXECUTOR [replication-1] Collection Cloner scheduled a remote command on the admin db: { listIndexes: UUID("79fe740f-34c2-436a-b791-66b5f56f6718") }
[js_test:shouldworkRepro] 2019-04-12T10:29:49.118-0400 d20021| 2019-04-12T10:29:49.117-0400 I  EXECUTOR [replication-1] initialSyncFuzzerSynchronizationPoint1 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:49.255-0400 d20020| 2019-04-12T10:29:49.255-0400 I  COMMAND  [conn1] renameCollectionForCommand: rename db1.coll1 to db2.coll1.
[js_test:shouldworkRepro] 2019-04-12T10:29:49.256-0400 d20020| 2019-04-12T10:29:49.256-0400 I  STORAGE  [conn1] createCollection: db2.tmpg4znI.renameCollection with provided UUID: 99548b69-9a01-48f0-a8e1-dea43b0decee
[js_test:shouldworkRepro] 2019-04-12T10:29:49.282-0400 d20020| 2019-04-12T10:29:49.282-0400 I  INDEX    [conn1] index build: done building index _id_ on ns db2.tmpg4znI.renameCollection
[js_test:shouldworkRepro] 2019-04-12T10:29:49.282-0400 d20020| 2019-04-12T10:29:49.282-0400 I  SHARDING [conn1] Marking collection db2.tmpg4znI.renameCollection as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:49.282-0400 d20020| 2019-04-12T10:29:49.282-0400 I  STORAGE  [conn1] renameCollection: renaming collection 99548b69-9a01-48f0-a8e1-dea43b0decee from db2.tmpg4znI.renameCollection to db2.coll1
[js_test:shouldworkRepro] 2019-04-12T10:29:49.282-0400 d20020| 2019-04-12T10:29:49.282-0400 I  COMMAND  [conn1] CMD: drop db1.coll1
[js_test:shouldworkRepro] 2019-04-12T10:29:49.282-0400 d20020| 2019-04-12T10:29:49.282-0400 I  STORAGE  [conn1] dropCollection: db1.coll1 (ec4b65f5-3ddb-4f12-8b09-0cf72100d944) - storage engine will take ownership of drop-pending collection with optime { ts: Timestamp(0, 0), t: -1 } and commit timestamp Timestamp(0, 0)
[js_test:shouldworkRepro] 2019-04-12T10:29:49.282-0400 d20020| 2019-04-12T10:29:49.282-0400 I  STORAGE  [conn1] Finishing collection drop for db1.coll1 (ec4b65f5-3ddb-4f12-8b09-0cf72100d944).
[js_test:shouldworkRepro] 2019-04-12T10:29:49.282-0400 d20020| 2019-04-12T10:29:49.282-0400 I  STORAGE  [conn1] Deferring table drop for index '_id_' on collection 'db1.coll1.$_id_ (ec4b65f5-3ddb-4f12-8b09-0cf72100d944)'. Ident: 'index-32--3534291432697553977', commit timestamp: 'Timestamp(1555079389, 3)'
[js_test:shouldworkRepro] 2019-04-12T10:29:49.283-0400 d20020| 2019-04-12T10:29:49.282-0400 I  STORAGE  [conn1] Deferring table drop for collection 'db1.coll1' (ec4b65f5-3ddb-4f12-8b09-0cf72100d944). Ident: collection-31--3534291432697553977, commit timestamp: Timestamp(1555079389, 3)
[js_test:shouldworkRepro] 2019-04-12T10:29:49.284-0400 d20021| 2019-04-12T10:29:49.283-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:49.284-0400 d20021| 2019-04-12T10:29:49.284-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:49.321-0400 d20021| 2019-04-12T10:29:49.321-0400 I  EXECUTOR [replication-1] initialSyncFuzzerSynchronizationPoint2 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:49.591-0400 d20021| 2019-04-12T10:29:49.591-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:49.591-0400 d20021| 2019-04-12T10:29:49.591-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:49.628-0400 d20021| 2019-04-12T10:29:49.628-0400 I  STORAGE  [repl-writer-worker-1] createCollection: admin.system.version with provided UUID: 79fe740f-34c2-436a-b791-66b5f56f6718
[js_test:shouldworkRepro] 2019-04-12T10:29:49.690-0400 d20021| 2019-04-12T10:29:49.689-0400 I  INDEX    [repl-writer-worker-1] index build: starting on admin.system.version properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.version" } using method: Foreground
[js_test:shouldworkRepro] 2019-04-12T10:29:49.690-0400 d20021| 2019-04-12T10:29:49.689-0400 I  INDEX    [repl-writer-worker-1] build may temporarily use up to 500 megabytes of RAM
[js_test:shouldworkRepro] 2019-04-12T10:29:49.691-0400 d20020| 2019-04-12T10:29:49.691-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61862 #18 (5 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:49.691-0400 d20020| 2019-04-12T10:29:49.691-0400 I  NETWORK  [conn18] received client metadata from 127.0.0.1:61862 conn18: { driver: { name: "MongoDB Internal Client", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:49.692-0400 d20021| 2019-04-12T10:29:49.692-0400 I  COMMAND  [repl-writer-worker-2] setting featureCompatibilityVersion to 4.2
[js_test:shouldworkRepro] 2019-04-12T10:29:49.692-0400 d20021| 2019-04-12T10:29:49.692-0400 I  NETWORK  [repl-writer-worker-2] Skip closing connection for connection # 2
[js_test:shouldworkRepro] 2019-04-12T10:29:49.692-0400 d20021| 2019-04-12T10:29:49.692-0400 I  NETWORK  [repl-writer-worker-2] Skip closing connection for connection # 1
[js_test:shouldworkRepro] 2019-04-12T10:29:49.692-0400 d20021| 2019-04-12T10:29:49.692-0400 I  INITSYNC [replication-1] CollectionCloner ns:admin.system.version finished cloning with status: OK
[js_test:shouldworkRepro] 2019-04-12T10:29:49.692-0400 d20020| 2019-04-12T10:29:49.692-0400 I  NETWORK  [conn18] end connection 127.0.0.1:61862 (4 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:49.693-0400 d20021| 2019-04-12T10:29:49.692-0400 I  INDEX    [replication-1] index build: inserted 1 keys from external sorter into index in 0 seconds
[js_test:shouldworkRepro] 2019-04-12T10:29:49.704-0400 d20021| 2019-04-12T10:29:49.704-0400 I  INDEX    [replication-1] index build: done building index _id_ on ns admin.system.version
[js_test:shouldworkRepro] 2019-04-12T10:29:49.705-0400 d20021| 2019-04-12T10:29:49.705-0400 I  EXECUTOR [replication-1] Collection Cloner scheduled a remote command on the config db: { listCollections: 1, filter: { $or: [ { type: "collection" }, { type: { $exists: false } } ] } }
[js_test:shouldworkRepro] 2019-04-12T10:29:49.705-0400 d20021| 2019-04-12T10:29:49.705-0400 I  EXECUTOR [replication-1] initialSyncFuzzerSynchronizationPoint1 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:49.798-0400 d20021| 2019-04-12T10:29:49.798-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:49.798-0400 d20021| 2019-04-12T10:29:49.798-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:49.810-0400 d20021| 2019-04-12T10:29:49.809-0400 I  EXECUTOR [replication-1] initialSyncFuzzerSynchronizationPoint2 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:50.102-0400 d20021| 2019-04-12T10:29:50.102-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:50.102-0400 d20021| 2019-04-12T10:29:50.102-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:50.114-0400 d20021| 2019-04-12T10:29:50.114-0400 I  INITSYNC [replication-0] CollectionCloner::start called, on ns:config.transactions
[js_test:shouldworkRepro] 2019-04-12T10:29:50.115-0400 d20021| 2019-04-12T10:29:50.115-0400 I  EXECUTOR [replication-1] Collection Cloner scheduled a remote command on the config db: { listIndexes: UUID("a6348a5e-ec46-4ef4-9436-f5d70a411e91") }
[js_test:shouldworkRepro] 2019-04-12T10:29:50.115-0400 d20021| 2019-04-12T10:29:50.115-0400 I  EXECUTOR [replication-1] initialSyncFuzzerSynchronizationPoint1 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:50.306-0400 d20021| 2019-04-12T10:29:50.306-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:50.307-0400 d20021| 2019-04-12T10:29:50.307-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:50.320-0400 d20021| 2019-04-12T10:29:50.320-0400 I  EXECUTOR [replication-1] initialSyncFuzzerSynchronizationPoint2 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:50.610-0400 d20021| 2019-04-12T10:29:50.610-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:50.611-0400 d20021| 2019-04-12T10:29:50.611-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:50.624-0400 d20021| 2019-04-12T10:29:50.624-0400 I  STORAGE  [repl-writer-worker-15] createCollection: config.transactions with provided UUID: a6348a5e-ec46-4ef4-9436-f5d70a411e91
[js_test:shouldworkRepro] 2019-04-12T10:29:50.727-0400 d20021| 2019-04-12T10:29:50.726-0400 I  INDEX    [repl-writer-worker-15] index build: starting on config.transactions properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "config.transactions" } using method: Hybrid
[js_test:shouldworkRepro] 2019-04-12T10:29:50.727-0400 d20021| 2019-04-12T10:29:50.726-0400 I  INDEX    [repl-writer-worker-15] build may temporarily use up to 500 megabytes of RAM
[js_test:shouldworkRepro] 2019-04-12T10:29:50.728-0400 d20020| 2019-04-12T10:29:50.728-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61863 #19 (5 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:50.728-0400 d20020| 2019-04-12T10:29:50.728-0400 I  NETWORK  [conn19] received client metadata from 127.0.0.1:61863 conn19: { driver: { name: "MongoDB Internal Client", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:50.729-0400 d20021| 2019-04-12T10:29:50.729-0400 I  INITSYNC [replication-1] CollectionCloner ns:config.transactions finished cloning with status: OK
[js_test:shouldworkRepro] 2019-04-12T10:29:50.729-0400 d20020| 2019-04-12T10:29:50.729-0400 I  NETWORK  [conn19] end connection 127.0.0.1:61863 (4 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:50.730-0400 d20021| 2019-04-12T10:29:50.730-0400 I  INDEX    [replication-1] index build: inserted 0 keys from external sorter into index in 0 seconds
[js_test:shouldworkRepro] 2019-04-12T10:29:50.735-0400 d20021| 2019-04-12T10:29:50.735-0400 I  INDEX    [replication-1] index build: done building index _id_ on ns config.transactions
[js_test:shouldworkRepro] 2019-04-12T10:29:50.747-0400 d20021| 2019-04-12T10:29:50.747-0400 I  EXECUTOR [replication-1] Collection Cloner scheduled a remote command on the db2 db: { listCollections: 1, filter: { $or: [ { type: "collection" }, { type: { $exists: false } } ] } }
[js_test:shouldworkRepro] 2019-04-12T10:29:50.747-0400 d20021| 2019-04-12T10:29:50.747-0400 I  EXECUTOR [replication-1] initialSyncFuzzerSynchronizationPoint1 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:50.815-0400 d20021| 2019-04-12T10:29:50.815-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:50.815-0400 d20021| 2019-04-12T10:29:50.815-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:50.850-0400 d20021| 2019-04-12T10:29:50.850-0400 I  EXECUTOR [replication-1] initialSyncFuzzerSynchronizationPoint2 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:51.119-0400 d20021| 2019-04-12T10:29:51.119-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:51.120-0400 d20021| 2019-04-12T10:29:51.119-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:51.153-0400 d20021| 2019-04-12T10:29:51.153-0400 I  INITSYNC [replication-0] CollectionCloner::start called, on ns:db2.coll2
[js_test:shouldworkRepro] 2019-04-12T10:29:51.154-0400 d20021| 2019-04-12T10:29:51.153-0400 I  EXECUTOR [replication-1] Collection Cloner scheduled a remote command on the db2 db: { listIndexes: UUID("2072af0a-fd07-46e6-8685-8a32259a9c95") }
[js_test:shouldworkRepro] 2019-04-12T10:29:51.154-0400 d20021| 2019-04-12T10:29:51.154-0400 I  EXECUTOR [replication-1] initialSyncFuzzerSynchronizationPoint1 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:51.323-0400 d20021| 2019-04-12T10:29:51.322-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:51.323-0400 d20021| 2019-04-12T10:29:51.323-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:51.359-0400 d20021| 2019-04-12T10:29:51.358-0400 I  EXECUTOR [replication-1] initialSyncFuzzerSynchronizationPoint2 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:51.630-0400 d20021| 2019-04-12T10:29:51.630-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:51.631-0400 d20021| 2019-04-12T10:29:51.631-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:51.665-0400 d20021| 2019-04-12T10:29:51.665-0400 I  STORAGE  [repl-writer-worker-3] createCollection: db2.coll2 with provided UUID: 2072af0a-fd07-46e6-8685-8a32259a9c95
[js_test:shouldworkRepro] 2019-04-12T10:29:51.718-0400 d20021| 2019-04-12T10:29:51.718-0400 I  INDEX    [repl-writer-worker-3] index build: done building index _id_ on ns db2.coll2
[js_test:shouldworkRepro] 2019-04-12T10:29:51.719-0400 d20020| 2019-04-12T10:29:51.719-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61865 #20 (5 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:51.719-0400 d20020| 2019-04-12T10:29:51.719-0400 I  NETWORK  [conn20] received client metadata from 127.0.0.1:61865 conn20: { driver: { name: "MongoDB Internal Client", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:51.719-0400 d20021| 2019-04-12T10:29:51.719-0400 I  INITSYNC [replication-1] CollectionCloner ns:db2.coll2 finished cloning with status: OK
[js_test:shouldworkRepro] 2019-04-12T10:29:51.719-0400 d20021| 2019-04-12T10:29:51.719-0400 I  INITSYNC [replication-1] CollectionCloner::start called, on ns:db2.coll1
[js_test:shouldworkRepro] 2019-04-12T10:29:51.720-0400 d20020| 2019-04-12T10:29:51.719-0400 I  NETWORK  [conn20] end connection 127.0.0.1:61865 (4 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:51.720-0400 d20021| 2019-04-12T10:29:51.720-0400 I  EXECUTOR [replication-0] Collection Cloner scheduled a remote command on the db2 db: { listIndexes: UUID("99548b69-9a01-48f0-a8e1-dea43b0decee") }
[js_test:shouldworkRepro] 2019-04-12T10:29:51.720-0400 d20021| 2019-04-12T10:29:51.720-0400 I  EXECUTOR [replication-0] initialSyncFuzzerSynchronizationPoint1 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:51.838-0400 d20021| 2019-04-12T10:29:51.838-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:51.839-0400 d20021| 2019-04-12T10:29:51.839-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:51.922-0400 d20021| 2019-04-12T10:29:51.922-0400 I  EXECUTOR [replication-0] initialSyncFuzzerSynchronizationPoint2 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:52.141-0400 d20021| 2019-04-12T10:29:52.140-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:52.141-0400 d20021| 2019-04-12T10:29:52.141-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:52.230-0400 d20021| 2019-04-12T10:29:52.229-0400 I  STORAGE  [repl-writer-worker-5] createCollection: db2.coll1 with provided UUID: 99548b69-9a01-48f0-a8e1-dea43b0decee
[js_test:shouldworkRepro] 2019-04-12T10:29:52.316-0400 d20021| 2019-04-12T10:29:52.316-0400 I  INDEX    [repl-writer-worker-5] index build: done building index _id_ on ns db2.coll1
[js_test:shouldworkRepro] 2019-04-12T10:29:52.317-0400 d20020| 2019-04-12T10:29:52.317-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61867 #21 (5 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:52.318-0400 d20020| 2019-04-12T10:29:52.318-0400 I  NETWORK  [conn21] received client metadata from 127.0.0.1:61867 conn21: { driver: { name: "MongoDB Internal Client", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:52.318-0400 d20021| 2019-04-12T10:29:52.318-0400 I  INITSYNC [replication-0] CollectionCloner ns:db2.coll1 finished cloning with status: OK
[js_test:shouldworkRepro] 2019-04-12T10:29:52.318-0400 d20020| 2019-04-12T10:29:52.318-0400 I  NETWORK  [conn21] end connection 127.0.0.1:61867 (4 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:52.318-0400 d20021| 2019-04-12T10:29:52.318-0400 I  INITSYNC [replication-0] Finished cloning data: OK. Beginning oplog replay.
[js_test:shouldworkRepro] 2019-04-12T10:29:52.318-0400 d20021| 2019-04-12T10:29:52.318-0400 I  INITSYNC [replication-1] Writing to the oplog and applying operations until { : Timestamp(1555079389, 3) } before initial sync can complete. (started fetching at { : Timestamp(1555079385, 5) } and applying at { : Timestamp(1555079385, 5) })
[js_test:shouldworkRepro] 2019-04-12T10:29:52.319-0400 d20021| 2019-04-12T10:29:52.319-0400 I  INITSYNC [replication-1] Initial Syncer is about to apply the next oplog batch of size: 1
[js_test:shouldworkRepro] 2019-04-12T10:29:52.319-0400 d20021| 2019-04-12T10:29:52.319-0400 I  INITSYNC [replication-1] initialSyncFuzzerSynchronizationPoint1 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:52.345-0400 d20021| 2019-04-12T10:29:52.344-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:52.345-0400 d20021| 2019-04-12T10:29:52.345-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:52.419-0400 d20021| 2019-04-12T10:29:52.419-0400 I  INITSYNC [replication-1] initialSyncFuzzerSynchronizationPoint2 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:52.650-0400 d20021| 2019-04-12T10:29:52.649-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:52.650-0400 d20021| 2019-04-12T10:29:52.650-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:52.730-0400 d20021| 2019-04-12T10:29:52.729-0400 I  SHARDING [replication-0] Marking collection local.replset.oplogTruncateAfterPoint as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:52.731-0400 d20021| 2019-04-12T10:29:52.731-0400 I  STORAGE  [repl-writer-worker-7] renameCollection: renaming collection 2072af0a-fd07-46e6-8685-8a32259a9c95 from db2.coll2 to db2.tmpKpwDl.renameCollection
[js_test:shouldworkRepro] 2019-04-12T10:29:52.732-0400 d20021| 2019-04-12T10:29:52.732-0400 I  INITSYNC [replication-1] Initial Syncer is about to apply the next oplog batch of size: 1
[js_test:shouldworkRepro] 2019-04-12T10:29:52.733-0400 d20021| 2019-04-12T10:29:52.732-0400 I  INITSYNC [replication-1] initialSyncFuzzerSynchronizationPoint1 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:52.859-0400 d20021| 2019-04-12T10:29:52.859-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:52.861-0400 d20021| 2019-04-12T10:29:52.860-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:52.934-0400 d20021| 2019-04-12T10:29:52.934-0400 I  INITSYNC [replication-1] initialSyncFuzzerSynchronizationPoint2 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:53.166-0400 d20021| 2019-04-12T10:29:53.166-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:53.166-0400 d20021| 2019-04-12T10:29:53.166-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:53.239-0400 d20021| 2019-04-12T10:29:53.239-0400 I  COMMAND  [repl-writer-worker-9] renameCollectionForApplyOps: rename db2.tmpKpwDl.renameCollection (2072af0a-fd07-46e6-8685-8a32259a9c95) to db2.coll1 and drop fd81e91b-6cae-471a-84ac-12035c9f2c3a.
[js_test:shouldworkRepro] 2019-04-12T10:29:53.239-0400 d20021| 2019-04-12T10:29:53.239-0400 I  SHARDING [repl-writer-worker-9] Marking collection db2.tmpKpwDl.renameCollection as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:53.240-0400 d20021| 2019-04-12T10:29:53.239-0400 I  SHARDING [repl-writer-worker-9] Marking collection db2.coll1 as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:53.240-0400 d20021| 2019-04-12T10:29:53.239-0400 I  STORAGE  [repl-writer-worker-9] renameCollection: renaming collection 99548b69-9a01-48f0-a8e1-dea43b0decee from db2.coll1 to db2.tmpLIlqP.rename
[js_test:shouldworkRepro] 2019-04-12T10:29:53.240-0400 d20021| 2019-04-12T10:29:53.239-0400 I  COMMAND  [repl-writer-worker-9] Successfully renamed the target db2.coll1 (99548b69-9a01-48f0-a8e1-dea43b0decee) to db2.tmpLIlqP.rename so that the source db2.tmpKpwDl.renameCollection (2072af0a-fd07-46e6-8685-8a32259a9c95) could be renamed to db2.coll1
[js_test:shouldworkRepro] 2019-04-12T10:29:53.240-0400 d20021| 2019-04-12T10:29:53.239-0400 I  STORAGE  [repl-writer-worker-9] renameCollection: renaming collection 2072af0a-fd07-46e6-8685-8a32259a9c95 from db2.tmpKpwDl.renameCollection to db2.coll1
[js_test:shouldworkRepro] 2019-04-12T10:29:53.241-0400 d20021| 2019-04-12T10:29:53.240-0400 I  INITSYNC [replication-0] Initial Syncer is about to apply the next oplog batch of size: 1
[js_test:shouldworkRepro] 2019-04-12T10:29:53.241-0400 d20021| 2019-04-12T10:29:53.240-0400 I  INITSYNC [replication-0] initialSyncFuzzerSynchronizationPoint1 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:53.376-0400 d20021| 2019-04-12T10:29:53.376-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:53.377-0400 d20021| 2019-04-12T10:29:53.377-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:53.442-0400 d20021| 2019-04-12T10:29:53.442-0400 I  INITSYNC [replication-0] initialSyncFuzzerSynchronizationPoint2 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:53.684-0400 d20021| 2019-04-12T10:29:53.684-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:53.685-0400 d20021| 2019-04-12T10:29:53.685-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:53.753-0400 d20021| 2019-04-12T10:29:53.753-0400 I  INITSYNC [replication-0] Initial Syncer is about to apply the next oplog batch of size: 1
[js_test:shouldworkRepro] 2019-04-12T10:29:53.754-0400 d20021| 2019-04-12T10:29:53.753-0400 I  INITSYNC [replication-0] initialSyncFuzzerSynchronizationPoint1 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:53.894-0400 d20021| 2019-04-12T10:29:53.894-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:53.895-0400 d20021| 2019-04-12T10:29:53.895-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:53.957-0400 d20021| 2019-04-12T10:29:53.957-0400 I  INITSYNC [replication-0] initialSyncFuzzerSynchronizationPoint2 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:54.202-0400 d20021| 2019-04-12T10:29:54.202-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:54.204-0400 d20021| 2019-04-12T10:29:54.203-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:54.263-0400 d20021| 2019-04-12T10:29:54.263-0400 I  INITSYNC [replication-1] Initial Syncer is about to apply the next oplog batch of size: 1
[js_test:shouldworkRepro] 2019-04-12T10:29:54.263-0400 d20021| 2019-04-12T10:29:54.263-0400 I  INITSYNC [replication-1] initialSyncFuzzerSynchronizationPoint1 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:54.410-0400 d20021| 2019-04-12T10:29:54.410-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:54.411-0400 d20021| 2019-04-12T10:29:54.411-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:54.465-0400 d20021| 2019-04-12T10:29:54.465-0400 I  INITSYNC [replication-1] initialSyncFuzzerSynchronizationPoint2 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:54.713-0400 d20021| 2019-04-12T10:29:54.713-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:54.714-0400 d20021| 2019-04-12T10:29:54.714-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:54.777-0400 d20021| 2019-04-12T10:29:54.777-0400 I  COMMAND  [repl-writer-worker-0] renameCollectionForApplyOps: rename db2.coll1 (2072af0a-fd07-46e6-8685-8a32259a9c95) to db2.coll2.
[js_test:shouldworkRepro] 2019-04-12T10:29:54.778-0400 d20021| 2019-04-12T10:29:54.777-0400 I  STORAGE  [repl-writer-worker-0] renameCollection: renaming collection 2072af0a-fd07-46e6-8685-8a32259a9c95 from db2.coll1 to db2.coll2
[js_test:shouldworkRepro] 2019-04-12T10:29:54.778-0400 d20021| 2019-04-12T10:29:54.778-0400 I  INITSYNC [replication-0] Initial Syncer is about to apply the next oplog batch of size: 1
[js_test:shouldworkRepro] 2019-04-12T10:29:54.778-0400 d20021| 2019-04-12T10:29:54.778-0400 I  INITSYNC [replication-0] initialSyncFuzzerSynchronizationPoint1 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:54.920-0400 d20021| 2019-04-12T10:29:54.919-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:54.921-0400 d20021| 2019-04-12T10:29:54.920-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:54.982-0400 d20021| 2019-04-12T10:29:54.982-0400 I  INITSYNC [replication-0] initialSyncFuzzerSynchronizationPoint2 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:55.229-0400 d20021| 2019-04-12T10:29:55.228-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:55.229-0400 d20021| 2019-04-12T10:29:55.229-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:55.291-0400 d20021| 2019-04-12T10:29:55.291-0400 I  SHARDING [repl-writer-worker-1] Marking collection db1.coll1 as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:55.291-0400 d20021| 2019-04-12T10:29:55.291-0400 I  STORAGE  [repl-writer-worker-1] createCollection: db1.coll1 with provided UUID: 53e9cfbc-0461-4cb7-9fe4-c404b6839cd6
[js_test:shouldworkRepro] 2019-04-12T10:29:55.334-0400 d20021| 2019-04-12T10:29:55.334-0400 I  INDEX    [repl-writer-worker-1] index build: done building index _id_ on ns db1.coll1
[js_test:shouldworkRepro] 2019-04-12T10:29:55.334-0400 d20021| 2019-04-12T10:29:55.334-0400 I  INITSYNC [replication-0] Initial Syncer is about to apply the next oplog batch of size: 1
[js_test:shouldworkRepro] 2019-04-12T10:29:55.334-0400 d20021| 2019-04-12T10:29:55.334-0400 I  INITSYNC [replication-0] initialSyncFuzzerSynchronizationPoint1 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:55.438-0400 d20021| 2019-04-12T10:29:55.437-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:55.438-0400 d20021| 2019-04-12T10:29:55.438-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:55.439-0400 d20021| 2019-04-12T10:29:55.438-0400 I  INITSYNC [replication-0] initialSyncFuzzerSynchronizationPoint2 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:55.440-0400 d20021| 2019-04-12T10:29:55.440-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:55.441-0400 d20021| 2019-04-12T10:29:55.441-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:55.544-0400 d20021| 2019-04-12T10:29:55.544-0400 I  SHARDING [repl-writer-worker-15] Marking collection db1.tmpRxZmH.convertToCapped.coll1 as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:55.545-0400 d20021| 2019-04-12T10:29:55.544-0400 I  STORAGE  [repl-writer-worker-15] createCollection: db1.tmpRxZmH.convertToCapped.coll1 with provided UUID: ec4b65f5-3ddb-4f12-8b09-0cf72100d944
[js_test:shouldworkRepro] 2019-04-12T10:29:55.573-0400 d20021| 2019-04-12T10:29:55.573-0400 I  INDEX    [repl-writer-worker-15] index build: done building index _id_ on ns db1.tmpRxZmH.convertToCapped.coll1
[js_test:shouldworkRepro] 2019-04-12T10:29:55.574-0400 d20021| 2019-04-12T10:29:55.573-0400 I  INITSYNC [replication-1] Initial Syncer is about to apply the next oplog batch of size: 1
[js_test:shouldworkRepro] 2019-04-12T10:29:55.574-0400 d20021| 2019-04-12T10:29:55.573-0400 I  INITSYNC [replication-1] initialSyncFuzzerSynchronizationPoint1 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:55.648-0400 d20021| 2019-04-12T10:29:55.648-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:55.648-0400 d20021| 2019-04-12T10:29:55.648-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:55.674-0400 d20021| 2019-04-12T10:29:55.674-0400 I  INITSYNC [replication-1] initialSyncFuzzerSynchronizationPoint2 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:55.954-0400 d20021| 2019-04-12T10:29:55.954-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:55.955-0400 d20021| 2019-04-12T10:29:55.954-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:55.979-0400 d20021| 2019-04-12T10:29:55.979-0400 I  COMMAND  [repl-writer-worker-5] renameCollectionForApplyOps: rename db1.tmpRxZmH.convertToCapped.coll1 (ec4b65f5-3ddb-4f12-8b09-0cf72100d944) to db1.coll1 and drop 53e9cfbc-0461-4cb7-9fe4-c404b6839cd6.
[js_test:shouldworkRepro] 2019-04-12T10:29:55.979-0400 d20021| 2019-04-12T10:29:55.979-0400 I  STORAGE  [repl-writer-worker-5] dropCollection: db1.coll1 (53e9cfbc-0461-4cb7-9fe4-c404b6839cd6) - storage engine will take ownership of drop-pending collection with optime { ts: Timestamp(1555079388, 4), t: 1 } and commit timestamp Timestamp(1555079388, 4)
[js_test:shouldworkRepro] 2019-04-12T10:29:55.979-0400 d20021| 2019-04-12T10:29:55.979-0400 I  STORAGE  [repl-writer-worker-5] Finishing collection drop for db1.coll1 (53e9cfbc-0461-4cb7-9fe4-c404b6839cd6).
[js_test:shouldworkRepro] 2019-04-12T10:29:55.979-0400 d20021| 2019-04-12T10:29:55.979-0400 I  STORAGE  [repl-writer-worker-5] renameCollection: renaming collection ec4b65f5-3ddb-4f12-8b09-0cf72100d944 from db1.tmpRxZmH.convertToCapped.coll1 to db1.coll1
[js_test:shouldworkRepro] 2019-04-12T10:29:55.979-0400 d20021| 2019-04-12T10:29:55.979-0400 I  STORAGE  [repl-writer-worker-5] Deferring table drop for index '_id_' on collection 'db1.coll1.$_id_ (53e9cfbc-0461-4cb7-9fe4-c404b6839cd6)'. Ident: 'index-28-5001527355485947772', commit timestamp: 'Timestamp(1555079388, 4)'
[js_test:shouldworkRepro] 2019-04-12T10:29:55.979-0400 d20021| 2019-04-12T10:29:55.979-0400 I  STORAGE  [repl-writer-worker-5] Deferring table drop for collection 'db1.coll1' (53e9cfbc-0461-4cb7-9fe4-c404b6839cd6). Ident: collection-27-5001527355485947772, commit timestamp: Timestamp(1555079388, 4)
[js_test:shouldworkRepro] 2019-04-12T10:29:55.980-0400 d20021| 2019-04-12T10:29:55.980-0400 I  INITSYNC [replication-1] Initial Syncer is about to apply the next oplog batch of size: 1
[js_test:shouldworkRepro] 2019-04-12T10:29:55.980-0400 d20021| 2019-04-12T10:29:55.980-0400 I  INITSYNC [replication-1] initialSyncFuzzerSynchronizationPoint1 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:56.163-0400 d20021| 2019-04-12T10:29:56.162-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:56.164-0400 d20021| 2019-04-12T10:29:56.164-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:56.188-0400 d20021| 2019-04-12T10:29:56.188-0400 I  INITSYNC [replication-1] initialSyncFuzzerSynchronizationPoint2 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:56.472-0400 d20021| 2019-04-12T10:29:56.472-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:56.473-0400 d20021| 2019-04-12T10:29:56.473-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:56.494-0400 d20021| 2019-04-12T10:29:56.494-0400 I  STORAGE  [repl-writer-worker-7] renameCollection: renaming collection 99548b69-9a01-48f0-a8e1-dea43b0decee from db2.tmpLIlqP.rename to db2.tmpg4znI.renameCollection
[js_test:shouldworkRepro] 2019-04-12T10:29:56.494-0400 d20021| 2019-04-12T10:29:56.494-0400 I  INITSYNC [replication-1] Initial Syncer is about to apply the next oplog batch of size: 1
[js_test:shouldworkRepro] 2019-04-12T10:29:56.494-0400 d20021| 2019-04-12T10:29:56.494-0400 I  INITSYNC [replication-1] initialSyncFuzzerSynchronizationPoint1 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:56.682-0400 d20021| 2019-04-12T10:29:56.682-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:56.684-0400 d20021| 2019-04-12T10:29:56.684-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:56.701-0400 d20021| 2019-04-12T10:29:56.701-0400 I  INITSYNC [replication-1] initialSyncFuzzerSynchronizationPoint2 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:56.993-0400 d20021| 2019-04-12T10:29:56.993-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:56.994-0400 d20021| 2019-04-12T10:29:56.994-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:57.013-0400 d20021| 2019-04-12T10:29:57.012-0400 I  COMMAND  [repl-writer-worker-9] renameCollectionForApplyOps: rename db2.tmpg4znI.renameCollection (99548b69-9a01-48f0-a8e1-dea43b0decee) to db2.coll1.
[js_test:shouldworkRepro] 2019-04-12T10:29:57.013-0400 d20021| 2019-04-12T10:29:57.013-0400 I  SHARDING [repl-writer-worker-9] Marking collection db2.tmpg4znI.renameCollection as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:57.013-0400 d20021| 2019-04-12T10:29:57.013-0400 I  STORAGE  [repl-writer-worker-9] renameCollection: renaming collection 99548b69-9a01-48f0-a8e1-dea43b0decee from db2.tmpg4znI.renameCollection to db2.coll1
[js_test:shouldworkRepro] 2019-04-12T10:29:57.013-0400 d20021| 2019-04-12T10:29:57.013-0400 I  INITSYNC [replication-1] Initial Syncer is about to apply the next oplog batch of size: 1
[js_test:shouldworkRepro] 2019-04-12T10:29:57.013-0400 d20021| 2019-04-12T10:29:57.013-0400 I  INITSYNC [replication-1] initialSyncFuzzerSynchronizationPoint1 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:57.200-0400 d20021| 2019-04-12T10:29:57.200-0400 I  STORAGE  [TimestampMonitor] Removing drop-pending idents with drop timestamps before timestamp Timestamp(1555079389, 2)
[js_test:shouldworkRepro] 2019-04-12T10:29:57.201-0400 d20021| 2019-04-12T10:29:57.200-0400 I  STORAGE  [TimestampMonitor] Completing drop for ident index-28-5001527355485947772 (ns: db1.coll1.$_id_) with drop timestamp Timestamp(1555079388, 4)
[js_test:shouldworkRepro] 2019-04-12T10:29:57.204-0400 d20021| 2019-04-12T10:29:57.204-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:57.205-0400 d20021| 2019-04-12T10:29:57.205-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:57.209-0400 d20021| 2019-04-12T10:29:57.209-0400 I  STORAGE  [TimestampMonitor] Completing drop for ident collection-27-5001527355485947772 (ns: db1.coll1) with drop timestamp Timestamp(1555079388, 4)
[js_test:shouldworkRepro] 2019-04-12T10:29:57.220-0400 d20021| 2019-04-12T10:29:57.220-0400 I  INITSYNC [replication-1] initialSyncFuzzerSynchronizationPoint2 fail point enabled.
[js_test:shouldworkRepro] 2019-04-12T10:29:57.514-0400 d20021| 2019-04-12T10:29:57.513-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 1, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:57.515-0400 d20021| 2019-04-12T10:29:57.514-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint2 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:57.528-0400 d20021| 2019-04-12T10:29:57.527-0400 I  COMMAND  [repl-writer-worker-11] CMD: drop db1.coll1
[js_test:shouldworkRepro] 2019-04-12T10:29:57.528-0400 d20021| 2019-04-12T10:29:57.528-0400 I  STORAGE  [repl-writer-worker-11] dropCollection: db1.coll1 (ec4b65f5-3ddb-4f12-8b09-0cf72100d944) - storage engine will take ownership of drop-pending collection with optime { ts: Timestamp(1555079389, 3), t: 1 } and commit timestamp Timestamp(1555079389, 3)
[js_test:shouldworkRepro] 2019-04-12T10:29:57.528-0400 d20021| 2019-04-12T10:29:57.528-0400 I  STORAGE  [repl-writer-worker-11] Finishing collection drop for db1.coll1 (ec4b65f5-3ddb-4f12-8b09-0cf72100d944).
[js_test:shouldworkRepro] 2019-04-12T10:29:57.528-0400 d20021| 2019-04-12T10:29:57.528-0400 I  STORAGE  [repl-writer-worker-11] Deferring table drop for index '_id_' on collection 'db1.coll1.$_id_ (ec4b65f5-3ddb-4f12-8b09-0cf72100d944)'. Ident: 'index-30-5001527355485947772', commit timestamp: 'Timestamp(1555079389, 3)'
[js_test:shouldworkRepro] 2019-04-12T10:29:57.528-0400 d20021| 2019-04-12T10:29:57.528-0400 I  STORAGE  [repl-writer-worker-11] Deferring table drop for collection 'db1.coll1' (ec4b65f5-3ddb-4f12-8b09-0cf72100d944). Ident: collection-29-5001527355485947772, commit timestamp: Timestamp(1555079389, 3)
[js_test:shouldworkRepro] 2019-04-12T10:29:57.530-0400 d20021| 2019-04-12T10:29:57.530-0400 I  INITSYNC [replication-0] Finished fetching oplog during initial sync: CallbackCanceled: error in fetcher batch callback: oplog fetcher is shutting down. Last fetched optime: { ts: Timestamp(1555079389, 3), t: 1 }
[js_test:shouldworkRepro] 2019-04-12T10:29:57.530-0400 d20021| 2019-04-12T10:29:57.530-0400 I  INITSYNC [replication-0] Initial sync attempt finishing up.
[js_test:shouldworkRepro] 2019-04-12T10:29:57.530-0400 d20021| 2019-04-12T10:29:57.530-0400 I  INITSYNC [replication-0] Initial Sync Attempt Statistics: { failedInitialSyncAttempts: 0, maxFailedInitialSyncAttempts: 10, initialSyncStart: new Date(1555079387164), initialSyncAttempts: [], fetchedMissingDocs: 0, appliedOps: 11, initialSyncOplogStart: Timestamp(1555079385, 5), initialSyncOplogEnd: Timestamp(1555079389, 3), databases: { databasesCloned: 3, admin: { collections: 2, clonedCollections: 2, start: new Date(1555079388048), end: new Date(1555079390114), elapsedMillis: 2066, admin.system.keys: { documentsToCopy: 2, documentsCopied: 2, indexes: 1, fetchedBatches: 1, start: new Date(1555079388566), end: new Date(1555079389117), elapsedMillis: 551, receivedBatches: 1 }, admin.system.version: { documentsToCopy: 1, documentsCopied: 1, indexes: 1, fetchedBatches: 1, start: new Date(1555079389117), end: new Date(1555079390114), elapsedMillis: 997, receivedBatches: 1 } }, config: { collections: 1, clonedCollections: 1, start: new Date(1555079389705), end: new Date(1555079391153), elapsedMillis: 1448, config.transactions: { documentsToCopy: 0, documentsCopied: 0, indexes: 1, fetchedBatches: 0, start: new Date(1555079390114), end: new Date(1555079391153), elapsedMillis: 1039, receivedBatches: 0 } }, db2: { collections: 2, clonedCollections: 2, start: new Date(1555079390747), end: new Date(1555079392318), elapsedMillis: 1571, db2.coll2: { documentsToCopy: 0, documentsCopied: 0, indexes: 1, fetchedBatches: 0, start: new Date(1555079391153), end: new Date(1555079391719), elapsedMillis: 566, receivedBatches: 0 }, db2.coll1: { documentsToCopy: 0, documentsCopied: 0, indexes: 1, fetchedBatches: 0, start: new Date(1555079391719), end: new Date(1555079392318), elapsedMillis: 599, receivedBatches: 0 } } } }
[js_test:shouldworkRepro] 2019-04-12T10:29:57.531-0400 d20021| 2019-04-12T10:29:57.530-0400 I  STORAGE  [replication-0] Finishing collection drop for local.temp_oplog_buffer (14a74fad-be16-4c0a-aa3b-cbb7300342b9).
[js_test:shouldworkRepro] 2019-04-12T10:29:57.546-0400 d20021| 2019-04-12T10:29:57.546-0400 I  INITSYNC [replication-0] initial sync done; took 10s.
[js_test:shouldworkRepro] 2019-04-12T10:29:57.546-0400 d20021| 2019-04-12T10:29:57.546-0400 I  STORAGE  [replication-0] Triggering the first stable checkpoint. Initial Data: Timestamp(1555079389, 3) PrevStable: Timestamp(0, 0) CurrStable: Timestamp(1555079389, 3)
[js_test:shouldworkRepro] 2019-04-12T10:29:57.547-0400 d20021| 2019-04-12T10:29:57.546-0400 I  REPL     [replication-0] transition to RECOVERING from STARTUP2
[js_test:shouldworkRepro] 2019-04-12T10:29:57.547-0400 d20021| 2019-04-12T10:29:57.546-0400 I  REPL     [replication-0] Starting replication fetcher thread
[js_test:shouldworkRepro] 2019-04-12T10:29:57.547-0400 d20021| 2019-04-12T10:29:57.546-0400 I  REPL     [replication-0] Starting replication applier thread
[js_test:shouldworkRepro] 2019-04-12T10:29:57.547-0400 d20021| 2019-04-12T10:29:57.546-0400 I  REPL     [replication-0] Starting replication reporter thread
[js_test:shouldworkRepro] 2019-04-12T10:29:57.547-0400 d20021| 2019-04-12T10:29:57.546-0400 I  REPL     [rsSync-0] Starting oplog application
[js_test:shouldworkRepro] 2019-04-12T10:29:57.547-0400 d20021| 2019-04-12T10:29:57.547-0400 I  REPL     [rsBackgroundSync] could not find member to sync from
[js_test:shouldworkRepro] 2019-04-12T10:29:57.547-0400 d20021| 2019-04-12T10:29:57.547-0400 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
[js_test:shouldworkRepro] 2019-04-12T10:29:57.547-0400 d20021| 2019-04-12T10:29:57.547-0400 I  REPL     [rsSync-0] Resetting sync source to empty, which was :27017
[js_test:shouldworkRepro] 2019-04-12T10:29:57.722-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:57.722-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:57.722-0400 [jsTest] ----
[js_test:shouldworkRepro] 2019-04-12T10:29:57.722-0400 [jsTest] Transitioning to: "kInitialSyncCompleted"
[js_test:shouldworkRepro] 2019-04-12T10:29:57.722-0400 [jsTest] ----
[js_test:shouldworkRepro] 2019-04-12T10:29:57.722-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:57.722-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:57.722-0400 d20021| 2019-04-12T10:29:57.722-0400 W  COMMAND  [conn1] failpoint: initialSyncFuzzerSynchronizationPoint1 set to: { mode: 0, data: {} }
[js_test:shouldworkRepro] 2019-04-12T10:29:57.723-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:57.723-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:57.723-0400 [jsTest] ----
[js_test:shouldworkRepro] 2019-04-12T10:29:57.723-0400 [jsTest] Transitioning to: "kStopped"
[js_test:shouldworkRepro] 2019-04-12T10:29:57.724-0400 [jsTest] ----
[js_test:shouldworkRepro] 2019-04-12T10:29:57.724-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:57.724-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:57.733-0400 d20020| 2019-04-12T10:29:57.733-0400 I  COMMAND  [conn1] CMD fsync: sync:1 lock:1
[js_test:shouldworkRepro] 2019-04-12T10:29:57.847-0400 d20020| 2019-04-12T10:29:57.846-0400 I  COMMAND  [conn1] mongod is locked and no writes are allowed. db.fsyncUnlock() to unlock
[js_test:shouldworkRepro] 2019-04-12T10:29:57.847-0400 d20020| 2019-04-12T10:29:57.846-0400 I  COMMAND  [conn1] Lock count is 1
[js_test:shouldworkRepro] 2019-04-12T10:29:57.847-0400 d20020| 2019-04-12T10:29:57.847-0400 I  COMMAND  [conn1]     For more info see http://dochub.mongodb.org/core/fsynccommand
[js_test:shouldworkRepro] 2019-04-12T10:29:57.847-0400 d20020| 2019-04-12T10:29:57.847-0400 I  COMMAND  [conn1] command admin.$cmd appName: "MongoDB Shell" command: fsync { fsync: 1.0, lock: 1.0, allowFsyncFailure: true, lsid: { id: UUID("939ef704-1682-44c8-9bdb-67cba9e46184") }, $clusterTime: { clusterTime: Timestamp(1555079397, 3), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $readPreference: { mode: "secondaryPreferred" }, $db: "admin" } numYields:0 reslen:307 locks:{ Mutex: { acquireCount: { W: 1 } } } protocol:op_msg 113ms
[js_test:shouldworkRepro] 2019-04-12T10:29:57.852-0400 ReplSetTest awaitReplication: starting: optime for primary, vladmac:20020, is { "ts" : Timestamp(1555079397, 3), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:57.852-0400 ReplSetTest awaitReplication: checking secondaries against latest primary optime { "ts" : Timestamp(1555079397, 3), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:57.853-0400 ReplSetTest awaitReplication: checking secondary #1: vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:57.854-0400 ReplSetTest awaitReplication: optime for secondary #1, vladmac:20021, is { "ts" : Timestamp(1555079389, 3), "t" : NumberLong(1) } but latest is { "ts" : Timestamp(1555079397, 3), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:57.854-0400 ReplSetTest awaitReplication: secondary #1, vladmac:20021, is NOT synced
[js_test:shouldworkRepro] 2019-04-12T10:29:58.055-0400 ReplSetTest awaitReplication: checking secondaries against latest primary optime { "ts" : Timestamp(1555079397, 3), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:58.057-0400 ReplSetTest awaitReplication: checking secondary #1: vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:58.059-0400 ReplSetTest awaitReplication: optime for secondary #1, vladmac:20021, is { "ts" : Timestamp(1555079389, 3), "t" : NumberLong(1) } but latest is { "ts" : Timestamp(1555079397, 3), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:58.059-0400 ReplSetTest awaitReplication: secondary #1, vladmac:20021, is NOT synced
[js_test:shouldworkRepro] 2019-04-12T10:29:58.064-0400 d20020| 2019-04-12T10:29:58.064-0400 I  STORAGE  [TimestampMonitor] Removing drop-pending idents with drop timestamps before timestamp Timestamp(1555079392, 3)
[js_test:shouldworkRepro] 2019-04-12T10:29:58.263-0400 ReplSetTest awaitReplication: checking secondaries against latest primary optime { "ts" : Timestamp(1555079397, 3), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:58.266-0400 ReplSetTest awaitReplication: checking secondary #1: vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:58.269-0400 ReplSetTest awaitReplication: optime for secondary #1, vladmac:20021, is { "ts" : Timestamp(1555079389, 3), "t" : NumberLong(1) } but latest is { "ts" : Timestamp(1555079397, 3), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:58.269-0400 ReplSetTest awaitReplication: secondary #1, vladmac:20021, is NOT synced
[js_test:shouldworkRepro] 2019-04-12T10:29:58.472-0400 ReplSetTest awaitReplication: checking secondaries against latest primary optime { "ts" : Timestamp(1555079397, 3), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:58.473-0400 ReplSetTest awaitReplication: checking secondary #1: vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:58.474-0400 ReplSetTest awaitReplication: optime for secondary #1, vladmac:20021, is { "ts" : Timestamp(1555079389, 3), "t" : NumberLong(1) } but latest is { "ts" : Timestamp(1555079397, 3), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:58.474-0400 ReplSetTest awaitReplication: secondary #1, vladmac:20021, is NOT synced
[js_test:shouldworkRepro] 2019-04-12T10:29:58.547-0400 d20021| 2019-04-12T10:29:58.547-0400 I  REPL     [rsBackgroundSync] sync source candidate: vladmac:20020
[js_test:shouldworkRepro] 2019-04-12T10:29:58.548-0400 d20021| 2019-04-12T10:29:58.548-0400 I  REPL     [rsBackgroundSync] Changed sync source from empty to vladmac:20020
[js_test:shouldworkRepro] 2019-04-12T10:29:58.548-0400 d20021| 2019-04-12T10:29:58.548-0400 I  REPL     [rsBackgroundSync] scheduling fetcher to read remote oplog on vladmac:20020 starting at filter: { ts: { $gte: Timestamp(1555079389, 3) } }
[js_test:shouldworkRepro] 2019-04-12T10:29:58.548-0400 d20021| 2019-04-12T10:29:58.548-0400 I  CONNPOOL [RS] Connecting to vladmac:20020
[js_test:shouldworkRepro] 2019-04-12T10:29:58.549-0400 d20021| 2019-04-12T10:29:58.549-0400 I  SHARDING [repl-writer-worker-4] Marking collection config.transactions as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:58.550-0400 d20020| 2019-04-12T10:29:58.549-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61869 #22 (5 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:58.550-0400 d20020| 2019-04-12T10:29:58.550-0400 I  NETWORK  [conn22] received client metadata from 127.0.0.1:61869 conn22: { driver: { name: "NetworkInterfaceTL", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:58.552-0400 d20021| 2019-04-12T10:29:58.552-0400 I  SHARDING [repl-writer-worker-1] Marking collection db2.coll2 as collection version: <unsharded>
[js_test:shouldworkRepro] 2019-04-12T10:29:58.675-0400 ReplSetTest awaitReplication: checking secondaries against latest primary optime { "ts" : Timestamp(1555079397, 3), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:58.677-0400 ReplSetTest awaitReplication: checking secondary #1: vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:58.678-0400 ReplSetTest awaitReplication: secondary #1, vladmac:20021, is synced
[js_test:shouldworkRepro] 2019-04-12T10:29:58.678-0400 ReplSetTest awaitReplication: finished: all 1 secondaries synced at optime { "ts" : Timestamp(1555079397, 3), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:58.708-0400 d20020| 2019-04-12T10:29:58.708-0400 I  COMMAND  [conn1] command: unlock requested
[js_test:shouldworkRepro] 2019-04-12T10:29:58.708-0400 d20020| 2019-04-12T10:29:58.708-0400 I  STORAGE  [TimestampMonitor] Completing drop for ident index-20--3534291432697553977 (ns: db1.coll2.$_id_) with drop timestamp Timestamp(1555079385, 5)
[js_test:shouldworkRepro] 2019-04-12T10:29:58.708-0400 d20020| 2019-04-12T10:29:58.708-0400 I  COMMAND  [conn1] fsyncUnlock completed. mongod is now unlocked and free to accept writes
[js_test:shouldworkRepro] 2019-04-12T10:29:58.715-0400 d20020| 2019-04-12T10:29:58.715-0400 I  COMMAND  [conn1] CMD fsync: sync:1 lock:1
[js_test:shouldworkRepro] 2019-04-12T10:29:58.724-0400 d20020| 2019-04-12T10:29:58.724-0400 I  STORAGE  [TimestampMonitor] Completing drop for ident collection-19--3534291432697553977 (ns: db1.coll2) with drop timestamp Timestamp(1555079385, 5)
[js_test:shouldworkRepro] 2019-04-12T10:29:58.742-0400 d20020| 2019-04-12T10:29:58.742-0400 I  STORAGE  [TimestampMonitor] Completing drop for ident index-22--3534291432697553977 (ns: db2.coll1.$_id_) with drop timestamp Timestamp(1555079387, 2)
[js_test:shouldworkRepro] 2019-04-12T10:29:58.749-0400 d20020| 2019-04-12T10:29:58.749-0400 I  STORAGE  [TimestampMonitor] Completing drop for ident collection-21--3534291432697553977 (ns: db2.coll1) with drop timestamp Timestamp(1555079387, 2)
[js_test:shouldworkRepro] 2019-04-12T10:29:58.756-0400 d20020| 2019-04-12T10:29:58.756-0400 I  STORAGE  [TimestampMonitor] Completing drop for ident index-26--3534291432697553977 (ns: db1.coll2.$_id_) with drop timestamp Timestamp(1555079387, 3)
[js_test:shouldworkRepro] 2019-04-12T10:29:58.762-0400 d20020| 2019-04-12T10:29:58.762-0400 I  STORAGE  [TimestampMonitor] Completing drop for ident collection-25--3534291432697553977 (ns: db1.coll2) with drop timestamp Timestamp(1555079387, 3)
[js_test:shouldworkRepro] 2019-04-12T10:29:58.769-0400 d20020| 2019-04-12T10:29:58.769-0400 I  STORAGE  [TimestampMonitor] Completing drop for ident index-24--3534291432697553977 (ns: db2.coll2.$_id_) with drop timestamp Timestamp(1555079387, 4)
[js_test:shouldworkRepro] 2019-04-12T10:29:58.776-0400 d20020| 2019-04-12T10:29:58.776-0400 I  STORAGE  [TimestampMonitor] Completing drop for ident collection-23--3534291432697553977 (ns: db2.coll2) with drop timestamp Timestamp(1555079387, 4)
[js_test:shouldworkRepro] 2019-04-12T10:29:58.783-0400 d20020| 2019-04-12T10:29:58.782-0400 I  STORAGE  [TimestampMonitor] Completing drop for ident index-30--3534291432697553977 (ns: db1.coll1.$_id_) with drop timestamp Timestamp(1555079388, 4)
[js_test:shouldworkRepro] 2019-04-12T10:29:58.789-0400 d20020| 2019-04-12T10:29:58.789-0400 I  STORAGE  [TimestampMonitor] Completing drop for ident collection-29--3534291432697553977 (ns: db1.coll1) with drop timestamp Timestamp(1555079388, 4)
[js_test:shouldworkRepro] 2019-04-12T10:29:58.795-0400 d20020| 2019-04-12T10:29:58.795-0400 I  STORAGE  [TimestampMonitor] Completing drop for ident index-32--3534291432697553977 (ns: db1.coll1.$_id_) with drop timestamp Timestamp(1555079389, 3)
[js_test:shouldworkRepro] 2019-04-12T10:29:58.807-0400 d20020| 2019-04-12T10:29:58.807-0400 I  STORAGE  [TimestampMonitor] Completing drop for ident collection-31--3534291432697553977 (ns: db1.coll1) with drop timestamp Timestamp(1555079389, 3)
[js_test:shouldworkRepro] 2019-04-12T10:29:58.890-0400 d20020| 2019-04-12T10:29:58.890-0400 I  COMMAND  [conn1] mongod is locked and no writes are allowed. db.fsyncUnlock() to unlock
[js_test:shouldworkRepro] 2019-04-12T10:29:58.890-0400 d20020| 2019-04-12T10:29:58.890-0400 I  COMMAND  [conn1] Lock count is 1
[js_test:shouldworkRepro] 2019-04-12T10:29:58.890-0400 d20020| 2019-04-12T10:29:58.890-0400 I  COMMAND  [conn1]     For more info see http://dochub.mongodb.org/core/fsynccommand
[js_test:shouldworkRepro] 2019-04-12T10:29:58.891-0400 d20020| 2019-04-12T10:29:58.890-0400 I  COMMAND  [conn1] command admin.$cmd appName: "MongoDB Shell" command: fsync { fsync: 1.0, lock: 1.0, allowFsyncFailure: true, lsid: { id: UUID("939ef704-1682-44c8-9bdb-67cba9e46184") }, $clusterTime: { clusterTime: Timestamp(1555079398, 3), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $readPreference: { mode: "secondaryPreferred" }, $db: "admin" } numYields:0 reslen:307 locks:{ Mutex: { acquireCount: { W: 1 } } } protocol:op_msg 174ms
[js_test:shouldworkRepro] 2019-04-12T10:29:58.891-0400 ReplSetTest awaitReplication: going to check only vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:58.898-0400 ReplSetTest awaitReplication: starting: optime for primary, vladmac:20020, is { "ts" : Timestamp(1555079398, 3), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:58.898-0400 ReplSetTest awaitReplication: checking secondaries against latest primary optime { "ts" : Timestamp(1555079398, 3), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:58.899-0400 ReplSetTest awaitReplication: checking secondary #1: vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:58.900-0400 ReplSetTest awaitReplication: secondary #1, vladmac:20021, is synced
[js_test:shouldworkRepro] 2019-04-12T10:29:58.900-0400 ReplSetTest awaitReplication: finished: all 1 secondaries synced at optime { "ts" : Timestamp(1555079398, 3), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:58.905-0400 d20020| 2019-04-12T10:29:58.905-0400 I  COMMAND  [conn1] command: unlock requested
[js_test:shouldworkRepro] 2019-04-12T10:29:58.906-0400 d20020| 2019-04-12T10:29:58.905-0400 I  COMMAND  [conn1] fsyncUnlock completed. mongod is now unlocked and free to accept writes
[js_test:shouldworkRepro] 2019-04-12T10:29:58.913-0400 d20020| 2019-04-12T10:29:58.913-0400 I  COMMAND  [conn1] CMD fsync: sync:1 lock:1
[js_test:shouldworkRepro] 2019-04-12T10:29:59.010-0400 d20020| 2019-04-12T10:29:59.010-0400 I  COMMAND  [conn1] mongod is locked and no writes are allowed. db.fsyncUnlock() to unlock
[js_test:shouldworkRepro] 2019-04-12T10:29:59.010-0400 d20020| 2019-04-12T10:29:59.010-0400 I  COMMAND  [conn1] Lock count is 1
[js_test:shouldworkRepro] 2019-04-12T10:29:59.011-0400 d20020| 2019-04-12T10:29:59.010-0400 I  COMMAND  [conn1]     For more info see http://dochub.mongodb.org/core/fsynccommand
[js_test:shouldworkRepro] 2019-04-12T10:29:59.011-0400 ReplSetTest awaitReplication: going to check only vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:59.016-0400 ReplSetTest awaitReplication: starting: optime for primary, vladmac:20020, is { "ts" : Timestamp(1555079398, 6), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:59.016-0400 ReplSetTest awaitReplication: checking secondaries against latest primary optime { "ts" : Timestamp(1555079398, 6), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:59.017-0400 ReplSetTest awaitReplication: checking secondary #1: vladmac:20021
[js_test:shouldworkRepro] 2019-04-12T10:29:59.018-0400 ReplSetTest awaitReplication: secondary #1, vladmac:20021, is synced
[js_test:shouldworkRepro] 2019-04-12T10:29:59.018-0400 ReplSetTest awaitReplication: finished: all 1 secondaries synced at optime { "ts" : Timestamp(1555079398, 6), "t" : NumberLong(1) }
[js_test:shouldworkRepro] 2019-04-12T10:29:59.053-0400 d20020| 2019-04-12T10:29:59.053-0400 I  COMMAND  [conn1] command: unlock requested
[js_test:shouldworkRepro] 2019-04-12T10:29:59.053-0400 d20020| 2019-04-12T10:29:59.053-0400 I  COMMAND  [conn1] fsyncUnlock completed. mongod is now unlocked and free to accept writes
[js_test:shouldworkRepro] 2019-04-12T10:29:59.053-0400 ReplSetTest stop *** Shutting down mongod in port 20020 ***
[js_test:shouldworkRepro] 2019-04-12T10:29:59.054-0400 d20020| 2019-04-12T10:29:59.054-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61870 #23 (6 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.054-0400 d20020| 2019-04-12T10:29:59.054-0400 I  NETWORK  [conn23] received client metadata from 127.0.0.1:61870 conn23: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:29:59.055-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:59.055-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:59.055-0400 [jsTest] ----
[js_test:shouldworkRepro] 2019-04-12T10:29:59.055-0400 [jsTest] New session started with sessionID: { "id" : UUID("eac2d43e-0e71-4666-b612-c96a1880d46e") }
[js_test:shouldworkRepro] 2019-04-12T10:29:59.055-0400 [jsTest] ----
[js_test:shouldworkRepro] 2019-04-12T10:29:59.055-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:59.055-0400 
[js_test:shouldworkRepro] 2019-04-12T10:29:59.056-0400 d20020| 2019-04-12T10:29:59.055-0400 I  COMMAND  [conn23] Attempting to step down in response to replSetStepDown command
[js_test:shouldworkRepro] 2019-04-12T10:29:59.056-0400 d20020| 2019-04-12T10:29:59.056-0400 I  REPL     [RstlKillOpthread] Starting to kill user operations
[js_test:shouldworkRepro] 2019-04-12T10:29:59.056-0400 d20020| 2019-04-12T10:29:59.056-0400 I  REPL     [RstlKillOpthread] Stopped killing user operations
[js_test:shouldworkRepro] 2019-04-12T10:29:59.056-0400 d20020| 2019-04-12T10:29:59.056-0400 I  REPL     [conn23] transition to SECONDARY from PRIMARY
[js_test:shouldworkRepro] 2019-04-12T10:29:59.056-0400 d20020| 2019-04-12T10:29:59.056-0400 I  REPL     [conn23] Successfully stepped down from primary, stats: { userOpsKilled: 0, userOpsRunning: 1 }
[js_test:shouldworkRepro] 2019-04-12T10:29:59.056-0400 d20020| 2019-04-12T10:29:59.056-0400 I  REPL     [conn23] 'freezing' for 86400 seconds
[js_test:shouldworkRepro] 2019-04-12T10:29:59.061-0400 d20020| 2019-04-12T10:29:59.061-0400 I  COMMAND  [conn23] CMD: validate admin.system.keys
[js_test:shouldworkRepro] 2019-04-12T10:29:59.062-0400 d20020| 2019-04-12T10:29:59.061-0400 I  INDEX    [conn23] validating collection admin.system.keys (UUID: 344947af-588d-4a6c-95ed-fc25d1d38bfc)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.062-0400 d20020| 2019-04-12T10:29:59.062-0400 I  INDEX    [conn23] validating index admin.system.keys.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:29:59.063-0400 d20020| 2019-04-12T10:29:59.063-0400 I  INDEX    [conn23] validated collection admin.system.keys (UUID: 344947af-588d-4a6c-95ed-fc25d1d38bfc)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.063-0400 d20020| 2019-04-12T10:29:59.063-0400 I  COMMAND  [conn23] CMD: validate admin.system.version
[js_test:shouldworkRepro] 2019-04-12T10:29:59.063-0400 d20020| 2019-04-12T10:29:59.063-0400 I  INDEX    [conn23] validating collection admin.system.version (UUID: 79fe740f-34c2-436a-b791-66b5f56f6718)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.065-0400 d20020| 2019-04-12T10:29:59.065-0400 I  INDEX    [conn23] validating index admin.system.version.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:29:59.066-0400 d20020| 2019-04-12T10:29:59.066-0400 I  INDEX    [conn23] validated collection admin.system.version (UUID: 79fe740f-34c2-436a-b791-66b5f56f6718)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.068-0400 d20020| 2019-04-12T10:29:59.068-0400 I  COMMAND  [conn23] CMD: validate config.transactions
[js_test:shouldworkRepro] 2019-04-12T10:29:59.068-0400 d20020| 2019-04-12T10:29:59.068-0400 I  INDEX    [conn23] validating collection config.transactions (UUID: a6348a5e-ec46-4ef4-9436-f5d70a411e91)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.068-0400 d20020| 2019-04-12T10:29:59.068-0400 I  INDEX    [conn23] validating index config.transactions.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:29:59.069-0400 d20020| 2019-04-12T10:29:59.068-0400 W  STORAGE  [conn23] Could not complete validation of table:index-16--3534291432697553977. This is a transient issue as the collection was actively in use by other operations.
[js_test:shouldworkRepro] 2019-04-12T10:29:59.069-0400 d20020| 2019-04-12T10:29:59.069-0400 I  INDEX    [conn23] validated collection config.transactions (UUID: a6348a5e-ec46-4ef4-9436-f5d70a411e91)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.070-0400 d20020| 2019-04-12T10:29:59.070-0400 I  COMMAND  [conn23] CMD: validate db2.coll1
[js_test:shouldworkRepro] 2019-04-12T10:29:59.070-0400 d20020| 2019-04-12T10:29:59.070-0400 I  INDEX    [conn23] validating collection db2.coll1 (UUID: 99548b69-9a01-48f0-a8e1-dea43b0decee)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.071-0400 d20020| 2019-04-12T10:29:59.071-0400 I  INDEX    [conn23] validating index db2.coll1.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:29:59.071-0400 d20020| 2019-04-12T10:29:59.071-0400 W  STORAGE  [conn23] Could not complete validation of table:index-34--3534291432697553977. This is a transient issue as the collection was actively in use by other operations.
[js_test:shouldworkRepro] 2019-04-12T10:29:59.071-0400 d20020| 2019-04-12T10:29:59.071-0400 I  INDEX    [conn23] validated collection db2.coll1 (UUID: 99548b69-9a01-48f0-a8e1-dea43b0decee)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.071-0400 d20020| 2019-04-12T10:29:59.071-0400 I  COMMAND  [conn23] CMD: validate db2.coll2
[js_test:shouldworkRepro] 2019-04-12T10:29:59.072-0400 d20020| 2019-04-12T10:29:59.071-0400 I  INDEX    [conn23] validating collection db2.coll2 (UUID: 2072af0a-fd07-46e6-8685-8a32259a9c95)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.072-0400 d20020| 2019-04-12T10:29:59.072-0400 I  INDEX    [conn23] validating index db2.coll2.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:29:59.072-0400 d20020| 2019-04-12T10:29:59.072-0400 W  STORAGE  [conn23] Could not complete validation of table:index-28--3534291432697553977. This is a transient issue as the collection was actively in use by other operations.
[js_test:shouldworkRepro] 2019-04-12T10:29:59.072-0400 d20020| 2019-04-12T10:29:59.072-0400 I  INDEX    [conn23] validated collection db2.coll2 (UUID: 2072af0a-fd07-46e6-8685-8a32259a9c95)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.073-0400 d20020| 2019-04-12T10:29:59.073-0400 I  COMMAND  [conn23] CMD: validate local.oplog.rs
[js_test:shouldworkRepro] 2019-04-12T10:29:59.073-0400 d20020| 2019-04-12T10:29:59.073-0400 I  INDEX    [conn23] validating collection local.oplog.rs (UUID: afad9498-39cf-4f83-bbe7-349638c18dd7)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.073-0400 d20020| 2019-04-12T10:29:59.073-0400 W  STORAGE  [conn23] Could not complete validation of table:collection-8--3534291432697553977. This is a transient issue as the collection was actively in use by other operations.
[js_test:shouldworkRepro] 2019-04-12T10:29:59.073-0400 d20020| 2019-04-12T10:29:59.073-0400 I  INDEX    [conn23] validated collection local.oplog.rs (UUID: afad9498-39cf-4f83-bbe7-349638c18dd7)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.074-0400 d20020| 2019-04-12T10:29:59.074-0400 I  COMMAND  [conn23] CMD: validate local.replset.election
[js_test:shouldworkRepro] 2019-04-12T10:29:59.074-0400 d20020| 2019-04-12T10:29:59.074-0400 I  INDEX    [conn23] validating collection local.replset.election (UUID: 262526c2-05ef-4187-9e13-4fa7027d2337)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.075-0400 d20020| 2019-04-12T10:29:59.075-0400 I  INDEX    [conn23] validating index local.replset.election.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:29:59.076-0400 d20020| 2019-04-12T10:29:59.076-0400 I  INDEX    [conn23] validated collection local.replset.election (UUID: 262526c2-05ef-4187-9e13-4fa7027d2337)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.077-0400 d20020| 2019-04-12T10:29:59.077-0400 I  COMMAND  [conn23] CMD: validate local.replset.minvalid
[js_test:shouldworkRepro] 2019-04-12T10:29:59.077-0400 d20020| 2019-04-12T10:29:59.077-0400 I  INDEX    [conn23] validating collection local.replset.minvalid (UUID: fb182528-7039-41c8-ba91-e1ce8b426c4d)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.077-0400 d20020| 2019-04-12T10:29:59.077-0400 W  STORAGE  [conn23] Could not complete validation of table:collection-4--3534291432697553977. This is a transient issue as the collection was actively in use by other operations.
[js_test:shouldworkRepro] 2019-04-12T10:29:59.077-0400 d20020| 2019-04-12T10:29:59.077-0400 I  INDEX    [conn23] validating index local.replset.minvalid.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:29:59.078-0400 d20020| 2019-04-12T10:29:59.078-0400 I  INDEX    [conn23] validated collection local.replset.minvalid (UUID: fb182528-7039-41c8-ba91-e1ce8b426c4d)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.078-0400 d20020| 2019-04-12T10:29:59.078-0400 I  COMMAND  [conn23] CMD: validate local.replset.oplogTruncateAfterPoint
[js_test:shouldworkRepro] 2019-04-12T10:29:59.078-0400 d20020| 2019-04-12T10:29:59.078-0400 I  INDEX    [conn23] validating collection local.replset.oplogTruncateAfterPoint (UUID: c21a565e-6449-4e31-96f3-cde8961d155d)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.079-0400 d20020| 2019-04-12T10:29:59.079-0400 I  INDEX    [conn23] validating index local.replset.oplogTruncateAfterPoint.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:29:59.098-0400 d20020| 2019-04-12T10:29:59.098-0400 I  INDEX    [conn23] validated collection local.replset.oplogTruncateAfterPoint (UUID: c21a565e-6449-4e31-96f3-cde8961d155d)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.099-0400 d20020| 2019-04-12T10:29:59.099-0400 I  COMMAND  [conn23] CMD: validate local.startup_log
[js_test:shouldworkRepro] 2019-04-12T10:29:59.099-0400 d20020| 2019-04-12T10:29:59.099-0400 I  INDEX    [conn23] validating collection local.startup_log (UUID: 36a8a47e-33f4-4c85-9009-1ded81383630)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.101-0400 d20020| 2019-04-12T10:29:59.101-0400 I  INDEX    [conn23] validating index local.startup_log.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:29:59.102-0400 d20020| 2019-04-12T10:29:59.102-0400 I  INDEX    [conn23] validated collection local.startup_log (UUID: 36a8a47e-33f4-4c85-9009-1ded81383630)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.103-0400 d20020| 2019-04-12T10:29:59.103-0400 I  COMMAND  [conn23] CMD: validate local.system.replset
[js_test:shouldworkRepro] 2019-04-12T10:29:59.103-0400 d20020| 2019-04-12T10:29:59.103-0400 I  INDEX    [conn23] validating collection local.system.replset (UUID: 16b3ac43-be77-4341-b45a-ff43144f3242)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.105-0400 d20020| 2019-04-12T10:29:59.105-0400 I  INDEX    [conn23] validating index local.system.replset.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:29:59.106-0400 d20020| 2019-04-12T10:29:59.106-0400 I  INDEX    [conn23] validated collection local.system.replset (UUID: 16b3ac43-be77-4341-b45a-ff43144f3242)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.106-0400 d20020| 2019-04-12T10:29:59.106-0400 I  COMMAND  [conn23] CMD: validate local.system.rollback.id
[js_test:shouldworkRepro] 2019-04-12T10:29:59.106-0400 d20020| 2019-04-12T10:29:59.106-0400 I  INDEX    [conn23] validating collection local.system.rollback.id (UUID: bad7ac5c-8865-4ae4-be02-4d2754fc9a6e)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.108-0400 d20020| 2019-04-12T10:29:59.108-0400 I  REPL     [replexec-2] Member vladmac:20021 is now in state SECONDARY
[js_test:shouldworkRepro] 2019-04-12T10:29:59.108-0400 d20020| 2019-04-12T10:29:59.108-0400 I  INDEX    [conn23] validating index local.system.rollback.id.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:29:59.109-0400 d20020| 2019-04-12T10:29:59.109-0400 I  INDEX    [conn23] validated collection local.system.rollback.id (UUID: bad7ac5c-8865-4ae4-be02-4d2754fc9a6e)
[js_test:shouldworkRepro] 2019-04-12T10:29:59.110-0400 d20020| 2019-04-12T10:29:59.110-0400 I  CONTROL  [signalProcessingThread] got signal 15 (Terminated: 15), will terminate after current cmd ends
[js_test:shouldworkRepro] 2019-04-12T10:29:59.110-0400 d20020| 2019-04-12T10:29:59.110-0400 I  NETWORK  [signalProcessingThread] shutdown: going to close listening sockets...
[js_test:shouldworkRepro] 2019-04-12T10:29:59.110-0400 d20020| 2019-04-12T10:29:59.110-0400 I  NETWORK  [signalProcessingThread] removing socket file: /tmp/mongodb-20020.sock
[js_test:shouldworkRepro] 2019-04-12T10:29:59.110-0400 d20020| 2019-04-12T10:29:59.110-0400 I  REPL     [signalProcessingThread] shutting down replication subsystems
[js_test:shouldworkRepro] 2019-04-12T10:29:59.111-0400 d20020| 2019-04-12T10:29:59.110-0400 I  REPL     [signalProcessingThread] Stopping replication reporter thread
[js_test:shouldworkRepro] 2019-04-12T10:29:59.111-0400 d20020| 2019-04-12T10:29:59.110-0400 I  REPL     [signalProcessingThread] Stopping replication fetcher thread
[js_test:shouldworkRepro] 2019-04-12T10:29:59.111-0400 d20020| 2019-04-12T10:29:59.110-0400 I  REPL     [signalProcessingThread] Stopping replication applier thread
[js_test:shouldworkRepro] 2019-04-12T10:29:59.111-0400 d20020| 2019-04-12T10:29:59.111-0400 I  REPL     [rsSync-0] Finished oplog application
[js_test:shouldworkRepro] 2019-04-12T10:30:00.071-0400 d20020| 2019-04-12T10:30:00.071-0400 I  REPL     [rsBackgroundSync] Stopping replication producer
[js_test:shouldworkRepro] 2019-04-12T10:30:00.071-0400 d20020| 2019-04-12T10:30:00.071-0400 I  REPL     [signalProcessingThread] Stopping replication storage threads
[js_test:shouldworkRepro] 2019-04-12T10:30:00.072-0400 d20020| 2019-04-12T10:30:00.071-0400 I  ASIO     [RS] Killing all outstanding egress activity.
[js_test:shouldworkRepro] 2019-04-12T10:30:00.072-0400 d20020| 2019-04-12T10:30:00.071-0400 I  ASIO     [RS] Killing all outstanding egress activity.
[js_test:shouldworkRepro] 2019-04-12T10:30:00.073-0400 d20020| 2019-04-12T10:30:00.072-0400 I  ASIO     [Replication] Killing all outstanding egress activity.
[js_test:shouldworkRepro] 2019-04-12T10:30:00.073-0400 d20020| 2019-04-12T10:30:00.073-0400 I  CONNPOOL [Replication] Dropping all pooled connections to vladmac:20021 due to ShutdownInProgress: Shutting down the connection pool
[js_test:shouldworkRepro] 2019-04-12T10:30:00.073-0400 d20021| 2019-04-12T10:30:00.073-0400 I  NETWORK  [conn2] end connection 127.0.0.1:61854 (1 connection now open)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.073-0400 d20020| 2019-04-12T10:30:00.073-0400 I  CONTROL  [signalProcessingThread] Shutting down free monitoring
[js_test:shouldworkRepro] 2019-04-12T10:30:00.073-0400 d20020| 2019-04-12T10:30:00.073-0400 I  FTDC     [signalProcessingThread] Shutting down full-time diagnostic data capture
[js_test:shouldworkRepro] 2019-04-12T10:30:00.077-0400 d20020| 2019-04-12T10:30:00.077-0400 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
[js_test:shouldworkRepro] 2019-04-12T10:30:00.077-0400 d20020| 2019-04-12T10:30:00.077-0400 I  STORAGE  [signalProcessingThread] Timestamp monitor shutting down
[js_test:shouldworkRepro] 2019-04-12T10:30:00.077-0400 d20020| 2019-04-12T10:30:00.077-0400 I  STORAGE  [signalProcessingThread] WiredTigerKVEngine shutting down
[js_test:shouldworkRepro] 2019-04-12T10:30:00.085-0400 d20020| 2019-04-12T10:30:00.084-0400 I  STORAGE  [signalProcessingThread] Shutting down session sweeper thread
[js_test:shouldworkRepro] 2019-04-12T10:30:00.085-0400 d20020| 2019-04-12T10:30:00.084-0400 I  STORAGE  [signalProcessingThread] Finished shutting down session sweeper thread
[js_test:shouldworkRepro] 2019-04-12T10:30:00.085-0400 d20020| 2019-04-12T10:30:00.084-0400 I  STORAGE  [signalProcessingThread] Shutting down journal flusher thread
[js_test:shouldworkRepro] 2019-04-12T10:30:00.121-0400 d20020| 2019-04-12T10:30:00.121-0400 I  STORAGE  [signalProcessingThread] Finished shutting down journal flusher thread
[js_test:shouldworkRepro] 2019-04-12T10:30:00.121-0400 d20020| 2019-04-12T10:30:00.121-0400 I  STORAGE  [signalProcessingThread] Shutting down checkpoint thread
[js_test:shouldworkRepro] 2019-04-12T10:30:00.121-0400 d20020| 2019-04-12T10:30:00.121-0400 I  STORAGE  [signalProcessingThread] Finished shutting down checkpoint thread
[js_test:shouldworkRepro] 2019-04-12T10:30:00.192-0400 d20020| 2019-04-12T10:30:00.192-0400 I  STORAGE  [signalProcessingThread] shutdown: removing fs lock...
[js_test:shouldworkRepro] 2019-04-12T10:30:00.192-0400 d20020| 2019-04-12T10:30:00.192-0400 I  CONTROL  [signalProcessingThread] now exiting
[js_test:shouldworkRepro] 2019-04-12T10:30:00.192-0400 d20020| 2019-04-12T10:30:00.192-0400 I  CONTROL  [signalProcessingThread] shutting down with code:0
[js_test:shouldworkRepro] 2019-04-12T10:30:00.198-0400 d20021| 2019-04-12T10:30:00.198-0400 I  REPL     [replication-1] Restarting oplog query due to error: HostUnreachable: error in fetcher batch callback :: caused by :: Connection closed by peer. Last fetched optime: { ts: Timestamp(1555079398, 6), t: 1 }. Restarts remaining: 1
[js_test:shouldworkRepro] 2019-04-12T10:30:00.198-0400 d20021| 2019-04-12T10:30:00.198-0400 I  CONNPOOL [RS] Ending connection to host vladmac:20020 due to bad connection status; 2 connections to that host remain open
[js_test:shouldworkRepro] 2019-04-12T10:30:00.198-0400 d20021| 2019-04-12T10:30:00.198-0400 I  REPL     [replication-1] Scheduled new oplog query Fetcher source: vladmac:20020 database: local query: { find: "oplog.rs", filter: { ts: { $gte: Timestamp(1555079398, 6) } }, tailable: true, oplogReplay: true, awaitData: true, maxTimeMS: 2000, batchSize: 13981010, term: 1, readConcern: { afterClusterTime: Timestamp(1555079398, 6) } } query metadata: { $replData: 1, $oplogQueryData: 1, $readPreference: { mode: "secondaryPreferred" } } active: 1 findNetworkTimeout: 7000ms getMoreNetworkTimeout: 10000ms shutting down?: 0 first: 1 firstCommandScheduler: RemoteCommandRetryScheduler request: RemoteCommand 70 -- target:vladmac:20020 db:local cmd:{ find: "oplog.rs", filter: { ts: { $gte: Timestamp(1555079398, 6) } }, tailable: true, oplogReplay: true, awaitData: true, maxTimeMS: 2000, batchSize: 13981010, term: 1, readConcern: { afterClusterTime: Timestamp(1555079398, 6) } } active: 1 callbackHandle.valid: 1 callbackHandle.cancelled: 0 attempt: 1 retryPolicy: RetryPolicyImpl maxAttempts: 1 maxTimeMillis: -1ms
[js_test:shouldworkRepro] 2019-04-12T10:30:00.198-0400 d20021| 2019-04-12T10:30:00.198-0400 I  REPL     [replication-0] Error returned from oplog query (no more query restarts left): HostUnreachable: error in fetcher batch callback :: caused by :: Connection reset by peer
[js_test:shouldworkRepro] 2019-04-12T10:30:00.198-0400 d20021| 2019-04-12T10:30:00.198-0400 I  CONNPOOL [RS] Ending connection to host vladmac:20020 due to bad connection status; 1 connections to that host remain open
[js_test:shouldworkRepro] 2019-04-12T10:30:00.199-0400 d20021| 2019-04-12T10:30:00.198-0400 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: HostUnreachable: error in fetcher batch callback :: caused by :: Connection reset by peer
[js_test:shouldworkRepro] 2019-04-12T10:30:00.199-0400 2019-04-12T10:30:00.198-0400 I  -        [js] shell: stopped mongo program on port 20020
[js_test:shouldworkRepro] 2019-04-12T10:30:00.199-0400 ReplSetTest stop *** Mongod in port 20020 shutdown with code (0) ***
[js_test:shouldworkRepro] 2019-04-12T10:30:00.199-0400 ReplSetTest stop *** Shutting down mongod in port 20021 ***
[js_test:shouldworkRepro] 2019-04-12T10:30:00.200-0400 d20021| 2019-04-12T10:30:00.199-0400 I  REPL     [rsBackgroundSync] Clearing sync source vladmac:20020 to choose a new one.
[js_test:shouldworkRepro] 2019-04-12T10:30:00.200-0400 d20021| 2019-04-12T10:30:00.200-0400 I  REPL     [rsBackgroundSync] could not find member to sync from
[js_test:shouldworkRepro] 2019-04-12T10:30:00.200-0400 d20021| 2019-04-12T10:30:00.200-0400 I  CONNPOOL [replexec-1] dropping unhealthy pooled connection to vladmac:20020
[js_test:shouldworkRepro] 2019-04-12T10:30:00.200-0400 d20021| 2019-04-12T10:30:00.200-0400 I  CONNPOOL [Replication] Connecting to vladmac:20020
[js_test:shouldworkRepro] 2019-04-12T10:30:00.200-0400 d20021| 2019-04-12T10:30:00.200-0400 I  NETWORK  [listener] connection accepted from 127.0.0.1:61871 #13 (2 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.201-0400 d20021| 2019-04-12T10:30:00.201-0400 I  NETWORK  [conn13] received client metadata from 127.0.0.1:61871 conn13: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.1.9-115-gb09f71e81a" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "18.0.0" } }
[js_test:shouldworkRepro] 2019-04-12T10:30:00.201-0400 d20021| 2019-04-12T10:30:00.201-0400 I  ELECTION [replexec-0] Scheduling catchup takeover at 2019-04-12T10:30:30.201-0400
[js_test:shouldworkRepro] 2019-04-12T10:30:00.201-0400 
[js_test:shouldworkRepro] 2019-04-12T10:30:00.201-0400 
[js_test:shouldworkRepro] 2019-04-12T10:30:00.201-0400 [jsTest] ----
[js_test:shouldworkRepro] 2019-04-12T10:30:00.201-0400 [jsTest] New session started with sessionID: { "id" : UUID("ef40e23b-47e8-4963-9420-795ed87ab7a5") }
[js_test:shouldworkRepro] 2019-04-12T10:30:00.202-0400 [jsTest] ----
[js_test:shouldworkRepro] 2019-04-12T10:30:00.202-0400 
[js_test:shouldworkRepro] 2019-04-12T10:30:00.202-0400 
[js_test:shouldworkRepro] 2019-04-12T10:30:00.203-0400 d20021| 2019-04-12T10:30:00.202-0400 I  COMMAND  [conn13] Attempting to step down in response to replSetStepDown command
[js_test:shouldworkRepro] 2019-04-12T10:30:00.203-0400 d20021| 2019-04-12T10:30:00.203-0400 I  REPL_HB  [replexec-1] Heartbeat to vladmac:20020 failed after 2 retries, response status: HostUnreachable: Error connecting to vladmac:20020 (127.0.0.1:20020) :: caused by :: Connection refused
[js_test:shouldworkRepro] 2019-04-12T10:30:00.203-0400 d20021| 2019-04-12T10:30:00.203-0400 I  REPL     [replexec-1] Member vladmac:20020 is now in state RS_DOWN
[js_test:shouldworkRepro] 2019-04-12T10:30:00.203-0400 d20021| 2019-04-12T10:30:00.203-0400 I  REPL     [conn13] 'freezing' for 86400 seconds
[js_test:shouldworkRepro] 2019-04-12T10:30:00.207-0400 d20021| 2019-04-12T10:30:00.207-0400 I  COMMAND  [conn13] CMD: validate admin.system.keys
[js_test:shouldworkRepro] 2019-04-12T10:30:00.208-0400 d20021| 2019-04-12T10:30:00.207-0400 I  INDEX    [conn13] validating collection admin.system.keys (UUID: 344947af-588d-4a6c-95ed-fc25d1d38bfc)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.208-0400 d20021| 2019-04-12T10:30:00.208-0400 I  INDEX    [conn13] validating index admin.system.keys.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:30:00.210-0400 d20021| 2019-04-12T10:30:00.210-0400 I  INDEX    [conn13] validated collection admin.system.keys (UUID: 344947af-588d-4a6c-95ed-fc25d1d38bfc)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.211-0400 d20021| 2019-04-12T10:30:00.211-0400 I  COMMAND  [conn13] CMD: validate admin.system.version
[js_test:shouldworkRepro] 2019-04-12T10:30:00.211-0400 d20021| 2019-04-12T10:30:00.211-0400 I  INDEX    [conn13] validating collection admin.system.version (UUID: 79fe740f-34c2-436a-b791-66b5f56f6718)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.211-0400 d20021| 2019-04-12T10:30:00.211-0400 I  INDEX    [conn13] validating index admin.system.version.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:30:00.213-0400 d20021| 2019-04-12T10:30:00.212-0400 I  INDEX    [conn13] validated collection admin.system.version (UUID: 79fe740f-34c2-436a-b791-66b5f56f6718)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.214-0400 d20021| 2019-04-12T10:30:00.214-0400 I  COMMAND  [conn13] CMD: validate config.transactions
[js_test:shouldworkRepro] 2019-04-12T10:30:00.214-0400 d20021| 2019-04-12T10:30:00.214-0400 I  INDEX    [conn13] validating collection config.transactions (UUID: a6348a5e-ec46-4ef4-9436-f5d70a411e91)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.215-0400 d20021| 2019-04-12T10:30:00.215-0400 I  INDEX    [conn13] validating index config.transactions.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:30:00.215-0400 d20021| 2019-04-12T10:30:00.215-0400 W  STORAGE  [conn13] Could not complete validation of table:index-20-5001527355485947772. This is a transient issue as the collection was actively in use by other operations.
[js_test:shouldworkRepro] 2019-04-12T10:30:00.215-0400 d20021| 2019-04-12T10:30:00.215-0400 I  INDEX    [conn13] validated collection config.transactions (UUID: a6348a5e-ec46-4ef4-9436-f5d70a411e91)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.216-0400 d20021| 2019-04-12T10:30:00.216-0400 I  COMMAND  [conn13] CMD: validate db2.coll1
[js_test:shouldworkRepro] 2019-04-12T10:30:00.217-0400 d20021| 2019-04-12T10:30:00.216-0400 I  INDEX    [conn13] validating collection db2.coll1 (UUID: 99548b69-9a01-48f0-a8e1-dea43b0decee)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.217-0400 d20021| 2019-04-12T10:30:00.217-0400 I  INDEX    [conn13] validating index db2.coll1.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:30:00.217-0400 d20021| 2019-04-12T10:30:00.217-0400 W  STORAGE  [conn13] Could not complete validation of table:index-26-5001527355485947772. This is a transient issue as the collection was actively in use by other operations.
[js_test:shouldworkRepro] 2019-04-12T10:30:00.217-0400 d20021| 2019-04-12T10:30:00.217-0400 I  INDEX    [conn13] validated collection db2.coll1 (UUID: 99548b69-9a01-48f0-a8e1-dea43b0decee)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.218-0400 d20021| 2019-04-12T10:30:00.218-0400 I  COMMAND  [conn13] CMD: validate db2.coll2
[js_test:shouldworkRepro] 2019-04-12T10:30:00.218-0400 d20021| 2019-04-12T10:30:00.218-0400 I  INDEX    [conn13] validating collection db2.coll2 (UUID: 2072af0a-fd07-46e6-8685-8a32259a9c95)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.219-0400 d20021| 2019-04-12T10:30:00.219-0400 I  INDEX    [conn13] validating index db2.coll2.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:30:00.219-0400 d20021| 2019-04-12T10:30:00.219-0400 W  STORAGE  [conn13] Could not complete validation of table:index-24-5001527355485947772. This is a transient issue as the collection was actively in use by other operations.
[js_test:shouldworkRepro] 2019-04-12T10:30:00.219-0400 d20021| 2019-04-12T10:30:00.219-0400 I  INDEX    [conn13] validated collection db2.coll2 (UUID: 2072af0a-fd07-46e6-8685-8a32259a9c95)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.221-0400 d20021| 2019-04-12T10:30:00.221-0400 I  COMMAND  [conn13] CMD: validate local.oplog.rs
[js_test:shouldworkRepro] 2019-04-12T10:30:00.221-0400 d20021| 2019-04-12T10:30:00.221-0400 I  INDEX    [conn13] validating collection local.oplog.rs (UUID: b7ba6591-1aec-4686-b3a3-54696c8418de)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.221-0400 d20021| 2019-04-12T10:30:00.221-0400 W  STORAGE  [conn13] Could not complete validation of table:collection-14-5001527355485947772. This is a transient issue as the collection was actively in use by other operations.
[js_test:shouldworkRepro] 2019-04-12T10:30:00.221-0400 d20021| 2019-04-12T10:30:00.221-0400 I  INDEX    [conn13] validated collection local.oplog.rs (UUID: b7ba6591-1aec-4686-b3a3-54696c8418de)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.222-0400 d20021| 2019-04-12T10:30:00.222-0400 I  COMMAND  [conn13] CMD: validate local.replset.minvalid
[js_test:shouldworkRepro] 2019-04-12T10:30:00.222-0400 d20021| 2019-04-12T10:30:00.222-0400 I  INDEX    [conn13] validating collection local.replset.minvalid (UUID: 4c232498-8574-4602-afbb-94d0091e7faa)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.222-0400 d20021| 2019-04-12T10:30:00.222-0400 W  STORAGE  [conn13] Could not complete validation of table:collection-4-5001527355485947772. This is a transient issue as the collection was actively in use by other operations.
[js_test:shouldworkRepro] 2019-04-12T10:30:00.222-0400 d20021| 2019-04-12T10:30:00.222-0400 I  INDEX    [conn13] validating index local.replset.minvalid.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:30:00.224-0400 d20021| 2019-04-12T10:30:00.224-0400 I  INDEX    [conn13] validated collection local.replset.minvalid (UUID: 4c232498-8574-4602-afbb-94d0091e7faa)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.225-0400 d20021| 2019-04-12T10:30:00.225-0400 I  COMMAND  [conn13] CMD: validate local.replset.oplogTruncateAfterPoint
[js_test:shouldworkRepro] 2019-04-12T10:30:00.225-0400 d20021| 2019-04-12T10:30:00.225-0400 I  INDEX    [conn13] validating collection local.replset.oplogTruncateAfterPoint (UUID: 4e8f2907-3b9f-4e9c-9431-4214b017890d)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.239-0400 d20021| 2019-04-12T10:30:00.239-0400 I  INDEX    [conn13] validating index local.replset.oplogTruncateAfterPoint.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:30:00.240-0400 d20021| 2019-04-12T10:30:00.240-0400 I  INDEX    [conn13] validated collection local.replset.oplogTruncateAfterPoint (UUID: 4e8f2907-3b9f-4e9c-9431-4214b017890d)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.241-0400 d20021| 2019-04-12T10:30:00.240-0400 I  COMMAND  [conn13] CMD: validate local.startup_log
[js_test:shouldworkRepro] 2019-04-12T10:30:00.241-0400 d20021| 2019-04-12T10:30:00.241-0400 I  INDEX    [conn13] validating collection local.startup_log (UUID: 53539d5c-5b1a-4df8-bb9f-5251f5653274)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.242-0400 d20021| 2019-04-12T10:30:00.242-0400 I  INDEX    [conn13] validating index local.startup_log.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:30:00.243-0400 d20021| 2019-04-12T10:30:00.243-0400 I  INDEX    [conn13] validated collection local.startup_log (UUID: 53539d5c-5b1a-4df8-bb9f-5251f5653274)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.244-0400 d20021| 2019-04-12T10:30:00.244-0400 I  COMMAND  [conn13] CMD: validate local.system.replset
[js_test:shouldworkRepro] 2019-04-12T10:30:00.244-0400 d20021| 2019-04-12T10:30:00.244-0400 I  INDEX    [conn13] validating collection local.system.replset (UUID: ee3bdbf1-d6ee-48d3-a9ac-ab1343c0d453)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.245-0400 d20021| 2019-04-12T10:30:00.245-0400 I  INDEX    [conn13] validating index local.system.replset.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:30:00.247-0400 d20021| 2019-04-12T10:30:00.247-0400 I  INDEX    [conn13] validated collection local.system.replset (UUID: ee3bdbf1-d6ee-48d3-a9ac-ab1343c0d453)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.248-0400 d20021| 2019-04-12T10:30:00.248-0400 I  COMMAND  [conn13] CMD: validate local.system.rollback.id
[js_test:shouldworkRepro] 2019-04-12T10:30:00.248-0400 d20021| 2019-04-12T10:30:00.248-0400 I  INDEX    [conn13] validating collection local.system.rollback.id (UUID: 1da3dfd8-7cfd-48a0-9d21-a5436718d2d7)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.250-0400 d20021| 2019-04-12T10:30:00.250-0400 I  INDEX    [conn13] validating index local.system.rollback.id.$_id_
[js_test:shouldworkRepro] 2019-04-12T10:30:00.252-0400 d20021| 2019-04-12T10:30:00.251-0400 I  INDEX    [conn13] validated collection local.system.rollback.id (UUID: 1da3dfd8-7cfd-48a0-9d21-a5436718d2d7)
[js_test:shouldworkRepro] 2019-04-12T10:30:00.252-0400 d20021| 2019-04-12T10:30:00.252-0400 I  CONTROL  [signalProcessingThread] got signal 15 (Terminated: 15), will terminate after current cmd ends
[js_test:shouldworkRepro] 2019-04-12T10:30:00.252-0400 d20021| 2019-04-12T10:30:00.252-0400 I  NETWORK  [signalProcessingThread] shutdown: going to close listening sockets...
[js_test:shouldworkRepro] 2019-04-12T10:30:00.252-0400 d20021| 2019-04-12T10:30:00.252-0400 I  NETWORK  [signalProcessingThread] removing socket file: /tmp/mongodb-20021.sock
[js_test:shouldworkRepro] 2019-04-12T10:30:00.252-0400 d20021| 2019-04-12T10:30:00.252-0400 I  REPL     [signalProcessingThread] shutting down replication subsystems
[js_test:shouldworkRepro] 2019-04-12T10:30:00.252-0400 d20021| 2019-04-12T10:30:00.252-0400 I  REPL     [signalProcessingThread] Stopping replication reporter thread
[js_test:shouldworkRepro] 2019-04-12T10:30:00.252-0400 d20021| 2019-04-12T10:30:00.252-0400 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to vladmac:20020: CallbackCanceled: Reporter no longer valid
[js_test:shouldworkRepro] 2019-04-12T10:30:00.253-0400 d20021| 2019-04-12T10:30:00.252-0400 I  REPL     [signalProcessingThread] Stopping replication fetcher thread
[js_test:shouldworkRepro] 2019-04-12T10:30:00.253-0400 d20021| 2019-04-12T10:30:00.252-0400 I  REPL     [signalProcessingThread] Stopping replication applier thread
[js_test:shouldworkRepro] 2019-04-12T10:30:00.253-0400 d20021| 2019-04-12T10:30:00.253-0400 I  REPL     [rsSync-0] Finished oplog application
[js_test:shouldworkRepro] 2019-04-12T10:30:00.717-0400 d20021| 2019-04-12T10:30:00.717-0400 I  REPL_HB  [replexec-2] Heartbeat to vladmac:20020 failed after 2 retries, response status: HostUnreachable: Error connecting to vladmac:20020 (127.0.0.1:20020) :: caused by :: Connection refused
[js_test:shouldworkRepro] 2019-04-12T10:30:01.203-0400 d20021| 2019-04-12T10:30:01.203-0400 I  REPL     [rsBackgroundSync] Stopping replication producer
[js_test:shouldworkRepro] 2019-04-12T10:30:01.204-0400 d20021| 2019-04-12T10:30:01.203-0400 I  REPL     [signalProcessingThread] Stopping replication storage threads
[js_test:shouldworkRepro] 2019-04-12T10:30:01.204-0400 d20021| 2019-04-12T10:30:01.204-0400 I  ASIO     [RS] Killing all outstanding egress activity.
[js_test:shouldworkRepro] 2019-04-12T10:30:01.204-0400 d20021| 2019-04-12T10:30:01.204-0400 I  ASIO     [RS] Killing all outstanding egress activity.
[js_test:shouldworkRepro] 2019-04-12T10:30:01.205-0400 d20021| 2019-04-12T10:30:01.205-0400 I  ASIO     [Replication] Killing all outstanding egress activity.
[js_test:shouldworkRepro] 2019-04-12T10:30:01.206-0400 d20021| 2019-04-12T10:30:01.206-0400 I  CONTROL  [signalProcessingThread] Shutting down free monitoring
[js_test:shouldworkRepro] 2019-04-12T10:30:01.206-0400 d20021| 2019-04-12T10:30:01.206-0400 I  FTDC     [signalProcessingThread] Shutting down full-time diagnostic data capture
[js_test:shouldworkRepro] 2019-04-12T10:30:01.209-0400 d20021| 2019-04-12T10:30:01.209-0400 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
[js_test:shouldworkRepro] 2019-04-12T10:30:01.209-0400 d20021| 2019-04-12T10:30:01.209-0400 I  STORAGE  [signalProcessingThread] Timestamp monitor shutting down
[js_test:shouldworkRepro] 2019-04-12T10:30:01.209-0400 d20021| 2019-04-12T10:30:01.209-0400 I  STORAGE  [signalProcessingThread] WiredTigerKVEngine shutting down
[js_test:shouldworkRepro] 2019-04-12T10:30:01.215-0400 d20021| 2019-04-12T10:30:01.215-0400 I  STORAGE  [signalProcessingThread] Shutting down session sweeper thread
[js_test:shouldworkRepro] 2019-04-12T10:30:01.216-0400 d20021| 2019-04-12T10:30:01.215-0400 I  STORAGE  [signalProcessingThread] Finished shutting down session sweeper thread
[js_test:shouldworkRepro] 2019-04-12T10:30:01.216-0400 d20021| 2019-04-12T10:30:01.215-0400 I  STORAGE  [signalProcessingThread] Shutting down journal flusher thread
[js_test:shouldworkRepro] 2019-04-12T10:30:01.305-0400 d20021| 2019-04-12T10:30:01.305-0400 I  STORAGE  [signalProcessingThread] Finished shutting down journal flusher thread
[js_test:shouldworkRepro] 2019-04-12T10:30:01.306-0400 d20021| 2019-04-12T10:30:01.305-0400 I  STORAGE  [signalProcessingThread] Shutting down checkpoint thread
[js_test:shouldworkRepro] 2019-04-12T10:30:01.306-0400 d20021| 2019-04-12T10:30:01.305-0400 I  STORAGE  [signalProcessingThread] Finished shutting down checkpoint thread
[js_test:shouldworkRepro] 2019-04-12T10:30:01.394-0400 d20021| 2019-04-12T10:30:01.394-0400 I  STORAGE  [signalProcessingThread] shutdown: removing fs lock...
[js_test:shouldworkRepro] 2019-04-12T10:30:01.394-0400 d20021| 2019-04-12T10:30:01.394-0400 I  CONTROL  [signalProcessingThread] now exiting
[js_test:shouldworkRepro] 2019-04-12T10:30:01.394-0400 d20021| 2019-04-12T10:30:01.394-0400 I  CONTROL  [signalProcessingThread] shutting down with code:0
[js_test:shouldworkRepro] 2019-04-12T10:30:01.400-0400 2019-04-12T10:30:01.400-0400 I  -        [js] shell: stopped mongo program on port 20021
[js_test:shouldworkRepro] 2019-04-12T10:30:01.400-0400 ReplSetTest stop *** Mongod in port 20021 shutdown with code (0) ***
[js_test:shouldworkRepro] 2019-04-12T10:30:01.400-0400 ReplSetTest stopSet deleting all dbpaths
[js_test:shouldworkRepro] 2019-04-12T10:30:01.412-0400 ReplSetTest stopSet *** Shut down repl set - test worked ****
[MongoDFixture:job0] 2019-04-12T10:30:01.413-0400 I  NETWORK  [conn4] end connection 127.0.0.1:61835 (0 connections now open)
[js_test:shouldworkRepro] 2019-04-12T10:30:01.413-0400 2019-04-12T10:30:01.413-0400 I  QUERY    [js] Failed to end session { id: UUID("939ef704-1682-44c8-9bdb-67cba9e46184") } due to HostUnreachable: network error while attempting to run command 'endSessions' on host '127.0.0.1:20020'
[js_test:shouldworkRepro] 2019-04-12T10:30:01.413-0400 2019-04-12T10:30:01.413-0400 I  QUERY    [js] Failed to end session { id: UUID("0fcb8d46-a6ae-420b-bfe5-aceb6ad06451") } due to HostUnreachable: network error while attempting to run command 'endSessions' on host '127.0.0.1:20021'
[js_test:shouldworkRepro] 2019-04-12T10:30:01.414-0400 2019-04-12T10:30:01.414-0400 I  QUERY    [js] Failed to end session { id: UUID("c61334b7-92d3-48b1-9828-84921e44a789") } due to HostUnreachable: network error while attempting to run command 'endSessions' on host '127.0.0.1:20021'
[js_test:shouldworkRepro] 2019-04-12T10:30:01.414-0400 2019-04-12T10:30:01.414-0400 I  QUERY    [js] Failed to end session { id: UUID("eac2d43e-0e71-4666-b612-c96a1880d46e") } due to HostUnreachable: network error while attempting to run command 'endSessions' on host 'localhost:20020'
[js_test:shouldworkRepro] 2019-04-12T10:30:01.415-0400 2019-04-12T10:30:01.414-0400 I  QUERY    [js] Failed to end session { id: UUID("ef40e23b-47e8-4963-9420-795ed87ab7a5") } due to HostUnreachable: network error while attempting to run command 'endSessions' on host 'localhost:20021'
[js_test:shouldworkRepro] 2019-04-12T10:30:01.422-0400 JSTest initSyncDBHashMismatch/shouldworkRepro.js finished.
[executor:js_test:job0] 2019-04-12T10:30:01.423-0400 shouldworkRepro.js ran in 22.22 seconds: no failures detected.
[executor:js_test:job0] 2019-04-12T10:30:01.423-0400 Running job0_fixture_teardown...
[js_test:job0_fixture_teardown] 2019-04-12T10:30:01.423-0400 Starting the teardown of MongoDFixture (Job #0).
[MongoDFixture:job0] Stopping mongod on port 20000 with pid 61774...
[executor] 2019-04-12T10:30:01.424-0400 Waiting for threads to complete
[MongoDFixture:job0] 2019-04-12T10:30:01.423-0400 I  CONTROL  [signalProcessingThread] got signal 15 (Terminated: 15), will terminate after current cmd ends
[MongoDFixture:job0] 2019-04-12T10:30:01.424-0400 I  NETWORK  [signalProcessingThread] shutdown: going to close listening sockets...
[MongoDFixture:job0] 2019-04-12T10:30:01.424-0400 I  NETWORK  [signalProcessingThread] removing socket file: /tmp/mongodb-20000.sock
[MongoDFixture:job0] 2019-04-12T10:30:01.424-0400 I  CONTROL  [signalProcessingThread] Shutting down free monitoring
[MongoDFixture:job0] 2019-04-12T10:30:01.424-0400 I  FTDC     [signalProcessingThread] Shutting down full-time diagnostic data capture
[MongoDFixture:job0] 2019-04-12T10:30:01.426-0400 I  STORAGE  [signalProcessingThread] Timestamp monitor shutting down
[MongoDFixture:job0] 2019-04-12T10:30:01.426-0400 I  STORAGE  [signalProcessingThread] WiredTigerKVEngine shutting down
[MongoDFixture:job0] 2019-04-12T10:30:01.432-0400 I  STORAGE  [signalProcessingThread] Shutting down session sweeper thread
[MongoDFixture:job0] 2019-04-12T10:30:01.432-0400 I  STORAGE  [signalProcessingThread] Finished shutting down session sweeper thread
[MongoDFixture:job0] 2019-04-12T10:30:01.432-0400 I  STORAGE  [signalProcessingThread] Shutting down journal flusher thread
[MongoDFixture:job0] 2019-04-12T10:30:01.497-0400 I  STORAGE  [signalProcessingThread] Finished shutting down journal flusher thread
[MongoDFixture:job0] 2019-04-12T10:30:01.497-0400 I  STORAGE  [signalProcessingThread] Shutting down checkpoint thread
[MongoDFixture:job0] 2019-04-12T10:30:01.497-0400 I  STORAGE  [signalProcessingThread] Finished shutting down checkpoint thread
[MongoDFixture:job0] 2019-04-12T10:30:01.591-0400 I  STORAGE  [signalProcessingThread] shutdown: removing fs lock...
[MongoDFixture:job0] 2019-04-12T10:30:01.592-0400 I  CONTROL  [signalProcessingThread] now exiting
[MongoDFixture:job0] 2019-04-12T10:30:01.592-0400 I  CONTROL  [signalProcessingThread] shutting down with code:0
[MongoDFixture:job0] Successfully stopped the mongod on port 20000.
[js_test:job0_fixture_teardown] 2019-04-12T10:30:01.595-0400 Finished the teardown of MongoDFixture (Job #0).
[executor:js_test:job0] 2019-04-12T10:30:01.595-0400 job0_fixture_teardown ran in 0.17 seconds: no failures detected.
[executor] 2019-04-12T10:30:01.595-0400 Threads are completed!
[executor] 2019-04-12T10:30:01.596-0400 Summary: All 3 test(s) passed in 23.13 seconds.
[resmoke] 2019-04-12T10:30:01.596-0400 ================================================================================
[resmoke] 2019-04-12T10:30:01.596-0400 Summary of with_server suite: All 3 test(s) passed in 23.13 seconds.
3 test(s) ran in 23.13 seconds (3 succeeded, 0 were skipped, 0 failed, 0 errored)
    js_tests: All 3 test(s) passed in 23.13 seconds.
[resmoke] 2019-04-12T10:30:01.596-0400 Exiting with code: 0
